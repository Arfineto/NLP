{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"datsets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar 28 13:00:48 2018\n",
    "\n",
    "@author: rniel\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "file  = open(dataset_path+\"imdb_labelled.txt\", \"rt\")\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "for c in contents:\n",
    "    temp=c.split(\"\\t\")\n",
    "    x.append(temp[0])\n",
    "    y.append(int(temp[1]))\n",
    "    \n",
    "file  = open(dataset_path+\"amazon_cells_labelled.txt\", \"rt\")\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "\n",
    "\n",
    "for c in contents:\n",
    "    temp=c.split(\"\\t\")\n",
    "    x.append(temp[0])\n",
    "    y.append(int(temp[1]))\n",
    "    \n",
    "file  = open(dataset_path+\"yelp_labelled.txt\", \"rt\")\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "\n",
    "\n",
    "for c in contents:\n",
    "    temp=c.split(\"\\t\")\n",
    "    x.append(temp[0])\n",
    "    y.append(int(temp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file  = open(dataset_path+\"book.txt\", \"rt\")\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "del contents[-1]\n",
    "\n",
    "for c in contents:\n",
    "    temp=c.split(\"\\t\")\n",
    "    x.append(temp[1])\n",
    "    y.append(int(temp[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file  = open(\"Text/rt-polarity-neg.txt\", \"rt\",encoding = \"ISO-8859-1\")\n",
    "\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "del contents[-1]\n",
    "\n",
    "for i in range(len(contents)):\n",
    "    x.append(contents[i])\n",
    "    y.append(0)\n",
    "\n",
    "file  = open(\"Text/rt-polarity-pos.txt\", \"rt\",encoding = \"ISO-8859-1\")\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "del contents[-1]\n",
    "\n",
    "for i in range(len(contents)):\n",
    "    x.append(contents[i])\n",
    "    y.append(1)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretreatment of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretreatment(sentences):\n",
    "    \"\"\" This function clean an array of sentence and split them into list of words.\n",
    "            \n",
    "        Keyword arguments:\n",
    "        sentences -- An array of sentences\n",
    "    \"\"\"\n",
    "    for s in sentences:\n",
    "        s = s.replace(\",\", \" \")\n",
    "        s = s.replace(\".\", \" \")\n",
    "        s = s.replace(\"-\", \" \")\n",
    "        s = s.replace(\"!\", \" \")\n",
    "        s = s.replace(\"?\", \" \")\n",
    "        s = s.replace(\":\", \" \")\n",
    "        s = s.replace(\"*\", \" \")\n",
    "        s = s.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "    ret_sentences = []\n",
    "    for s in sentences:\n",
    "        ret_sentences.append(s.split(\" \"))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        ret_sentences[i]=[w for w in ret_sentences[i] if w!=\"\"]\n",
    "\n",
    "    for s in ret_sentences:\n",
    "        for w in s :\n",
    "            w=w.lower()\n",
    "    return ret_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = pretreatment(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Training of the Word2Vec model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630688, 1126390)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences, size=300, min_count=20, workers=4,sorted_vocab=1)\n",
    "model.train(sentences,total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of the GoogleNews vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118192911148071)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the google word2vec model\n",
    "filename = 'lib/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True,limit= 500000)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation of sentences into list of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorization(sentences,word2vec_model,word_length=300,max_len=50,adaptable_len=False):\n",
    "    \"\"\" This function transform an array of list of words (the splitted sentences) into a array of list of vectors \n",
    "            representing the words thanks to an already trained word2Vec model\n",
    "            \n",
    "        Keyword arguments:\n",
    "        sentences -- The different sentences you want to vectorize\n",
    "        word2vec_model -- An already trained gensim Word2Vec model\n",
    "        word_length -- The number of dimension of the vectors representing the words (default 300) \n",
    "        max_len -- The maximum length of a sentence (default 50) \n",
    "        adaptable_len -- A boolean allowing to adapt the maximum length to the longest sentence in your dataset\n",
    "    \"\"\"\n",
    "    sentences_v=[]\n",
    "    unusued_word=[]\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    sp_words = set(stopwords.words('english'))    \n",
    "    \n",
    "    for s in sentences:\n",
    "        temp=[]\n",
    "        for w in s :\n",
    "            if(w not in sp_words):\n",
    "                try :\n",
    "                    temp.append(word2vec_model[w])\n",
    "                except : \n",
    "                    unusued_word.append(w)\n",
    "        sentences_v.append(temp)\n",
    "        \n",
    "    if (adaptable_len is True):\n",
    "        for i in range(len(sentences_v)):\n",
    "            if(len(sentences_v[i])>max_len):\n",
    "                max_len=len(sentences_v[i])\n",
    "\n",
    "    for s in sentences_v:\n",
    "        sentence_length = len(s)\n",
    "        if(sentence_length<max_len):    \n",
    "            for j in range(max_len-sentence_length):\n",
    "                s.append([0]*word_length)\n",
    "    return sentences_v,max_len\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_v = vectorization(sentences,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving of the variables for computation time gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.save(\"variables/sentences_v_google.npy\",sentences_v)\n",
    "np.save(\"variables/y.npy\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "x_vec_train = np.load(\"variables/sentences_v_google.npy\")\n",
    "y = np.load(\"variables/y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting of the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_vec_train,x_vec_test,y_vec_train,y_vec_test = train_test_split(sentences_v,y,test_size=0.2,random_state=42)\n",
    "x_vec_train=np.array(x_vec_train)\n",
    "x_vec_test=np.array(x_vec_test)\n",
    "from keras.utils import to_categorical\n",
    "y_vec_train=to_categorical(y_vec_train)\n",
    "y_vec_test=to_categorical(y_vec_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "def model_creation():\n",
    "    \n",
    "    WORD_LENGTH=300\n",
    "    max_len=50\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation,Convolution1D,Flatten,MaxPooling1D,Conv1D,LSTM\n",
    "    from keras.layers import Dropout\n",
    "\n",
    "    NN = Sequential()\n",
    "\n",
    "    NN.add(Dense(max_len, activation='relu',input_shape=(max_len,WORD_LENGTH)))\n",
    "    NN.add(Dropout(0.5))\n",
    "    \n",
    "    NN.add(Convolution1D(64,kernel_size=10,activation='relu',border_mode='same',input_shape=(max_len,WORD_LENGTH)))\n",
    "    NN.add(Dropout(0.5))\n",
    "    \n",
    "    NN.add(MaxPooling1D(pool_size=2, strides=None, padding='same'))\n",
    "\n",
    "    NN.add(LSTM(50))\n",
    "    NN.add(Dropout(0.5))\n",
    "        \n",
    "    NN.add(Dense(50, activation='relu'))\n",
    "    NN.add(Dropout(0.5))\n",
    "\n",
    "    NN.add(Dense(25, activation='relu'))\n",
    "    NN.add(Dropout(0.5))\n",
    "\n",
    "    NN.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting between test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1882967b811b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_vec_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_vec_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_vec_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_vec_valid\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vec_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_vec_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of lines in the validation set : \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vec_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of lines in the test set: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vec_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "x_vec_test,x_vec_valid,y_vec_test,y_vec_valid =  train_test_split(x_vec_test,y_vec_test,test_size=0.5,random_state=42)\n",
    "print(\"Number of lines in the validation set : \"+str(len(x_vec_valid)))\n",
    "print(\"Number of lines in the test set: \"+str(len(x_vec_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c534b8106736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mNB_STEPS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_creation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/client_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInteractiveSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msession_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import clone_model\n",
    "\n",
    "hist=[]\n",
    "NB_STEPS = 100\n",
    "NN = model_creation()\n",
    "for i in range(NB_STEPS):\n",
    "    print(\"Etape i :\" + str(i))\n",
    "    #temp_model=clone_model(NN)\n",
    "    #temp_model.set_weights(NN.get_weights())\n",
    "    NN.fit(np.array(x_vec_train), np.array(y_vec_train), epochs=1, batch_size=100)\n",
    "    #hist.append((temp_model,NN.evaluate(x_vec_valid,y_vec_valid)[1]))\n",
    "    hist.append(NN.evaluate(x_vec_valid,y_vec_valid)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting of validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4a89fe45c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(hist)    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the model on all the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=10, activation=\"relu\", input_shape=(50, 300), padding=\"same\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "10086/10086 [==============================] - 11s 1ms/step - loss: 0.6900 - acc: 0.5428\n",
      "Epoch 2/150\n",
      "10086/10086 [==============================] - 6s 559us/step - loss: 0.4709 - acc: 0.7948\n",
      "Epoch 3/150\n",
      "10086/10086 [==============================] - 6s 556us/step - loss: 0.2976 - acc: 0.8829\n",
      "Epoch 4/150\n",
      "10086/10086 [==============================] - 6s 563us/step - loss: 0.2702 - acc: 0.8899\n",
      "Epoch 5/150\n",
      "10086/10086 [==============================] - 6s 560us/step - loss: 0.2393 - acc: 0.9014\n",
      "Epoch 6/150\n",
      "10086/10086 [==============================] - 6s 561us/step - loss: 0.2327 - acc: 0.9005\n",
      "Epoch 7/150\n",
      "10086/10086 [==============================] - 6s 560us/step - loss: 0.2268 - acc: 0.9039\n",
      "Epoch 8/150\n",
      "10086/10086 [==============================] - 6s 558us/step - loss: 0.2177 - acc: 0.9106\n",
      "Epoch 9/150\n",
      "10086/10086 [==============================] - 6s 557us/step - loss: 0.2123 - acc: 0.9131\n",
      "Epoch 10/150\n",
      "10086/10086 [==============================] - 6s 559us/step - loss: 0.2104 - acc: 0.9113\n",
      "Epoch 11/150\n",
      "10086/10086 [==============================] - 6s 555us/step - loss: 0.2037 - acc: 0.9167\n",
      "Epoch 12/150\n",
      "10086/10086 [==============================] - 6s 580us/step - loss: 0.1994 - acc: 0.9167\n",
      "Epoch 13/150\n",
      "10086/10086 [==============================] - 6s 593us/step - loss: 0.1909 - acc: 0.9232\n",
      "Epoch 14/150\n",
      "10086/10086 [==============================] - 6s 593us/step - loss: 0.1914 - acc: 0.9192\n",
      "Epoch 15/150\n",
      "10086/10086 [==============================] - 6s 608us/step - loss: 0.1856 - acc: 0.9214\n",
      "Epoch 16/150\n",
      "10086/10086 [==============================] - 6s 614us/step - loss: 0.1797 - acc: 0.9269\n",
      "Epoch 17/150\n",
      "10086/10086 [==============================] - 6s 620us/step - loss: 0.1749 - acc: 0.9284\n",
      "Epoch 18/150\n",
      "10086/10086 [==============================] - 6s 621us/step - loss: 0.1792 - acc: 0.9285\n",
      "Epoch 19/150\n",
      "10086/10086 [==============================] - 6s 625us/step - loss: 0.1715 - acc: 0.9302\n",
      "Epoch 20/150\n",
      "10086/10086 [==============================] - 6s 632us/step - loss: 0.1679 - acc: 0.9307\n",
      "Epoch 21/150\n",
      "10086/10086 [==============================] - 6s 640us/step - loss: 0.1606 - acc: 0.9344\n",
      "Epoch 22/150\n",
      "10086/10086 [==============================] - 7s 667us/step - loss: 0.1618 - acc: 0.9352\n",
      "Epoch 23/150\n",
      "10086/10086 [==============================] - 6s 638us/step - loss: 0.1651 - acc: 0.9342\n",
      "Epoch 24/150\n",
      "10086/10086 [==============================] - 6s 639us/step - loss: 0.1627 - acc: 0.9377\n",
      "Epoch 25/150\n",
      "10086/10086 [==============================] - 7s 645us/step - loss: 0.1523 - acc: 0.9383\n",
      "Epoch 26/150\n",
      "10086/10086 [==============================] - 7s 671us/step - loss: 0.1529 - acc: 0.9388\n",
      "Epoch 27/150\n",
      "10086/10086 [==============================] - 7s 664us/step - loss: 0.1483 - acc: 0.9404\n",
      "Epoch 28/150\n",
      "10086/10086 [==============================] - 7s 659us/step - loss: 0.1477 - acc: 0.9429\n",
      "Epoch 29/150\n",
      "10086/10086 [==============================] - 7s 667us/step - loss: 0.1477 - acc: 0.9409\n",
      "Epoch 30/150\n",
      "10086/10086 [==============================] - 7s 672us/step - loss: 0.1493 - acc: 0.9403\n",
      "Epoch 31/150\n",
      "10086/10086 [==============================] - 7s 676us/step - loss: 0.1450 - acc: 0.9413\n",
      "Epoch 32/150\n",
      "10086/10086 [==============================] - 7s 650us/step - loss: 0.1393 - acc: 0.9427\n",
      "Epoch 33/150\n",
      "10086/10086 [==============================] - 7s 653us/step - loss: 0.1393 - acc: 0.9438\n",
      "Epoch 34/150\n",
      "10086/10086 [==============================] - 7s 666us/step - loss: 0.1416 - acc: 0.9424\n",
      "Epoch 35/150\n",
      "10086/10086 [==============================] - 7s 674us/step - loss: 0.1412 - acc: 0.9444\n",
      "Epoch 36/150\n",
      "10086/10086 [==============================] - 7s 667us/step - loss: 0.1379 - acc: 0.9423\n",
      "Epoch 37/150\n",
      "10086/10086 [==============================] - 7s 667us/step - loss: 0.1288 - acc: 0.9486\n",
      "Epoch 38/150\n",
      "10086/10086 [==============================] - 7s 663us/step - loss: 0.1214 - acc: 0.9499\n",
      "Epoch 39/150\n",
      "10086/10086 [==============================] - 7s 676us/step - loss: 0.1272 - acc: 0.9483\n",
      "Epoch 40/150\n",
      "10086/10086 [==============================] - 7s 684us/step - loss: 0.1357 - acc: 0.9434\n",
      "Epoch 41/150\n",
      "10086/10086 [==============================] - 7s 674us/step - loss: 0.1268 - acc: 0.9480\n",
      "Epoch 42/150\n",
      "10086/10086 [==============================] - 7s 691us/step - loss: 0.1292 - acc: 0.9451\n",
      "Epoch 43/150\n",
      "10086/10086 [==============================] - 7s 719us/step - loss: 0.1249 - acc: 0.9497\n",
      "Epoch 44/150\n",
      "10086/10086 [==============================] - 8s 794us/step - loss: 0.1236 - acc: 0.9477\n",
      "Epoch 45/150\n",
      "10086/10086 [==============================] - 7s 670us/step - loss: 0.1310 - acc: 0.9448\n",
      "Epoch 46/150\n",
      "10086/10086 [==============================] - 7s 700us/step - loss: 0.1234 - acc: 0.9502\n",
      "Epoch 47/150\n",
      "10086/10086 [==============================] - 8s 809us/step - loss: 0.1281 - acc: 0.9502\n",
      "Epoch 48/150\n",
      "10086/10086 [==============================] - 8s 791us/step - loss: 0.1198 - acc: 0.9495\n",
      "Epoch 49/150\n",
      "10086/10086 [==============================] - 8s 745us/step - loss: 0.1205 - acc: 0.9497\n",
      "Epoch 50/150\n",
      "10086/10086 [==============================] - 8s 762us/step - loss: 0.1194 - acc: 0.9515\n",
      "Epoch 51/150\n",
      "10086/10086 [==============================] - 8s 782us/step - loss: 0.1241 - acc: 0.9513\n",
      "Epoch 52/150\n",
      "10086/10086 [==============================] - 7s 739us/step - loss: 0.1161 - acc: 0.9516\n",
      "Epoch 53/150\n",
      "10086/10086 [==============================] - 7s 721us/step - loss: 0.1200 - acc: 0.9542\n",
      "Epoch 54/150\n",
      "10086/10086 [==============================] - 7s 730us/step - loss: 0.1210 - acc: 0.9515\n",
      "Epoch 55/150\n",
      "10086/10086 [==============================] - 8s 782us/step - loss: 0.1202 - acc: 0.9512\n",
      "Epoch 56/150\n",
      "10086/10086 [==============================] - 8s 764us/step - loss: 0.1237 - acc: 0.9506\n",
      "Epoch 57/150\n",
      "10086/10086 [==============================] - 7s 708us/step - loss: 0.1136 - acc: 0.9537\n",
      "Epoch 58/150\n",
      "10086/10086 [==============================] - 7s 727us/step - loss: 0.1183 - acc: 0.9518\n",
      "Epoch 59/150\n",
      "10086/10086 [==============================] - 8s 745us/step - loss: 0.1173 - acc: 0.9547\n",
      "Epoch 60/150\n",
      "10086/10086 [==============================] - 7s 686us/step - loss: 0.1123 - acc: 0.9514\n",
      "Epoch 61/150\n",
      "10086/10086 [==============================] - 7s 734us/step - loss: 0.1112 - acc: 0.9550\n",
      "Epoch 62/150\n",
      "10086/10086 [==============================] - 8s 774us/step - loss: 0.1138 - acc: 0.9515\n",
      "Epoch 63/150\n",
      "10086/10086 [==============================] - 7s 737us/step - loss: 0.1114 - acc: 0.9516\n",
      "Epoch 64/150\n",
      "10086/10086 [==============================] - 7s 742us/step - loss: 0.1096 - acc: 0.9566\n",
      "Epoch 65/150\n",
      "10086/10086 [==============================] - 8s 752us/step - loss: 0.1050 - acc: 0.9577\n",
      "Epoch 66/150\n",
      "10086/10086 [==============================] - 8s 772us/step - loss: 0.1160 - acc: 0.9530\n",
      "Epoch 67/150\n",
      "10086/10086 [==============================] - 8s 767us/step - loss: 0.1128 - acc: 0.9574\n",
      "Epoch 68/150\n",
      "10086/10086 [==============================] - 7s 736us/step - loss: 0.1102 - acc: 0.9551\n",
      "Epoch 69/150\n",
      "10086/10086 [==============================] - 7s 737us/step - loss: 0.1052 - acc: 0.9553\n",
      "Epoch 70/150\n",
      "10086/10086 [==============================] - 7s 728us/step - loss: 0.1113 - acc: 0.9525\n",
      "Epoch 71/150\n",
      "10086/10086 [==============================] - 8s 748us/step - loss: 0.1086 - acc: 0.9544\n",
      "Epoch 72/150\n",
      "10086/10086 [==============================] - 8s 755us/step - loss: 0.1034 - acc: 0.9576\n",
      "Epoch 73/150\n",
      "10086/10086 [==============================] - 8s 759us/step - loss: 0.1116 - acc: 0.9543\n",
      "Epoch 74/150\n",
      "10086/10086 [==============================] - 9s 900us/step - loss: 0.1119 - acc: 0.9559\n",
      "Epoch 75/150\n",
      "10086/10086 [==============================] - 8s 775us/step - loss: 0.1121 - acc: 0.9566\n",
      "Epoch 76/150\n",
      "10086/10086 [==============================] - 7s 694us/step - loss: 0.1116 - acc: 0.9557\n",
      "Epoch 77/150\n",
      "10086/10086 [==============================] - 7s 696us/step - loss: 0.1069 - acc: 0.9569\n",
      "Epoch 78/150\n",
      "10086/10086 [==============================] - 7s 703us/step - loss: 0.1088 - acc: 0.9542\n",
      "Epoch 79/150\n",
      "10086/10086 [==============================] - 7s 706us/step - loss: 0.1057 - acc: 0.9596\n",
      "Epoch 80/150\n",
      "10086/10086 [==============================] - 7s 722us/step - loss: 0.1059 - acc: 0.9576\n",
      "Epoch 81/150\n",
      "10086/10086 [==============================] - 7s 716us/step - loss: 0.1068 - acc: 0.9556\n",
      "Epoch 82/150\n",
      "10086/10086 [==============================] - 7s 730us/step - loss: 0.1062 - acc: 0.9547\n",
      "Epoch 83/150\n",
      "10086/10086 [==============================] - 7s 733us/step - loss: 0.1030 - acc: 0.9576\n",
      "Epoch 84/150\n",
      "10086/10086 [==============================] - 7s 727us/step - loss: 0.1068 - acc: 0.9567\n",
      "Epoch 85/150\n",
      "10086/10086 [==============================] - 7s 707us/step - loss: 0.0991 - acc: 0.9577\n",
      "Epoch 86/150\n",
      "10086/10086 [==============================] - 8s 745us/step - loss: 0.1035 - acc: 0.9584\n",
      "Epoch 87/150\n",
      "10086/10086 [==============================] - 7s 738us/step - loss: 0.1042 - acc: 0.9590\n",
      "Epoch 88/150\n",
      "10086/10086 [==============================] - 7s 726us/step - loss: 0.1130 - acc: 0.9552\n",
      "Epoch 89/150\n",
      "10086/10086 [==============================] - 7s 723us/step - loss: 0.1043 - acc: 0.9579\n",
      "Epoch 90/150\n",
      "10086/10086 [==============================] - 7s 724us/step - loss: 0.1056 - acc: 0.9572\n",
      "Epoch 91/150\n",
      "10086/10086 [==============================] - 7s 720us/step - loss: 0.0985 - acc: 0.9587\n",
      "Epoch 92/150\n",
      "10086/10086 [==============================] - 7s 717us/step - loss: 0.1071 - acc: 0.9555\n",
      "Epoch 93/150\n",
      "10086/10086 [==============================] - 7s 708us/step - loss: 0.1034 - acc: 0.9564\n",
      "Epoch 94/150\n",
      "10086/10086 [==============================] - 7s 716us/step - loss: 0.1061 - acc: 0.9570\n",
      "Epoch 95/150\n",
      "10086/10086 [==============================] - 7s 730us/step - loss: 0.1025 - acc: 0.9556\n",
      "Epoch 96/150\n",
      "10086/10086 [==============================] - 7s 741us/step - loss: 0.1031 - acc: 0.9614\n",
      "Epoch 97/150\n",
      "10086/10086 [==============================] - 7s 730us/step - loss: 0.0998 - acc: 0.9589\n",
      "Epoch 98/150\n",
      "10086/10086 [==============================] - 7s 728us/step - loss: 0.0966 - acc: 0.9593\n",
      "Epoch 99/150\n",
      "10086/10086 [==============================] - 7s 722us/step - loss: 0.0956 - acc: 0.9602\n",
      "Epoch 100/150\n",
      "10086/10086 [==============================] - 7s 723us/step - loss: 0.1011 - acc: 0.9587\n",
      "Epoch 101/150\n",
      "10086/10086 [==============================] - 7s 721us/step - loss: 0.0975 - acc: 0.9590\n",
      "Epoch 102/150\n",
      "10086/10086 [==============================] - 7s 719us/step - loss: 0.1013 - acc: 0.9557\n",
      "Epoch 103/150\n",
      "10086/10086 [==============================] - 7s 722us/step - loss: 0.0995 - acc: 0.9601\n",
      "Epoch 104/150\n",
      "10086/10086 [==============================] - 7s 735us/step - loss: 0.0967 - acc: 0.9614\n",
      "Epoch 105/150\n",
      "10086/10086 [==============================] - 7s 697us/step - loss: 0.0967 - acc: 0.9620\n",
      "Epoch 106/150\n",
      "10086/10086 [==============================] - 7s 713us/step - loss: 0.0945 - acc: 0.9613\n",
      "Epoch 107/150\n",
      "10086/10086 [==============================] - 7s 713us/step - loss: 0.1007 - acc: 0.9584\n",
      "Epoch 108/150\n",
      "10086/10086 [==============================] - 11s 1ms/step - loss: 0.0992 - acc: 0.9625\n",
      "Epoch 109/150\n",
      "10086/10086 [==============================] - 9s 934us/step - loss: 0.0960 - acc: 0.9600\n",
      "Epoch 110/150\n",
      "10086/10086 [==============================] - 7s 656us/step - loss: 0.0962 - acc: 0.9601\n",
      "Epoch 111/150\n",
      "10086/10086 [==============================] - 6s 638us/step - loss: 0.1033 - acc: 0.9585\n",
      "Epoch 112/150\n",
      "10086/10086 [==============================] - 8s 746us/step - loss: 0.0982 - acc: 0.9586\n",
      "Epoch 113/150\n",
      "10086/10086 [==============================] - 7s 682us/step - loss: 0.0918 - acc: 0.9628\n",
      "Epoch 114/150\n",
      "10086/10086 [==============================] - 7s 689us/step - loss: 0.0995 - acc: 0.9588\n",
      "Epoch 115/150\n",
      "10086/10086 [==============================] - 7s 699us/step - loss: 0.0986 - acc: 0.9579\n",
      "Epoch 116/150\n",
      "10086/10086 [==============================] - 7s 685us/step - loss: 0.0967 - acc: 0.9597\n",
      "Epoch 117/150\n",
      "10086/10086 [==============================] - 7s 716us/step - loss: 0.0934 - acc: 0.9609\n",
      "Epoch 118/150\n",
      "10086/10086 [==============================] - 7s 703us/step - loss: 0.0989 - acc: 0.9594\n",
      "Epoch 119/150\n",
      "10086/10086 [==============================] - 7s 707us/step - loss: 0.0953 - acc: 0.9602\n",
      "Epoch 120/150\n",
      "10086/10086 [==============================] - 7s 706us/step - loss: 0.1000 - acc: 0.9592\n",
      "Epoch 121/150\n",
      "10086/10086 [==============================] - 7s 716us/step - loss: 0.0931 - acc: 0.9630\n",
      "Epoch 122/150\n",
      "10086/10086 [==============================] - 7s 726us/step - loss: 0.0973 - acc: 0.9608\n",
      "Epoch 123/150\n",
      "10086/10086 [==============================] - 7s 728us/step - loss: 0.1016 - acc: 0.9591\n",
      "Epoch 124/150\n",
      "10086/10086 [==============================] - 7s 725us/step - loss: 0.0938 - acc: 0.9620\n",
      "Epoch 125/150\n",
      "10086/10086 [==============================] - 7s 711us/step - loss: 0.0994 - acc: 0.9615\n",
      "Epoch 126/150\n",
      "10086/10086 [==============================] - 7s 722us/step - loss: 0.1017 - acc: 0.9616\n",
      "Epoch 127/150\n",
      "10086/10086 [==============================] - 7s 723us/step - loss: 0.0950 - acc: 0.9615\n",
      "Epoch 128/150\n",
      "10086/10086 [==============================] - 7s 717us/step - loss: 0.1026 - acc: 0.9595\n",
      "Epoch 129/150\n",
      "10086/10086 [==============================] - 7s 716us/step - loss: 0.0927 - acc: 0.9628\n",
      "Epoch 130/150\n",
      "10086/10086 [==============================] - 7s 720us/step - loss: 0.0968 - acc: 0.9593\n",
      "Epoch 131/150\n",
      "10086/10086 [==============================] - 7s 737us/step - loss: 0.0965 - acc: 0.9591\n",
      "Epoch 132/150\n",
      "10086/10086 [==============================] - 7s 732us/step - loss: 0.0921 - acc: 0.9607\n",
      "Epoch 133/150\n",
      "10086/10086 [==============================] - 7s 721us/step - loss: 0.0934 - acc: 0.9609\n",
      "Epoch 134/150\n",
      "10086/10086 [==============================] - 7s 719us/step - loss: 0.0879 - acc: 0.9628\n",
      "Epoch 135/150\n",
      "10086/10086 [==============================] - 7s 728us/step - loss: 0.0934 - acc: 0.9593\n",
      "Epoch 136/150\n",
      "10086/10086 [==============================] - 7s 726us/step - loss: 0.0997 - acc: 0.9589\n",
      "Epoch 137/150\n",
      "10086/10086 [==============================] - 7s 727us/step - loss: 0.1005 - acc: 0.9584\n",
      "Epoch 138/150\n",
      "10086/10086 [==============================] - 7s 719us/step - loss: 0.0925 - acc: 0.9617\n",
      "Epoch 139/150\n",
      "10086/10086 [==============================] - 7s 720us/step - loss: 0.0950 - acc: 0.9614\n",
      "Epoch 140/150\n",
      "10086/10086 [==============================] - 7s 731us/step - loss: 0.0909 - acc: 0.9620\n",
      "Epoch 141/150\n",
      "10086/10086 [==============================] - 7s 742us/step - loss: 0.1035 - acc: 0.9592\n",
      "Epoch 142/150\n",
      "10086/10086 [==============================] - 7s 729us/step - loss: 0.0983 - acc: 0.9579\n",
      "Epoch 143/150\n",
      "10086/10086 [==============================] - 7s 735us/step - loss: 0.0922 - acc: 0.9591\n",
      "Epoch 144/150\n",
      "10086/10086 [==============================] - 7s 724us/step - loss: 0.0950 - acc: 0.9587\n",
      "Epoch 145/150\n",
      "10086/10086 [==============================] - 7s 724us/step - loss: 0.0893 - acc: 0.9635\n",
      "Epoch 146/150\n",
      "10086/10086 [==============================] - 7s 725us/step - loss: 0.0931 - acc: 0.9613\n",
      "Epoch 147/150\n",
      "10086/10086 [==============================] - 7s 731us/step - loss: 0.0944 - acc: 0.9614\n",
      "Epoch 148/150\n",
      "10086/10086 [==============================] - 7s 719us/step - loss: 0.0898 - acc: 0.9638\n",
      "Epoch 149/150\n",
      "10086/10086 [==============================] - 7s 731us/step - loss: 0.0871 - acc: 0.9637\n",
      "Epoch 150/150\n",
      "10086/10086 [==============================] - 7s 735us/step - loss: 0.0889 - acc: 0.9605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4a86252278>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final = model_creation()\n",
    "Final.fit(np.array(sentences_v),to_categorical(y),epochs=150,batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation and model comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_estimation_cross_val(nb_epochs,nb_s_epoch,nb_split):\n",
    "    nb_min = (nb_epochs*nb_s_epoch*nb_split*nb_split)/60\n",
    "    print(\"The cross_validation will take around : \"+str(nb_min)+\" mn\")\n",
    "    return nb_min\n",
    "    \n",
    "def cross_val(model_function,x_train,y_train,nb_splits,nb_epochs):\n",
    "    \"\"\" This function allows to do a cross validation over a keras model\n",
    "        Keyword arguments:\n",
    "        model_function -- A function returning your model\n",
    "        x_train -- The train data\n",
    "        y_train -- The train target\n",
    "        nb_splits -- The number of folds \n",
    "        nb_epochs -- The number of epochs you want to train your network with \n",
    "                     /!\\ One validation will be a total of nb_folds*nb_epochs total number of epochs\n",
    "    \"\"\"\n",
    "    import time    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    indices = np.linspace(0,len(x_train)-1,num=nb_splits+1,dtype=int)\n",
    "    split_data = []\n",
    "\n",
    "    # Creation of the splits\n",
    "    split_data.append((x_train[indices[0]:indices[1]],y_train[indices[0]:indices[1]]))\n",
    "    for i in range(1,len(indices)-1):\n",
    "        split_data.append((x_train[indices[i]+1:indices[i+1]],y_train[indices[i]+1:indices[i+1]]))\n",
    "\n",
    "    # Cross val    \n",
    "    score = []\n",
    "    for i in range(nb_splits):\n",
    "        model = model_function()\n",
    "        valid = split_data[i]\n",
    "        for j in range(nb_splits):\n",
    "            if (j != i):\n",
    "                model.fit(split_data[j][0],split_data[j][1],epochs=nb_epochs,batch_size=100)\n",
    "        score.append(model.evaluate(valid[0],valid[1]))\n",
    "        \n",
    "    std = np.std([a[1] for a in score])\n",
    "    mean = np.mean([a[1] for a in score])\n",
    "    \n",
    "    print(\"Accruracy neural network: \"+str(mean*100)+\"% +- \"+str(std*100)+\"%\")\n",
    "    print(\"--- %s minutes elapsed---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "    return model,score,(mean,std)\n",
    "\n",
    "\n",
    "def multi_cross_val(model_functions,x_train,y_train,nb_splits,nb_epochs):\n",
    "    \"\"\" This function allows to compare different models over a k-fold cross validation\n",
    "        Keyword arguments:\n",
    "        model_functions -- A list containing the functions returning the different models you want to test\n",
    "        x_train -- The train data\n",
    "        y_train -- The train target\n",
    "        nb_splits -- The number of folds \n",
    "        nb_epochs -- The number of epochs you want to train your network with \n",
    "                     /!\\ One validation will be a total of nb_folds*nb_epochs total number of epochs\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    indices = np.linspace(0,len(x_train)-1,num=nb_splits+1,dtype=int)\n",
    "    split_data = []\n",
    "\n",
    "    # Creeation of the splits\n",
    "    split_data.append((x_train[indices[0]:indices[1]],y_train[indices[0]:indices[1]]))\n",
    "    for i in range(1,len(indices)-1):\n",
    "        split_data.append((x_train[indices[i]+1:indices[i+1]],y_train[indices[i]+1:indices[i+1]]))\n",
    "\n",
    "    # Cross val    \n",
    "    score = []\n",
    "    for i in range(nb_splits):\n",
    "        print(\"Split no :\"+str(i))\n",
    "        models = [f() for f in model_functions]\n",
    "        valid = split_data[i]\n",
    "        for j in range(nb_splits):\n",
    "            if (j != i):\n",
    "                for m in models : \n",
    "                    m.fit(split_data[j][0],split_data[j][1],epochs=nb_epochs,batch_size=150)                 \n",
    "        for k in range(len(models)) :\n",
    "            score.append((k,models[k].evaluate(valid[0],valid[1])))\n",
    "    std = {}\n",
    "    mean = {}\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        tmp = [s for s in score if s[0]==i]\n",
    "        std[i]=np.std([a[1][1] for a in tmp])\n",
    "        mean[i]=np.mean([a[1][1] for a in tmp])\n",
    "\n",
    "    for k in mean.keys():\n",
    "        print(\"Accruracy neural network\"+str(k) +\" : \"+str(mean[k]*100)+\"% +- \"+str(std[k]*100)+\"%\")\n",
    "    \n",
    "    print(\"--- %s minutes elapsed---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "    return models,score,mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cross_validation will take around : 24.0 mn\n"
     ]
    }
   ],
   "source": [
    "time_estimation_cross_val(80,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function multi_cross_val in module __main__:\n",
      "\n",
      "multi_cross_val(model_functions, x_train, y_train, nb_splits, nb_epochs)\n",
      "    This function allows to compare different models over a k-fold cross validation\n",
      "    Keyword arguments:\n",
      "    model_functions -- A list containing the functions returning the different models you want to test\n",
      "    x_train -- The train data\n",
      "    y_train -- The train target\n",
      "    nb_splits -- The number of folds \n",
      "    nb_epochs -- The number of epochs you want to train your network with \n",
      "                 /!\\ One validation will be a total of nb_folds*nb_epochs total number of epochs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(multi_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=10, activation=\"relu\", input_shape=(50, 300), padding=\"same\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "2688/2688 [==============================] - 3s 1ms/step - loss: 0.6907 - acc: 0.5387\n",
      "Epoch 2/80\n",
      "2688/2688 [==============================] - 1s 553us/step - loss: 0.6896 - acc: 0.5446\n",
      "Epoch 3/80\n",
      "2688/2688 [==============================] - 2s 558us/step - loss: 0.6603 - acc: 0.5536\n",
      "Epoch 4/80\n",
      "2688/2688 [==============================] - 1s 556us/step - loss: 0.5177 - acc: 0.7582\n",
      "Epoch 5/80\n",
      "2688/2688 [==============================] - 1s 549us/step - loss: 0.4184 - acc: 0.8460\n",
      "Epoch 6/80\n",
      "2688/2688 [==============================] - 2s 558us/step - loss: 0.3574 - acc: 0.8735\n",
      "Epoch 7/80\n",
      "2688/2688 [==============================] - 1s 556us/step - loss: 0.3278 - acc: 0.8854\n",
      "Epoch 8/80\n",
      "2688/2688 [==============================] - 2s 558us/step - loss: 0.2962 - acc: 0.8996\n",
      "Epoch 9/80\n",
      "2688/2688 [==============================] - 1s 556us/step - loss: 0.2724 - acc: 0.9033\n",
      "Epoch 10/80\n",
      "2688/2688 [==============================] - 1s 554us/step - loss: 0.2502 - acc: 0.9022\n",
      "Epoch 11/80\n",
      "2688/2688 [==============================] - 2s 559us/step - loss: 0.2408 - acc: 0.9111\n",
      "Epoch 12/80\n",
      "2688/2688 [==============================] - 1s 557us/step - loss: 0.2369 - acc: 0.9118\n",
      "Epoch 13/80\n",
      "2688/2688 [==============================] - 1s 554us/step - loss: 0.2118 - acc: 0.9170\n",
      "Epoch 14/80\n",
      "2688/2688 [==============================] - 2s 560us/step - loss: 0.1832 - acc: 0.9245\n",
      "Epoch 15/80\n",
      "2688/2688 [==============================] - 1s 555us/step - loss: 0.2094 - acc: 0.9193\n",
      "Epoch 16/80\n",
      "2688/2688 [==============================] - 2s 562us/step - loss: 0.1935 - acc: 0.9323\n",
      "Epoch 17/80\n",
      "2688/2688 [==============================] - 2s 562us/step - loss: 0.1849 - acc: 0.9345\n",
      "Epoch 18/80\n",
      "2688/2688 [==============================] - 1s 557us/step - loss: 0.1699 - acc: 0.9349\n",
      "Epoch 19/80\n",
      "2688/2688 [==============================] - 2s 562us/step - loss: 0.1652 - acc: 0.9342\n",
      "Epoch 20/80\n",
      "2688/2688 [==============================] - 2s 569us/step - loss: 0.1642 - acc: 0.9342\n",
      "Epoch 21/80\n",
      "2688/2688 [==============================] - 2s 559us/step - loss: 0.1523 - acc: 0.9401\n",
      "Epoch 22/80\n",
      "2688/2688 [==============================] - 2s 561us/step - loss: 0.1527 - acc: 0.9472\n",
      "Epoch 23/80\n",
      "2688/2688 [==============================] - 2s 568us/step - loss: 0.1591 - acc: 0.9382\n",
      "Epoch 24/80\n",
      "2688/2688 [==============================] - 2s 566us/step - loss: 0.1471 - acc: 0.9423\n",
      "Epoch 25/80\n",
      "2688/2688 [==============================] - 1s 558us/step - loss: 0.1217 - acc: 0.9520\n",
      "Epoch 26/80\n",
      "2688/2688 [==============================] - 2s 563us/step - loss: 0.1325 - acc: 0.9464\n",
      "Epoch 27/80\n",
      "2688/2688 [==============================] - 2s 566us/step - loss: 0.1451 - acc: 0.9542\n",
      "Epoch 28/80\n",
      "2688/2688 [==============================] - 2s 568us/step - loss: 0.1366 - acc: 0.9498\n",
      "Epoch 29/80\n",
      "2688/2688 [==============================] - 2s 571us/step - loss: 0.1224 - acc: 0.9475\n",
      "Epoch 30/80\n",
      "2688/2688 [==============================] - 2s 565us/step - loss: 0.1355 - acc: 0.9528\n",
      "Epoch 31/80\n",
      "2688/2688 [==============================] - 2s 568us/step - loss: 0.1225 - acc: 0.9528\n",
      "Epoch 32/80\n",
      "2688/2688 [==============================] - 2s 567us/step - loss: 0.1115 - acc: 0.9561\n",
      "Epoch 33/80\n",
      "2688/2688 [==============================] - 2s 564us/step - loss: 0.1149 - acc: 0.9568\n",
      "Epoch 34/80\n",
      "2688/2688 [==============================] - 2s 565us/step - loss: 0.1105 - acc: 0.9594\n",
      "Epoch 35/80\n",
      "2688/2688 [==============================] - 2s 569us/step - loss: 0.1235 - acc: 0.9513\n",
      "Epoch 36/80\n",
      "2688/2688 [==============================] - 2s 567us/step - loss: 0.1104 - acc: 0.9557\n",
      "Epoch 37/80\n",
      "2688/2688 [==============================] - 2s 565us/step - loss: 0.1093 - acc: 0.9546\n",
      "Epoch 38/80\n",
      "2688/2688 [==============================] - 2s 565us/step - loss: 0.1029 - acc: 0.9632\n",
      "Epoch 39/80\n",
      "2688/2688 [==============================] - 2s 564us/step - loss: 0.1199 - acc: 0.9568\n",
      "Epoch 40/80\n",
      "2688/2688 [==============================] - 2s 566us/step - loss: 0.1083 - acc: 0.9617\n",
      "Epoch 41/80\n",
      "2688/2688 [==============================] - 2s 566us/step - loss: 0.0854 - acc: 0.9617\n",
      "Epoch 42/80\n",
      "2688/2688 [==============================] - 2s 571us/step - loss: 0.1149 - acc: 0.9576\n",
      "Epoch 43/80\n",
      "2688/2688 [==============================] - 2s 569us/step - loss: 0.1122 - acc: 0.9594\n",
      "Epoch 44/80\n",
      "2688/2688 [==============================] - 2s 569us/step - loss: 0.0943 - acc: 0.9587\n",
      "Epoch 45/80\n",
      "2688/2688 [==============================] - 2s 578us/step - loss: 0.1039 - acc: 0.9606\n",
      "Epoch 46/80\n",
      "2688/2688 [==============================] - 2s 574us/step - loss: 0.0980 - acc: 0.9598\n",
      "Epoch 47/80\n",
      "2688/2688 [==============================] - 2s 572us/step - loss: 0.0970 - acc: 0.9632\n",
      "Epoch 48/80\n",
      "2688/2688 [==============================] - 2s 618us/step - loss: 0.0939 - acc: 0.9624\n",
      "Epoch 49/80\n",
      "2688/2688 [==============================] - 2s 627us/step - loss: 0.0834 - acc: 0.9676\n",
      "Epoch 50/80\n",
      "2688/2688 [==============================] - 2s 582us/step - loss: 0.0839 - acc: 0.9717\n",
      "Epoch 51/80\n",
      "2688/2688 [==============================] - 2s 589us/step - loss: 0.0862 - acc: 0.9676\n",
      "Epoch 52/80\n",
      "2688/2688 [==============================] - 2s 590us/step - loss: 0.0811 - acc: 0.9669\n",
      "Epoch 53/80\n",
      "2688/2688 [==============================] - 2s 589us/step - loss: 0.0807 - acc: 0.9714\n",
      "Epoch 54/80\n",
      "2688/2688 [==============================] - 2s 590us/step - loss: 0.0912 - acc: 0.9684\n",
      "Epoch 55/80\n",
      "2688/2688 [==============================] - 2s 594us/step - loss: 0.0897 - acc: 0.9632\n",
      "Epoch 56/80\n",
      "2688/2688 [==============================] - 2s 598us/step - loss: 0.0926 - acc: 0.9594\n",
      "Epoch 57/80\n",
      "2688/2688 [==============================] - 2s 597us/step - loss: 0.0911 - acc: 0.9598\n",
      "Epoch 58/80\n",
      "2688/2688 [==============================] - 2s 610us/step - loss: 0.0824 - acc: 0.9661\n",
      "Epoch 59/80\n",
      "2688/2688 [==============================] - 2s 602us/step - loss: 0.0762 - acc: 0.9699\n",
      "Epoch 60/80\n",
      "2688/2688 [==============================] - 2s 620us/step - loss: 0.0926 - acc: 0.9624\n",
      "Epoch 61/80\n",
      "2688/2688 [==============================] - 2s 609us/step - loss: 0.0796 - acc: 0.9650\n",
      "Epoch 62/80\n",
      "2688/2688 [==============================] - 2s 611us/step - loss: 0.0811 - acc: 0.9669\n",
      "Epoch 63/80\n",
      "2688/2688 [==============================] - 2s 615us/step - loss: 0.0821 - acc: 0.9661\n",
      "Epoch 64/80\n",
      "2688/2688 [==============================] - 2s 622us/step - loss: 0.0765 - acc: 0.9658\n",
      "Epoch 65/80\n",
      "2688/2688 [==============================] - 2s 619us/step - loss: 0.1005 - acc: 0.9632\n",
      "Epoch 66/80\n",
      "2688/2688 [==============================] - 2s 613us/step - loss: 0.0857 - acc: 0.9702\n",
      "Epoch 67/80\n",
      "2688/2688 [==============================] - 2s 613us/step - loss: 0.0733 - acc: 0.9710\n",
      "Epoch 68/80\n",
      "2688/2688 [==============================] - 2s 618us/step - loss: 0.0777 - acc: 0.9654\n",
      "Epoch 69/80\n",
      "2688/2688 [==============================] - 2s 630us/step - loss: 0.0924 - acc: 0.9621\n",
      "Epoch 70/80\n",
      "2688/2688 [==============================] - 2s 700us/step - loss: 0.0779 - acc: 0.9695\n",
      "Epoch 71/80\n",
      "2688/2688 [==============================] - 2s 709us/step - loss: 0.0824 - acc: 0.9695\n",
      "Epoch 72/80\n",
      "2688/2688 [==============================] - 2s 631us/step - loss: 0.0773 - acc: 0.9706\n",
      "Epoch 73/80\n",
      "2688/2688 [==============================] - 2s 626us/step - loss: 0.0704 - acc: 0.9702\n",
      "Epoch 74/80\n",
      "2688/2688 [==============================] - 2s 661us/step - loss: 0.0941 - acc: 0.9628\n",
      "Epoch 75/80\n",
      "2688/2688 [==============================] - 2s 706us/step - loss: 0.0851 - acc: 0.9688\n",
      "Epoch 76/80\n",
      "2688/2688 [==============================] - 2s 674us/step - loss: 0.0855 - acc: 0.9680\n",
      "Epoch 77/80\n",
      "2688/2688 [==============================] - 2s 612us/step - loss: 0.0732 - acc: 0.9669\n",
      "Epoch 78/80\n",
      "2688/2688 [==============================] - 2s 611us/step - loss: 0.0763 - acc: 0.9706\n",
      "Epoch 79/80\n",
      "2688/2688 [==============================] - 2s 701us/step - loss: 0.0762 - acc: 0.9676\n",
      "Epoch 80/80\n",
      "2688/2688 [==============================] - 2s 703us/step - loss: 0.0737 - acc: 0.9754\n",
      "Epoch 1/80\n",
      "2688/2688 [==============================] - 2s 639us/step - loss: 0.3658 - acc: 0.8769\n",
      "Epoch 2/80\n",
      "2688/2688 [==============================] - 2s 619us/step - loss: 0.2854 - acc: 0.8925\n",
      "Epoch 3/80\n",
      "2688/2688 [==============================] - 2s 656us/step - loss: 0.2555 - acc: 0.9066\n",
      "Epoch 4/80\n",
      "2688/2688 [==============================] - 2s 701us/step - loss: 0.2375 - acc: 0.9133\n",
      "Epoch 5/80\n",
      "2688/2688 [==============================] - 2s 679us/step - loss: 0.2336 - acc: 0.9170\n",
      "Epoch 6/80\n",
      "2688/2688 [==============================] - 2s 612us/step - loss: 0.2111 - acc: 0.9096\n",
      "Epoch 7/80\n",
      "2688/2688 [==============================] - 2s 615us/step - loss: 0.2112 - acc: 0.9156\n",
      "Epoch 8/80\n",
      "2688/2688 [==============================] - 2s 621us/step - loss: 0.1951 - acc: 0.9245\n",
      "Epoch 9/80\n",
      "2688/2688 [==============================] - 2s 612us/step - loss: 0.1882 - acc: 0.9230\n",
      "Epoch 10/80\n",
      "2688/2688 [==============================] - 2s 619us/step - loss: 0.1854 - acc: 0.9237\n",
      "Epoch 11/80\n",
      "2688/2688 [==============================] - 2s 622us/step - loss: 0.1779 - acc: 0.9282\n",
      "Epoch 12/80\n",
      "2688/2688 [==============================] - 2s 631us/step - loss: 0.1787 - acc: 0.9293\n",
      "Epoch 13/80\n",
      "2688/2688 [==============================] - 2s 631us/step - loss: 0.1664 - acc: 0.9371\n",
      "Epoch 14/80\n",
      "2688/2688 [==============================] - 2s 630us/step - loss: 0.1560 - acc: 0.9412\n",
      "Epoch 15/80\n",
      "2688/2688 [==============================] - 2s 633us/step - loss: 0.1524 - acc: 0.9342\n",
      "Epoch 16/80\n",
      "2688/2688 [==============================] - 2s 639us/step - loss: 0.1597 - acc: 0.9423\n",
      "Epoch 17/80\n",
      "2688/2688 [==============================] - 2s 650us/step - loss: 0.1427 - acc: 0.9412\n",
      "Epoch 18/80\n",
      "2688/2688 [==============================] - 2s 641us/step - loss: 0.1515 - acc: 0.9427\n",
      "Epoch 19/80\n",
      "2688/2688 [==============================] - 2s 645us/step - loss: 0.1366 - acc: 0.9446\n",
      "Epoch 20/80\n",
      "2688/2688 [==============================] - 2s 643us/step - loss: 0.1345 - acc: 0.9487\n",
      "Epoch 21/80\n",
      "2688/2688 [==============================] - 2s 649us/step - loss: 0.1336 - acc: 0.9457\n",
      "Epoch 22/80\n",
      "2688/2688 [==============================] - 2s 643us/step - loss: 0.1346 - acc: 0.9483\n",
      "Epoch 23/80\n",
      "2688/2688 [==============================] - 2s 649us/step - loss: 0.1256 - acc: 0.9490\n",
      "Epoch 24/80\n",
      "2688/2688 [==============================] - 2s 647us/step - loss: 0.1187 - acc: 0.9528\n",
      "Epoch 25/80\n",
      "2688/2688 [==============================] - 2s 650us/step - loss: 0.1184 - acc: 0.9542\n",
      "Epoch 26/80\n",
      "2688/2688 [==============================] - 2s 653us/step - loss: 0.1238 - acc: 0.9494\n",
      "Epoch 27/80\n",
      "2688/2688 [==============================] - 2s 659us/step - loss: 0.1197 - acc: 0.9487\n",
      "Epoch 28/80\n",
      "2688/2688 [==============================] - 2s 664us/step - loss: 0.1185 - acc: 0.9498\n",
      "Epoch 29/80\n",
      "2688/2688 [==============================] - 2s 658us/step - loss: 0.1194 - acc: 0.9520\n",
      "Epoch 30/80\n",
      "2688/2688 [==============================] - 2s 665us/step - loss: 0.1226 - acc: 0.9550\n",
      "Epoch 31/80\n",
      "2688/2688 [==============================] - 2s 659us/step - loss: 0.1166 - acc: 0.9546\n",
      "Epoch 32/80\n",
      "2688/2688 [==============================] - 2s 656us/step - loss: 0.1111 - acc: 0.9550\n",
      "Epoch 33/80\n",
      "2688/2688 [==============================] - 2s 661us/step - loss: 0.1008 - acc: 0.9594\n",
      "Epoch 34/80\n",
      "2688/2688 [==============================] - 2s 656us/step - loss: 0.0940 - acc: 0.9609\n",
      "Epoch 35/80\n",
      "2688/2688 [==============================] - 2s 673us/step - loss: 0.1184 - acc: 0.9565\n",
      "Epoch 36/80\n",
      "2688/2688 [==============================] - 2s 656us/step - loss: 0.1041 - acc: 0.9550\n",
      "Epoch 37/80\n",
      "2688/2688 [==============================] - 2s 672us/step - loss: 0.0964 - acc: 0.9580\n",
      "Epoch 38/80\n",
      "2688/2688 [==============================] - 2s 659us/step - loss: 0.1034 - acc: 0.9598\n",
      "Epoch 39/80\n",
      "2688/2688 [==============================] - 2s 663us/step - loss: 0.1003 - acc: 0.9635\n",
      "Epoch 40/80\n",
      "2688/2688 [==============================] - 2s 665us/step - loss: 0.1036 - acc: 0.9621\n",
      "Epoch 41/80\n",
      "2688/2688 [==============================] - 2s 663us/step - loss: 0.0940 - acc: 0.9572\n",
      "Epoch 42/80\n",
      "2688/2688 [==============================] - 2s 669us/step - loss: 0.0956 - acc: 0.9606\n",
      "Epoch 43/80\n",
      "2688/2688 [==============================] - 2s 669us/step - loss: 0.0973 - acc: 0.9583\n",
      "Epoch 44/80\n",
      "2688/2688 [==============================] - 2s 675us/step - loss: 0.0997 - acc: 0.9594\n",
      "Epoch 45/80\n",
      "2688/2688 [==============================] - 2s 663us/step - loss: 0.0999 - acc: 0.9598\n",
      "Epoch 46/80\n",
      "2688/2688 [==============================] - 2s 658us/step - loss: 0.0908 - acc: 0.9617\n",
      "Epoch 47/80\n",
      "2688/2688 [==============================] - 2s 673us/step - loss: 0.0973 - acc: 0.9647\n",
      "Epoch 48/80\n",
      "2688/2688 [==============================] - 2s 673us/step - loss: 0.0942 - acc: 0.9580\n",
      "Epoch 49/80\n",
      "2688/2688 [==============================] - 2s 671us/step - loss: 0.1023 - acc: 0.9594\n",
      "Epoch 50/80\n",
      "2688/2688 [==============================] - 2s 671us/step - loss: 0.1046 - acc: 0.9628\n",
      "Epoch 51/80\n",
      "2688/2688 [==============================] - 2s 682us/step - loss: 0.0818 - acc: 0.9658\n",
      "Epoch 52/80\n",
      "2688/2688 [==============================] - 2s 671us/step - loss: 0.0844 - acc: 0.9606\n",
      "Epoch 53/80\n",
      "2688/2688 [==============================] - 2s 671us/step - loss: 0.0792 - acc: 0.9691\n",
      "Epoch 54/80\n",
      "2688/2688 [==============================] - 2s 668us/step - loss: 0.0972 - acc: 0.9572\n",
      "Epoch 55/80\n",
      "2688/2688 [==============================] - 2s 679us/step - loss: 0.0916 - acc: 0.9650\n",
      "Epoch 56/80\n",
      "2688/2688 [==============================] - 2s 664us/step - loss: 0.0909 - acc: 0.9650\n",
      "Epoch 57/80\n",
      "2688/2688 [==============================] - 2s 680us/step - loss: 0.0920 - acc: 0.9591\n",
      "Epoch 58/80\n",
      "2688/2688 [==============================] - 2s 677us/step - loss: 0.0944 - acc: 0.9639\n",
      "Epoch 59/80\n",
      "2688/2688 [==============================] - 2s 689us/step - loss: 0.0861 - acc: 0.9647\n",
      "Epoch 60/80\n",
      "2688/2688 [==============================] - 2s 681us/step - loss: 0.0975 - acc: 0.9609\n",
      "Epoch 61/80\n",
      "2688/2688 [==============================] - 2s 670us/step - loss: 0.0836 - acc: 0.9632\n",
      "Epoch 62/80\n",
      "2688/2688 [==============================] - 2s 677us/step - loss: 0.0958 - acc: 0.9647\n",
      "Epoch 63/80\n",
      "2688/2688 [==============================] - 2s 678us/step - loss: 0.0850 - acc: 0.9635\n",
      "Epoch 64/80\n",
      "2688/2688 [==============================] - 2s 682us/step - loss: 0.0875 - acc: 0.9654\n",
      "Epoch 65/80\n",
      "2688/2688 [==============================] - 2s 687us/step - loss: 0.0870 - acc: 0.9602\n",
      "Epoch 66/80\n",
      "2688/2688 [==============================] - 2s 676us/step - loss: 0.0925 - acc: 0.9598\n",
      "Epoch 67/80\n",
      "2688/2688 [==============================] - 2s 682us/step - loss: 0.0883 - acc: 0.9647\n",
      "Epoch 68/80\n",
      "2688/2688 [==============================] - 2s 682us/step - loss: 0.0771 - acc: 0.9654\n",
      "Epoch 69/80\n",
      "2688/2688 [==============================] - 2s 674us/step - loss: 0.0922 - acc: 0.9632\n",
      "Epoch 70/80\n",
      "2688/2688 [==============================] - 2s 676us/step - loss: 0.0812 - acc: 0.9639\n",
      "Epoch 71/80\n",
      "2688/2688 [==============================] - 2s 687us/step - loss: 0.0874 - acc: 0.9676\n",
      "Epoch 72/80\n",
      "2688/2688 [==============================] - 2s 683us/step - loss: 0.0856 - acc: 0.9680\n",
      "Epoch 73/80\n",
      "2688/2688 [==============================] - 2s 686us/step - loss: 0.0812 - acc: 0.9680\n",
      "Epoch 74/80\n",
      "2688/2688 [==============================] - 2s 682us/step - loss: 0.0860 - acc: 0.9665\n",
      "Epoch 75/80\n",
      "2688/2688 [==============================] - 2s 693us/step - loss: 0.0872 - acc: 0.9621\n",
      "Epoch 76/80\n",
      "2688/2688 [==============================] - 2s 682us/step - loss: 0.0783 - acc: 0.9680\n",
      "Epoch 77/80\n",
      "2688/2688 [==============================] - 2s 685us/step - loss: 0.0869 - acc: 0.9661\n",
      "Epoch 78/80\n",
      "2688/2688 [==============================] - 2s 680us/step - loss: 0.0858 - acc: 0.9684\n",
      "Epoch 79/80\n",
      "2688/2688 [==============================] - 2s 688us/step - loss: 0.0794 - acc: 0.9654\n",
      "Epoch 80/80\n",
      "2688/2688 [==============================] - 2s 709us/step - loss: 0.0743 - acc: 0.9680\n",
      "2689/2689 [==============================] - 2s 656us/step\n",
      "Epoch 1/80\n",
      "2689/2689 [==============================] - 4s 1ms/step - loss: 0.6915 - acc: 0.5377\n",
      "Epoch 2/80\n",
      "2689/2689 [==============================] - 2s 657us/step - loss: 0.6894 - acc: 0.5418\n",
      "Epoch 3/80\n",
      "2689/2689 [==============================] - 2s 651us/step - loss: 0.6847 - acc: 0.5537\n",
      "Epoch 4/80\n",
      "2689/2689 [==============================] - 2s 659us/step - loss: 0.5490 - acc: 0.7211\n",
      "Epoch 5/80\n",
      "2689/2689 [==============================] - 2s 670us/step - loss: 0.4617 - acc: 0.8100\n",
      "Epoch 6/80\n",
      "2689/2689 [==============================] - 2s 668us/step - loss: 0.3874 - acc: 0.8457\n",
      "Epoch 7/80\n",
      "2689/2689 [==============================] - 2s 672us/step - loss: 0.3541 - acc: 0.8669\n",
      "Epoch 8/80\n",
      "2689/2689 [==============================] - 2s 677us/step - loss: 0.3097 - acc: 0.8888\n",
      "Epoch 9/80\n",
      "2689/2689 [==============================] - 2s 681us/step - loss: 0.2682 - acc: 0.8925\n",
      "Epoch 10/80\n",
      "2689/2689 [==============================] - 2s 674us/step - loss: 0.2531 - acc: 0.9022\n",
      "Epoch 11/80\n",
      "2689/2689 [==============================] - 2s 677us/step - loss: 0.2428 - acc: 0.9078\n",
      "Epoch 12/80\n",
      "2689/2689 [==============================] - 2s 685us/step - loss: 0.2283 - acc: 0.9100\n",
      "Epoch 13/80\n",
      "2689/2689 [==============================] - 2s 693us/step - loss: 0.2164 - acc: 0.9130\n",
      "Epoch 14/80\n",
      "2689/2689 [==============================] - 2s 705us/step - loss: 0.1975 - acc: 0.9156\n",
      "Epoch 15/80\n",
      "2689/2689 [==============================] - 2s 711us/step - loss: 0.1884 - acc: 0.9212\n",
      "Epoch 16/80\n",
      "2689/2689 [==============================] - 2s 700us/step - loss: 0.1821 - acc: 0.9245\n",
      "Epoch 17/80\n",
      "2689/2689 [==============================] - 2s 702us/step - loss: 0.1953 - acc: 0.9226\n",
      "Epoch 18/80\n",
      "2689/2689 [==============================] - 2s 724us/step - loss: 0.1779 - acc: 0.9319\n",
      "Epoch 19/80\n",
      "2689/2689 [==============================] - 2s 717us/step - loss: 0.1605 - acc: 0.9345\n",
      "Epoch 20/80\n",
      "2689/2689 [==============================] - 2s 713us/step - loss: 0.1740 - acc: 0.9357\n",
      "Epoch 21/80\n",
      "2689/2689 [==============================] - 2s 696us/step - loss: 0.1457 - acc: 0.9446\n",
      "Epoch 22/80\n",
      "2689/2689 [==============================] - 2s 707us/step - loss: 0.1465 - acc: 0.9420\n",
      "Epoch 23/80\n",
      "2689/2689 [==============================] - 2s 706us/step - loss: 0.1525 - acc: 0.9372\n",
      "Epoch 24/80\n",
      "2689/2689 [==============================] - 2s 708us/step - loss: 0.1402 - acc: 0.9453\n",
      "Epoch 25/80\n",
      "2689/2689 [==============================] - 2s 688us/step - loss: 0.1322 - acc: 0.9472\n",
      "Epoch 26/80\n",
      "2689/2689 [==============================] - 2s 696us/step - loss: 0.1463 - acc: 0.9398\n",
      "Epoch 27/80\n",
      "2689/2689 [==============================] - 2s 697us/step - loss: 0.1457 - acc: 0.9468\n",
      "Epoch 28/80\n",
      "2689/2689 [==============================] - 2s 714us/step - loss: 0.1195 - acc: 0.9502\n",
      "Epoch 29/80\n",
      "2689/2689 [==============================] - 2s 685us/step - loss: 0.1153 - acc: 0.9565\n",
      "Epoch 30/80\n",
      "2689/2689 [==============================] - 2s 702us/step - loss: 0.1172 - acc: 0.9535\n",
      "Epoch 31/80\n",
      "2689/2689 [==============================] - 2s 700us/step - loss: 0.1275 - acc: 0.9565\n",
      "Epoch 32/80\n",
      "2689/2689 [==============================] - 2s 688us/step - loss: 0.1143 - acc: 0.9569\n",
      "Epoch 33/80\n",
      "2689/2689 [==============================] - 2s 701us/step - loss: 0.1216 - acc: 0.9476\n",
      "Epoch 34/80\n",
      "2689/2689 [==============================] - 2s 687us/step - loss: 0.1134 - acc: 0.9546\n",
      "Epoch 35/80\n",
      "2689/2689 [==============================] - 2s 693us/step - loss: 0.1066 - acc: 0.9617\n",
      "Epoch 36/80\n",
      "2689/2689 [==============================] - 2s 699us/step - loss: 0.1112 - acc: 0.9554\n",
      "Epoch 37/80\n",
      "2689/2689 [==============================] - 2s 701us/step - loss: 0.1342 - acc: 0.9535\n",
      "Epoch 38/80\n",
      "2689/2689 [==============================] - 2s 707us/step - loss: 0.1026 - acc: 0.9632\n",
      "Epoch 39/80\n",
      "2689/2689 [==============================] - 2s 699us/step - loss: 0.1160 - acc: 0.9528\n",
      "Epoch 40/80\n",
      "2689/2689 [==============================] - 2s 701us/step - loss: 0.1079 - acc: 0.9598\n",
      "Epoch 41/80\n",
      "2689/2689 [==============================] - 2s 706us/step - loss: 0.1087 - acc: 0.9576\n",
      "Epoch 42/80\n",
      "2689/2689 [==============================] - 2s 713us/step - loss: 0.0876 - acc: 0.9699\n",
      "Epoch 43/80\n",
      "2689/2689 [==============================] - 2s 705us/step - loss: 0.1139 - acc: 0.9554\n",
      "Epoch 44/80\n",
      "2689/2689 [==============================] - 2s 712us/step - loss: 0.1028 - acc: 0.9639\n",
      "Epoch 45/80\n",
      "2689/2689 [==============================] - 2s 706us/step - loss: 0.0985 - acc: 0.9617\n",
      "Epoch 46/80\n",
      "2689/2689 [==============================] - 2s 714us/step - loss: 0.0836 - acc: 0.9710\n",
      "Epoch 47/80\n",
      "2689/2689 [==============================] - 2s 722us/step - loss: 0.0944 - acc: 0.9628\n",
      "Epoch 48/80\n",
      "2689/2689 [==============================] - 2s 712us/step - loss: 0.1077 - acc: 0.9569\n",
      "Epoch 49/80\n",
      "2689/2689 [==============================] - 2s 705us/step - loss: 0.0885 - acc: 0.9658\n",
      "Epoch 50/80\n",
      "2689/2689 [==============================] - 2s 707us/step - loss: 0.0884 - acc: 0.9647\n",
      "Epoch 51/80\n",
      "2689/2689 [==============================] - 2s 724us/step - loss: 0.0854 - acc: 0.9628\n",
      "Epoch 52/80\n",
      "2689/2689 [==============================] - 2s 724us/step - loss: 0.0860 - acc: 0.9673\n",
      "Epoch 53/80\n",
      "2689/2689 [==============================] - 2s 707us/step - loss: 0.0851 - acc: 0.9650\n",
      "Epoch 54/80\n",
      "2689/2689 [==============================] - 2s 700us/step - loss: 0.0762 - acc: 0.9647\n",
      "Epoch 55/80\n",
      "2689/2689 [==============================] - 2s 708us/step - loss: 0.0867 - acc: 0.9684\n",
      "Epoch 56/80\n",
      "2689/2689 [==============================] - 2s 698us/step - loss: 0.0741 - acc: 0.9650\n",
      "Epoch 57/80\n",
      "2689/2689 [==============================] - 2s 710us/step - loss: 0.0674 - acc: 0.9695\n",
      "Epoch 58/80\n",
      "2689/2689 [==============================] - 2s 715us/step - loss: 0.0878 - acc: 0.9647\n",
      "Epoch 59/80\n",
      "2689/2689 [==============================] - 2s 703us/step - loss: 0.0810 - acc: 0.9650\n",
      "Epoch 60/80\n",
      "2689/2689 [==============================] - 2s 708us/step - loss: 0.0833 - acc: 0.9699\n",
      "Epoch 61/80\n",
      "2689/2689 [==============================] - 2s 701us/step - loss: 0.0864 - acc: 0.9639\n",
      "Epoch 62/80\n",
      "2689/2689 [==============================] - 2s 714us/step - loss: 0.0779 - acc: 0.9680\n",
      "Epoch 63/80\n",
      "2689/2689 [==============================] - 2s 710us/step - loss: 0.0796 - acc: 0.9669\n",
      "Epoch 64/80\n",
      "2689/2689 [==============================] - 2s 717us/step - loss: 0.0892 - acc: 0.9639\n",
      "Epoch 65/80\n",
      "2689/2689 [==============================] - 2s 696us/step - loss: 0.0724 - acc: 0.9673\n",
      "Epoch 66/80\n",
      "2689/2689 [==============================] - 2s 698us/step - loss: 0.0683 - acc: 0.9773\n",
      "Epoch 67/80\n",
      "2689/2689 [==============================] - 2s 698us/step - loss: 0.0835 - acc: 0.9643\n",
      "Epoch 68/80\n",
      "2689/2689 [==============================] - 2s 691us/step - loss: 0.0772 - acc: 0.9710\n",
      "Epoch 69/80\n",
      "2689/2689 [==============================] - 2s 704us/step - loss: 0.0800 - acc: 0.9688\n",
      "Epoch 70/80\n",
      "2689/2689 [==============================] - 2s 702us/step - loss: 0.0748 - acc: 0.9658\n",
      "Epoch 71/80\n",
      "2689/2689 [==============================] - 2s 713us/step - loss: 0.0714 - acc: 0.9654\n",
      "Epoch 72/80\n",
      "2689/2689 [==============================] - 2s 701us/step - loss: 0.0759 - acc: 0.9684\n",
      "Epoch 73/80\n",
      "2689/2689 [==============================] - 2s 704us/step - loss: 0.0707 - acc: 0.9695\n",
      "Epoch 74/80\n",
      "2689/2689 [==============================] - 2s 709us/step - loss: 0.0650 - acc: 0.9755\n",
      "Epoch 75/80\n",
      "2689/2689 [==============================] - 2s 715us/step - loss: 0.0763 - acc: 0.9710\n",
      "Epoch 76/80\n",
      "2689/2689 [==============================] - 2s 703us/step - loss: 0.0875 - acc: 0.9665\n",
      "Epoch 77/80\n",
      "2689/2689 [==============================] - 2s 705us/step - loss: 0.0697 - acc: 0.9736\n",
      "Epoch 78/80\n",
      "2689/2689 [==============================] - 2s 704us/step - loss: 0.0754 - acc: 0.9676\n",
      "Epoch 79/80\n",
      "2689/2689 [==============================] - 2s 709us/step - loss: 0.0657 - acc: 0.9706\n",
      "Epoch 80/80\n",
      "2689/2689 [==============================] - 2s 705us/step - loss: 0.0701 - acc: 0.9751\n",
      "Epoch 1/80\n",
      "2688/2688 [==============================] - 2s 711us/step - loss: 0.4088 - acc: 0.8787\n",
      "Epoch 2/80\n",
      "2688/2688 [==============================] - 2s 730us/step - loss: 0.2846 - acc: 0.8914\n",
      "Epoch 3/80\n",
      "2688/2688 [==============================] - 2s 735us/step - loss: 0.2628 - acc: 0.8988\n",
      "Epoch 4/80\n",
      "2688/2688 [==============================] - 2s 720us/step - loss: 0.2523 - acc: 0.8955\n",
      "Epoch 5/80\n",
      "2688/2688 [==============================] - 2s 711us/step - loss: 0.2377 - acc: 0.9077\n",
      "Epoch 6/80\n",
      "2688/2688 [==============================] - 2s 728us/step - loss: 0.2336 - acc: 0.9077\n",
      "Epoch 7/80\n",
      "2688/2688 [==============================] - 2s 739us/step - loss: 0.2207 - acc: 0.9089\n",
      "Epoch 8/80\n",
      "2688/2688 [==============================] - 2s 716us/step - loss: 0.2103 - acc: 0.9182\n",
      "Epoch 9/80\n",
      "2688/2688 [==============================] - 2s 708us/step - loss: 0.2073 - acc: 0.9237\n",
      "Epoch 10/80\n",
      "2688/2688 [==============================] - 2s 733us/step - loss: 0.1910 - acc: 0.9219\n",
      "Epoch 11/80\n",
      "2688/2688 [==============================] - 2s 734us/step - loss: 0.1910 - acc: 0.9185\n",
      "Epoch 12/80\n",
      "2688/2688 [==============================] - 2s 708us/step - loss: 0.1817 - acc: 0.9308\n",
      "Epoch 13/80\n",
      "2688/2688 [==============================] - 2s 758us/step - loss: 0.1727 - acc: 0.9260\n",
      "Epoch 14/80\n",
      "2688/2688 [==============================] - 2s 837us/step - loss: 0.1795 - acc: 0.9301\n",
      "Epoch 15/80\n",
      "2688/2688 [==============================] - 2s 863us/step - loss: 0.1725 - acc: 0.9315\n",
      "Epoch 16/80\n",
      "2688/2688 [==============================] - 2s 788us/step - loss: 0.1592 - acc: 0.9353\n",
      "Epoch 17/80\n",
      "2688/2688 [==============================] - 2s 724us/step - loss: 0.1642 - acc: 0.9356\n",
      "Epoch 18/80\n",
      "2688/2688 [==============================] - 2s 738us/step - loss: 0.1626 - acc: 0.9386\n",
      "Epoch 19/80\n",
      "2688/2688 [==============================] - 2s 748us/step - loss: 0.1440 - acc: 0.9427\n",
      "Epoch 20/80\n",
      "2688/2688 [==============================] - 2s 736us/step - loss: 0.1419 - acc: 0.9438\n",
      "Epoch 21/80\n",
      "2688/2688 [==============================] - 2s 737us/step - loss: 0.1409 - acc: 0.9412\n",
      "Epoch 22/80\n",
      "2688/2688 [==============================] - 2s 729us/step - loss: 0.1312 - acc: 0.9483\n",
      "Epoch 23/80\n",
      "2688/2688 [==============================] - 2s 718us/step - loss: 0.1433 - acc: 0.9446\n",
      "Epoch 24/80\n",
      "2688/2688 [==============================] - 2s 727us/step - loss: 0.1241 - acc: 0.9468\n",
      "Epoch 25/80\n",
      "2688/2688 [==============================] - 2s 741us/step - loss: 0.1251 - acc: 0.9483\n",
      "Epoch 26/80\n",
      "2688/2688 [==============================] - 2s 742us/step - loss: 0.1123 - acc: 0.9539\n",
      "Epoch 27/80\n",
      "2688/2688 [==============================] - 2s 745us/step - loss: 0.1214 - acc: 0.9464\n",
      "Epoch 28/80\n",
      "2688/2688 [==============================] - 2s 750us/step - loss: 0.1248 - acc: 0.9505\n",
      "Epoch 29/80\n",
      "2688/2688 [==============================] - 2s 729us/step - loss: 0.1256 - acc: 0.9446\n",
      "Epoch 30/80\n",
      "2688/2688 [==============================] - 2s 748us/step - loss: 0.1011 - acc: 0.9568\n",
      "Epoch 31/80\n",
      "2688/2688 [==============================] - 2s 751us/step - loss: 0.1155 - acc: 0.9554\n",
      "Epoch 32/80\n",
      "2688/2688 [==============================] - 2s 745us/step - loss: 0.1035 - acc: 0.9550\n",
      "Epoch 33/80\n",
      "2688/2688 [==============================] - 2s 736us/step - loss: 0.0971 - acc: 0.9568\n",
      "Epoch 34/80\n",
      "2688/2688 [==============================] - 2s 740us/step - loss: 0.1116 - acc: 0.9539\n",
      "Epoch 35/80\n",
      "2688/2688 [==============================] - 2s 730us/step - loss: 0.1078 - acc: 0.9565\n",
      "Epoch 36/80\n",
      "2688/2688 [==============================] - 2s 733us/step - loss: 0.0946 - acc: 0.9628\n",
      "Epoch 37/80\n",
      "2688/2688 [==============================] - 2s 743us/step - loss: 0.1069 - acc: 0.9565\n",
      "Epoch 38/80\n",
      "2688/2688 [==============================] - 2s 732us/step - loss: 0.0996 - acc: 0.9583\n",
      "Epoch 39/80\n",
      "2688/2688 [==============================] - 2s 741us/step - loss: 0.1003 - acc: 0.9572\n",
      "Epoch 40/80\n",
      "2688/2688 [==============================] - 2s 762us/step - loss: 0.0993 - acc: 0.9628\n",
      "Epoch 41/80\n",
      "2688/2688 [==============================] - 2s 757us/step - loss: 0.1001 - acc: 0.9568\n",
      "Epoch 42/80\n",
      "2688/2688 [==============================] - 2s 754us/step - loss: 0.0861 - acc: 0.9624\n",
      "Epoch 43/80\n",
      "2688/2688 [==============================] - 2s 759us/step - loss: 0.0962 - acc: 0.9632\n",
      "Epoch 44/80\n",
      "2688/2688 [==============================] - 2s 735us/step - loss: 0.0920 - acc: 0.9613\n",
      "Epoch 45/80\n",
      "2688/2688 [==============================] - 2s 734us/step - loss: 0.0904 - acc: 0.9632\n",
      "Epoch 46/80\n",
      "2688/2688 [==============================] - 2s 736us/step - loss: 0.0949 - acc: 0.9635\n",
      "Epoch 47/80\n",
      "2688/2688 [==============================] - 2s 747us/step - loss: 0.0900 - acc: 0.9628\n",
      "Epoch 48/80\n",
      "2688/2688 [==============================] - 2s 877us/step - loss: 0.0891 - acc: 0.9661\n",
      "Epoch 49/80\n",
      "2688/2688 [==============================] - 2s 880us/step - loss: 0.0856 - acc: 0.9628\n",
      "Epoch 50/80\n",
      "2688/2688 [==============================] - 2s 755us/step - loss: 0.0856 - acc: 0.9628\n",
      "Epoch 51/80\n",
      "2688/2688 [==============================] - 2s 749us/step - loss: 0.0790 - acc: 0.9628\n",
      "Epoch 52/80\n",
      "2688/2688 [==============================] - 2s 732us/step - loss: 0.0896 - acc: 0.9624\n",
      "Epoch 53/80\n",
      "2688/2688 [==============================] - 2s 740us/step - loss: 0.0772 - acc: 0.9643\n",
      "Epoch 54/80\n",
      "2688/2688 [==============================] - 2s 741us/step - loss: 0.0850 - acc: 0.9606\n",
      "Epoch 55/80\n",
      "2688/2688 [==============================] - 2s 742us/step - loss: 0.0826 - acc: 0.9621\n",
      "Epoch 56/80\n",
      "2688/2688 [==============================] - 2s 757us/step - loss: 0.0789 - acc: 0.9624\n",
      "Epoch 57/80\n",
      "2688/2688 [==============================] - 2s 772us/step - loss: 0.0830 - acc: 0.9635\n",
      "Epoch 58/80\n",
      "2688/2688 [==============================] - 2s 751us/step - loss: 0.0931 - acc: 0.9654\n",
      "Epoch 59/80\n",
      "2688/2688 [==============================] - 2s 741us/step - loss: 0.0895 - acc: 0.9661\n",
      "Epoch 60/80\n",
      "2688/2688 [==============================] - 2s 747us/step - loss: 0.0807 - acc: 0.9613\n",
      "Epoch 61/80\n",
      "2688/2688 [==============================] - 2s 754us/step - loss: 0.0716 - acc: 0.9669\n",
      "Epoch 62/80\n",
      "2688/2688 [==============================] - 2s 758us/step - loss: 0.0773 - acc: 0.9635\n",
      "Epoch 63/80\n",
      "2688/2688 [==============================] - 2s 766us/step - loss: 0.0792 - acc: 0.9654\n",
      "Epoch 64/80\n",
      "2688/2688 [==============================] - 2s 763us/step - loss: 0.0802 - acc: 0.9635\n",
      "Epoch 65/80\n",
      "2688/2688 [==============================] - 2s 766us/step - loss: 0.0925 - acc: 0.9617\n",
      "Epoch 66/80\n",
      "2688/2688 [==============================] - 2s 751us/step - loss: 0.0821 - acc: 0.9650\n",
      "Epoch 67/80\n",
      "2688/2688 [==============================] - 2s 762us/step - loss: 0.0784 - acc: 0.9639\n",
      "Epoch 68/80\n",
      "2688/2688 [==============================] - 2s 761us/step - loss: 0.0864 - acc: 0.9647\n",
      "Epoch 69/80\n",
      "2688/2688 [==============================] - 2s 748us/step - loss: 0.0794 - acc: 0.9676\n",
      "Epoch 70/80\n",
      "2688/2688 [==============================] - 2s 776us/step - loss: 0.0814 - acc: 0.9676\n",
      "Epoch 71/80\n",
      "2688/2688 [==============================] - 2s 763us/step - loss: 0.0774 - acc: 0.9669\n",
      "Epoch 72/80\n",
      "2688/2688 [==============================] - 2s 773us/step - loss: 0.0770 - acc: 0.9661\n",
      "Epoch 73/80\n",
      "2688/2688 [==============================] - 2s 758us/step - loss: 0.0769 - acc: 0.9624\n",
      "Epoch 74/80\n",
      "2688/2688 [==============================] - 2s 748us/step - loss: 0.0767 - acc: 0.9699\n",
      "Epoch 75/80\n",
      "2688/2688 [==============================] - 2s 772us/step - loss: 0.0721 - acc: 0.9673\n",
      "Epoch 76/80\n",
      "2688/2688 [==============================] - 2s 760us/step - loss: 0.0724 - acc: 0.9639\n",
      "Epoch 77/80\n",
      "2688/2688 [==============================] - 2s 781us/step - loss: 0.0684 - acc: 0.9658\n",
      "Epoch 78/80\n",
      "2688/2688 [==============================] - 2s 764us/step - loss: 0.0724 - acc: 0.9684\n",
      "Epoch 79/80\n",
      "2688/2688 [==============================] - 2s 765us/step - loss: 0.0808 - acc: 0.9661\n",
      "Epoch 80/80\n",
      "2688/2688 [==============================] - 2s 758us/step - loss: 0.0783 - acc: 0.9647\n",
      "2688/2688 [==============================] - 2s 564us/step\n",
      "Epoch 1/80\n",
      "2689/2689 [==============================] - 4s 1ms/step - loss: 0.6901 - acc: 0.5448\n",
      "Epoch 2/80\n",
      "2689/2689 [==============================] - 2s 734us/step - loss: 0.6898 - acc: 0.5396\n",
      "Epoch 3/80\n",
      "2689/2689 [==============================] - 2s 729us/step - loss: 0.6495 - acc: 0.6318\n",
      "Epoch 4/80\n",
      "2689/2689 [==============================] - 2s 729us/step - loss: 0.5017 - acc: 0.7921\n",
      "Epoch 5/80\n",
      "2689/2689 [==============================] - 2s 732us/step - loss: 0.3983 - acc: 0.8457\n",
      "Epoch 6/80\n",
      "2689/2689 [==============================] - 2s 739us/step - loss: 0.3584 - acc: 0.8702\n",
      "Epoch 7/80\n",
      "2689/2689 [==============================] - 2s 731us/step - loss: 0.3121 - acc: 0.8810\n",
      "Epoch 8/80\n",
      "2689/2689 [==============================] - 2s 725us/step - loss: 0.2796 - acc: 0.8922\n",
      "Epoch 9/80\n",
      "2689/2689 [==============================] - 2s 740us/step - loss: 0.2672 - acc: 0.9022\n",
      "Epoch 10/80\n",
      "2689/2689 [==============================] - 2s 739us/step - loss: 0.2638 - acc: 0.8988\n",
      "Epoch 11/80\n",
      "2689/2689 [==============================] - 2s 744us/step - loss: 0.2369 - acc: 0.9081\n",
      "Epoch 12/80\n",
      "2689/2689 [==============================] - 2s 738us/step - loss: 0.2352 - acc: 0.9015\n",
      "Epoch 13/80\n",
      "2689/2689 [==============================] - 2s 739us/step - loss: 0.2085 - acc: 0.9186\n",
      "Epoch 14/80\n",
      "2689/2689 [==============================] - 2s 748us/step - loss: 0.1983 - acc: 0.9186\n",
      "Epoch 15/80\n",
      "2689/2689 [==============================] - 2s 742us/step - loss: 0.2053 - acc: 0.9215\n",
      "Epoch 16/80\n",
      "2689/2689 [==============================] - 2s 734us/step - loss: 0.1885 - acc: 0.9241\n",
      "Epoch 17/80\n",
      "2689/2689 [==============================] - 2s 748us/step - loss: 0.1888 - acc: 0.9219\n",
      "Epoch 18/80\n",
      "2689/2689 [==============================] - 2s 738us/step - loss: 0.1850 - acc: 0.9282\n",
      "Epoch 19/80\n",
      "2689/2689 [==============================] - 2s 742us/step - loss: 0.1744 - acc: 0.9301\n",
      "Epoch 20/80\n",
      "2689/2689 [==============================] - 2s 744us/step - loss: 0.1794 - acc: 0.9360\n",
      "Epoch 21/80\n",
      "2689/2689 [==============================] - 2s 748us/step - loss: 0.1579 - acc: 0.9409\n",
      "Epoch 22/80\n",
      "2689/2689 [==============================] - 2s 752us/step - loss: 0.1492 - acc: 0.9409\n",
      "Epoch 23/80\n",
      "2689/2689 [==============================] - 2s 750us/step - loss: 0.1552 - acc: 0.9442\n",
      "Epoch 24/80\n",
      "2689/2689 [==============================] - 2s 753us/step - loss: 0.1632 - acc: 0.9360\n",
      "Epoch 25/80\n",
      "2689/2689 [==============================] - 2s 745us/step - loss: 0.1444 - acc: 0.9483\n",
      "Epoch 26/80\n",
      "2689/2689 [==============================] - 2s 748us/step - loss: 0.1313 - acc: 0.9494\n",
      "Epoch 27/80\n",
      "2689/2689 [==============================] - 2s 754us/step - loss: 0.1375 - acc: 0.9531\n",
      "Epoch 28/80\n",
      "2689/2689 [==============================] - 2s 757us/step - loss: 0.1292 - acc: 0.9472\n",
      "Epoch 29/80\n",
      "2689/2689 [==============================] - 2s 745us/step - loss: 0.1367 - acc: 0.9494\n",
      "Epoch 30/80\n",
      "2689/2689 [==============================] - 2s 756us/step - loss: 0.1281 - acc: 0.9520\n",
      "Epoch 31/80\n",
      "2689/2689 [==============================] - 2s 742us/step - loss: 0.1209 - acc: 0.9528\n",
      "Epoch 32/80\n",
      "2689/2689 [==============================] - 2s 741us/step - loss: 0.1330 - acc: 0.9491\n",
      "Epoch 33/80\n",
      "2689/2689 [==============================] - 2s 754us/step - loss: 0.1265 - acc: 0.9528\n",
      "Epoch 34/80\n",
      "2689/2689 [==============================] - 2s 759us/step - loss: 0.1220 - acc: 0.9528\n",
      "Epoch 35/80\n",
      "2689/2689 [==============================] - 2s 738us/step - loss: 0.1140 - acc: 0.9561\n",
      "Epoch 36/80\n",
      "2689/2689 [==============================] - 2s 743us/step - loss: 0.0999 - acc: 0.9632\n",
      "Epoch 37/80\n",
      "2689/2689 [==============================] - 2s 747us/step - loss: 0.1067 - acc: 0.9554\n",
      "Epoch 38/80\n",
      "2689/2689 [==============================] - 2s 742us/step - loss: 0.0982 - acc: 0.9598\n",
      "Epoch 39/80\n",
      "2689/2689 [==============================] - 2s 771us/step - loss: 0.1192 - acc: 0.9587\n",
      "Epoch 40/80\n",
      "2689/2689 [==============================] - 2s 754us/step - loss: 0.0997 - acc: 0.9572\n",
      "Epoch 41/80\n",
      "2689/2689 [==============================] - 2s 776us/step - loss: 0.1169 - acc: 0.9546\n",
      "Epoch 42/80\n",
      "2689/2689 [==============================] - 2s 740us/step - loss: 0.0982 - acc: 0.9632\n",
      "Epoch 43/80\n",
      "2689/2689 [==============================] - 2s 739us/step - loss: 0.1010 - acc: 0.9587\n",
      "Epoch 44/80\n",
      "2689/2689 [==============================] - 2s 764us/step - loss: 0.0892 - acc: 0.9598\n",
      "Epoch 45/80\n",
      "2689/2689 [==============================] - 2s 779us/step - loss: 0.0925 - acc: 0.9632\n",
      "Epoch 46/80\n",
      "2689/2689 [==============================] - 2s 759us/step - loss: 0.0748 - acc: 0.9673\n",
      "Epoch 47/80\n",
      "2689/2689 [==============================] - 2s 763us/step - loss: 0.0857 - acc: 0.9688\n",
      "Epoch 48/80\n",
      "2689/2689 [==============================] - 2s 766us/step - loss: 0.1006 - acc: 0.9650\n",
      "Epoch 49/80\n",
      "2689/2689 [==============================] - 2s 775us/step - loss: 0.0969 - acc: 0.9613\n",
      "Epoch 50/80\n",
      "2689/2689 [==============================] - 2s 781us/step - loss: 0.0971 - acc: 0.9636\n",
      "Epoch 51/80\n",
      "2689/2689 [==============================] - 2s 763us/step - loss: 0.1041 - acc: 0.9613\n",
      "Epoch 52/80\n",
      "2689/2689 [==============================] - 2s 762us/step - loss: 0.0828 - acc: 0.9695\n",
      "Epoch 53/80\n",
      "2689/2689 [==============================] - 2s 755us/step - loss: 0.0927 - acc: 0.9621\n",
      "Epoch 54/80\n",
      "2689/2689 [==============================] - 2s 756us/step - loss: 0.0779 - acc: 0.9665\n",
      "Epoch 55/80\n",
      "2689/2689 [==============================] - 2s 759us/step - loss: 0.0849 - acc: 0.9662\n",
      "Epoch 56/80\n",
      "2689/2689 [==============================] - 2s 752us/step - loss: 0.0868 - acc: 0.9613\n",
      "Epoch 57/80\n",
      "2689/2689 [==============================] - 2s 761us/step - loss: 0.0822 - acc: 0.9669\n",
      "Epoch 58/80\n",
      "2689/2689 [==============================] - 2s 763us/step - loss: 0.0755 - acc: 0.9695\n",
      "Epoch 59/80\n",
      "2689/2689 [==============================] - 2s 756us/step - loss: 0.0782 - acc: 0.9662\n",
      "Epoch 60/80\n",
      "2689/2689 [==============================] - 2s 761us/step - loss: 0.0694 - acc: 0.9699\n",
      "Epoch 61/80\n",
      "2689/2689 [==============================] - 2s 762us/step - loss: 0.0731 - acc: 0.9740\n",
      "Epoch 62/80\n",
      "2689/2689 [==============================] - 2s 790us/step - loss: 0.0723 - acc: 0.9725\n",
      "Epoch 63/80\n",
      "2689/2689 [==============================] - 2s 858us/step - loss: 0.0754 - acc: 0.9665\n",
      "Epoch 64/80\n",
      "2689/2689 [==============================] - 2s 798us/step - loss: 0.0767 - acc: 0.9643\n",
      "Epoch 65/80\n",
      "2689/2689 [==============================] - 2s 772us/step - loss: 0.0882 - acc: 0.9676\n",
      "Epoch 66/80\n",
      "2689/2689 [==============================] - 2s 760us/step - loss: 0.0785 - acc: 0.9695\n",
      "Epoch 67/80\n",
      "2689/2689 [==============================] - 2s 772us/step - loss: 0.0827 - acc: 0.9688\n",
      "Epoch 68/80\n",
      "2689/2689 [==============================] - 2s 762us/step - loss: 0.0609 - acc: 0.9773\n",
      "Epoch 69/80\n",
      "2689/2689 [==============================] - 2s 755us/step - loss: 0.0618 - acc: 0.9755\n",
      "Epoch 70/80\n",
      "2689/2689 [==============================] - 2s 761us/step - loss: 0.0709 - acc: 0.9714\n",
      "Epoch 71/80\n",
      "2689/2689 [==============================] - 2s 767us/step - loss: 0.0699 - acc: 0.9714\n",
      "Epoch 72/80\n",
      "2689/2689 [==============================] - 2s 771us/step - loss: 0.0812 - acc: 0.9699\n",
      "Epoch 73/80\n",
      "2689/2689 [==============================] - 2s 785us/step - loss: 0.0837 - acc: 0.9680\n",
      "Epoch 74/80\n",
      "2689/2689 [==============================] - 2s 788us/step - loss: 0.0836 - acc: 0.9695\n",
      "Epoch 75/80\n",
      "2689/2689 [==============================] - 4s 1ms/step - loss: 0.0724 - acc: 0.9725\n",
      "Epoch 76/80\n",
      "2689/2689 [==============================] - 4s 1ms/step - loss: 0.0746 - acc: 0.9702\n",
      "Epoch 77/80\n",
      "2689/2689 [==============================] - 3s 1ms/step - loss: 0.0551 - acc: 0.9736\n",
      "Epoch 78/80\n",
      "2689/2689 [==============================] - 3s 1ms/step - loss: 0.0709 - acc: 0.9680\n",
      "Epoch 79/80\n",
      "2689/2689 [==============================] - 2s 898us/step - loss: 0.0761 - acc: 0.9729\n",
      "Epoch 80/80\n",
      "2689/2689 [==============================] - 2s 759us/step - loss: 0.0687 - acc: 0.9725\n",
      "Epoch 1/80\n",
      "2688/2688 [==============================] - 2s 745us/step - loss: 0.3538 - acc: 0.8761\n",
      "Epoch 2/80\n",
      "2688/2688 [==============================] - 2s 683us/step - loss: 0.2723 - acc: 0.8858\n",
      "Epoch 3/80\n",
      "2688/2688 [==============================] - 2s 671us/step - loss: 0.2548 - acc: 0.8958\n",
      "Epoch 4/80\n",
      "2688/2688 [==============================] - 2s 727us/step - loss: 0.2337 - acc: 0.9018\n",
      "Epoch 5/80\n",
      "2688/2688 [==============================] - 2s 785us/step - loss: 0.2223 - acc: 0.9126\n",
      "Epoch 6/80\n",
      "2688/2688 [==============================] - 2s 684us/step - loss: 0.2130 - acc: 0.9144\n",
      "Epoch 7/80\n",
      "2688/2688 [==============================] - 2s 681us/step - loss: 0.2056 - acc: 0.9111\n",
      "Epoch 8/80\n",
      "2688/2688 [==============================] - 2s 690us/step - loss: 0.1956 - acc: 0.9275\n",
      "Epoch 9/80\n",
      "2688/2688 [==============================] - 2s 694us/step - loss: 0.1997 - acc: 0.9137\n",
      "Epoch 10/80\n",
      "2688/2688 [==============================] - 2s 722us/step - loss: 0.1804 - acc: 0.9249\n",
      "Epoch 11/80\n",
      "2688/2688 [==============================] - 2s 725us/step - loss: 0.1661 - acc: 0.9345\n",
      "Epoch 12/80\n",
      "2688/2688 [==============================] - 2s 801us/step - loss: 0.1777 - acc: 0.9211\n",
      "Epoch 13/80\n",
      "2688/2688 [==============================] - 2s 884us/step - loss: 0.1611 - acc: 0.9334\n",
      "Epoch 14/80\n",
      "2688/2688 [==============================] - 2s 785us/step - loss: 0.1644 - acc: 0.9301\n",
      "Epoch 15/80\n",
      "2688/2688 [==============================] - 2s 739us/step - loss: 0.1477 - acc: 0.9379\n",
      "Epoch 16/80\n",
      "2688/2688 [==============================] - 2s 718us/step - loss: 0.1523 - acc: 0.9464\n",
      "Epoch 17/80\n",
      "2688/2688 [==============================] - 2s 734us/step - loss: 0.1381 - acc: 0.9472\n",
      "Epoch 18/80\n",
      "2688/2688 [==============================] - 2s 741us/step - loss: 0.1209 - acc: 0.9494\n",
      "Epoch 19/80\n",
      "2688/2688 [==============================] - 2s 750us/step - loss: 0.1299 - acc: 0.9516\n",
      "Epoch 20/80\n",
      "2688/2688 [==============================] - 2s 758us/step - loss: 0.1222 - acc: 0.9509\n",
      "Epoch 21/80\n",
      "2688/2688 [==============================] - 2s 739us/step - loss: 0.1366 - acc: 0.9446\n",
      "Epoch 22/80\n",
      "2688/2688 [==============================] - 2s 735us/step - loss: 0.1197 - acc: 0.9546\n",
      "Epoch 23/80\n",
      "2688/2688 [==============================] - 2s 743us/step - loss: 0.1228 - acc: 0.9513\n",
      "Epoch 24/80\n",
      "2688/2688 [==============================] - 2s 742us/step - loss: 0.1141 - acc: 0.9509\n",
      "Epoch 25/80\n",
      "2688/2688 [==============================] - 2s 741us/step - loss: 0.1178 - acc: 0.9557\n",
      "Epoch 26/80\n",
      "2688/2688 [==============================] - 2s 768us/step - loss: 0.1137 - acc: 0.9516\n",
      "Epoch 27/80\n",
      "2688/2688 [==============================] - 2s 751us/step - loss: 0.1160 - acc: 0.9550\n",
      "Epoch 28/80\n",
      "2688/2688 [==============================] - 2s 779us/step - loss: 0.1114 - acc: 0.9554\n",
      "Epoch 29/80\n",
      "2688/2688 [==============================] - 2s 740us/step - loss: 0.1232 - acc: 0.9472\n",
      "Epoch 30/80\n",
      "2688/2688 [==============================] - 2s 849us/step - loss: 0.1103 - acc: 0.9546\n",
      "Epoch 31/80\n",
      "2688/2688 [==============================] - 2s 872us/step - loss: 0.1051 - acc: 0.9565\n",
      "Epoch 32/80\n",
      "2688/2688 [==============================] - 2s 752us/step - loss: 0.1025 - acc: 0.9561\n",
      "Epoch 33/80\n",
      "2688/2688 [==============================] - 2s 787us/step - loss: 0.0983 - acc: 0.9531\n",
      "Epoch 34/80\n",
      "2688/2688 [==============================] - 2s 786us/step - loss: 0.1090 - acc: 0.9539\n",
      "Epoch 35/80\n",
      "2688/2688 [==============================] - 2s 792us/step - loss: 0.1107 - acc: 0.9516\n",
      "Epoch 36/80\n",
      "2688/2688 [==============================] - 2s 789us/step - loss: 0.0948 - acc: 0.9606\n",
      "Epoch 37/80\n",
      "2688/2688 [==============================] - 2s 789us/step - loss: 0.0937 - acc: 0.9591\n",
      "Epoch 38/80\n",
      "2688/2688 [==============================] - 2s 773us/step - loss: 0.1015 - acc: 0.9583\n",
      "Epoch 39/80\n",
      "2688/2688 [==============================] - 2s 781us/step - loss: 0.0901 - acc: 0.9613\n",
      "Epoch 40/80\n",
      "2688/2688 [==============================] - 2s 775us/step - loss: 0.0916 - acc: 0.9617\n",
      "Epoch 41/80\n",
      "2688/2688 [==============================] - 2s 779us/step - loss: 0.0883 - acc: 0.9632\n",
      "Epoch 42/80\n",
      "2688/2688 [==============================] - 2s 787us/step - loss: 0.0902 - acc: 0.9632\n",
      "Epoch 43/80\n",
      "2688/2688 [==============================] - 2s 778us/step - loss: 0.0831 - acc: 0.9594\n",
      "Epoch 44/80\n",
      "2688/2688 [==============================] - 2s 749us/step - loss: 0.0817 - acc: 0.9680\n",
      "Epoch 45/80\n",
      "2688/2688 [==============================] - 2s 770us/step - loss: 0.0797 - acc: 0.9639\n",
      "Epoch 46/80\n",
      "2688/2688 [==============================] - 2s 773us/step - loss: 0.0899 - acc: 0.9546\n",
      "Epoch 47/80\n",
      "2688/2688 [==============================] - 2s 762us/step - loss: 0.0993 - acc: 0.9617\n",
      "Epoch 48/80\n",
      "2688/2688 [==============================] - 2s 772us/step - loss: 0.0810 - acc: 0.9624\n",
      "Epoch 49/80\n",
      "2688/2688 [==============================] - 2s 769us/step - loss: 0.0916 - acc: 0.9554\n",
      "Epoch 50/80\n",
      "2688/2688 [==============================] - 2s 776us/step - loss: 0.0930 - acc: 0.9568\n",
      "Epoch 51/80\n",
      "2688/2688 [==============================] - 2s 779us/step - loss: 0.0845 - acc: 0.9609\n",
      "Epoch 52/80\n",
      "2688/2688 [==============================] - 2s 767us/step - loss: 0.0848 - acc: 0.9643\n",
      "Epoch 53/80\n",
      "2688/2688 [==============================] - 2s 779us/step - loss: 0.0770 - acc: 0.9702\n",
      "Epoch 54/80\n",
      "2688/2688 [==============================] - 2s 770us/step - loss: 0.0852 - acc: 0.9643\n",
      "Epoch 55/80\n",
      "2688/2688 [==============================] - 2s 765us/step - loss: 0.0824 - acc: 0.9647\n",
      "Epoch 56/80\n",
      "2688/2688 [==============================] - 2s 785us/step - loss: 0.0845 - acc: 0.9635\n",
      "Epoch 57/80\n",
      "2688/2688 [==============================] - 2s 799us/step - loss: 0.0890 - acc: 0.9613\n",
      "Epoch 58/80\n",
      "2688/2688 [==============================] - 2s 786us/step - loss: 0.0877 - acc: 0.9617\n",
      "Epoch 59/80\n",
      "2688/2688 [==============================] - 2s 854us/step - loss: 0.0889 - acc: 0.9624\n",
      "Epoch 60/80\n",
      "2688/2688 [==============================] - 2s 851us/step - loss: 0.0805 - acc: 0.9647\n",
      "Epoch 61/80\n",
      "2688/2688 [==============================] - 2s 826us/step - loss: 0.0771 - acc: 0.9661\n",
      "Epoch 62/80\n",
      "2688/2688 [==============================] - 2s 792us/step - loss: 0.0761 - acc: 0.9676\n",
      "Epoch 63/80\n",
      "2688/2688 [==============================] - 2s 888us/step - loss: 0.0883 - acc: 0.9647\n",
      "Epoch 64/80\n",
      "2688/2688 [==============================] - 2s 862us/step - loss: 0.0871 - acc: 0.9654\n",
      "Epoch 65/80\n",
      "2688/2688 [==============================] - 2s 839us/step - loss: 0.0693 - acc: 0.9628\n",
      "Epoch 66/80\n",
      "2688/2688 [==============================] - 2s 853us/step - loss: 0.0704 - acc: 0.9658\n",
      "Epoch 67/80\n",
      "2688/2688 [==============================] - 2s 824us/step - loss: 0.0845 - acc: 0.9665\n",
      "Epoch 68/80\n",
      "2688/2688 [==============================] - 2s 819us/step - loss: 0.0856 - acc: 0.9602\n",
      "Epoch 69/80\n",
      "2688/2688 [==============================] - 2s 847us/step - loss: 0.0868 - acc: 0.9680\n",
      "Epoch 70/80\n",
      "2688/2688 [==============================] - 2s 884us/step - loss: 0.0788 - acc: 0.9647\n",
      "Epoch 71/80\n",
      "2688/2688 [==============================] - 2s 871us/step - loss: 0.0754 - acc: 0.9632\n",
      "Epoch 72/80\n",
      "2688/2688 [==============================] - 2s 863us/step - loss: 0.0879 - acc: 0.9665\n",
      "Epoch 73/80\n",
      "2688/2688 [==============================] - 2s 875us/step - loss: 0.0719 - acc: 0.9673\n",
      "Epoch 74/80\n",
      "2688/2688 [==============================] - 2s 882us/step - loss: 0.0869 - acc: 0.9643\n",
      "Epoch 75/80\n",
      "2688/2688 [==============================] - 2s 843us/step - loss: 0.0833 - acc: 0.9688\n",
      "Epoch 76/80\n",
      "2688/2688 [==============================] - 2s 853us/step - loss: 0.0730 - acc: 0.9661\n",
      "Epoch 77/80\n",
      "2688/2688 [==============================] - 2s 852us/step - loss: 0.0784 - acc: 0.9684\n",
      "Epoch 78/80\n",
      "2688/2688 [==============================] - 3s 1ms/step - loss: 0.0863 - acc: 0.9621\n",
      "Epoch 79/80\n",
      "2688/2688 [==============================] - 3s 995us/step - loss: 0.0815 - acc: 0.9639\n",
      "Epoch 80/80\n",
      "2688/2688 [==============================] - 2s 919us/step - loss: 0.0745 - acc: 0.9688\n",
      "2688/2688 [==============================] - 2s 737us/step\n",
      "Accruracy neural network: 89.919404525787% +- 0.09114398888944657%\n",
      "--- 15.611328609784444 minutes elapsed---\n"
     ]
    }
   ],
   "source": [
    "NN,score, printing = cross_val(model_creation,x_vec_train,y_vec_train,3,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_functions = [model_creation,model_creation1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split no :0\n",
      "WARNING:tensorflow:From /home/robin/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=10, activation=\"relu\", input_shape=(50, 300), padding=\"same\")`\n",
      "  app.launch_new_instance()\n",
      "/home/robin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=4, activation=\"relu\", input_shape=(50, 300), padding=\"same\")`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.6929 - acc: 0.5081\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 1s 903us/step - loss: 0.6933 - acc: 0.4981\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 1s 925us/step - loss: 0.6916 - acc: 0.5357\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 1s 952us/step - loss: 0.6936 - acc: 0.5144\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 1s 960us/step - loss: 0.6935 - acc: 0.5019\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 1s 970us/step - loss: 0.6934 - acc: 0.4994\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 1s 996us/step - loss: 0.6907 - acc: 0.5319\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.6952 - acc: 0.4906\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.6940 - acc: 0.5056\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.6928 - acc: 0.5044\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.6938 - acc: 0.5019\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.6929 - acc: 0.5219\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.6908 - acc: 0.5119\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.6672 - acc: 0.5970\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.5681 - acc: 0.7397\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.5287 - acc: 0.7772\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.5351 - acc: 0.8010\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.4551 - acc: 0.8298\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.4191 - acc: 0.8536\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.4256 - acc: 0.8611\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.3964 - acc: 0.8598\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.3646 - acc: 0.8698\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.3519 - acc: 0.8773\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.3223 - acc: 0.8861\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.3224 - acc: 0.8899\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.2931 - acc: 0.9161\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.3082 - acc: 0.8974\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.2736 - acc: 0.9136\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.2730 - acc: 0.9186\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.2485 - acc: 0.9212\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.2065 - acc: 0.9412\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1839 - acc: 0.9437\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1798 - acc: 0.9474\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.2162 - acc: 0.9324\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1846 - acc: 0.9587\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1724 - acc: 0.9549\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1706 - acc: 0.9524\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1503 - acc: 0.9524\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1786 - acc: 0.9549\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1690 - acc: 0.9499\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.6891 - acc: 0.5451\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 0s 597us/step - loss: 0.6666 - acc: 0.6383\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 0s 587us/step - loss: 0.6348 - acc: 0.7146\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 1s 644us/step - loss: 0.5812 - acc: 0.7622\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 1s 659us/step - loss: 0.4890 - acc: 0.8173\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 1s 632us/step - loss: 0.4122 - acc: 0.8310\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 1s 672us/step - loss: 0.3583 - acc: 0.8548\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 1s 665us/step - loss: 0.3096 - acc: 0.8686\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 0s 601us/step - loss: 0.2718 - acc: 0.8811\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 0s 607us/step - loss: 0.2409 - acc: 0.9049\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 0s 566us/step - loss: 0.2065 - acc: 0.9186\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 0s 605us/step - loss: 0.1875 - acc: 0.9374\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 0s 611us/step - loss: 0.1613 - acc: 0.9474\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 0s 602us/step - loss: 0.1349 - acc: 0.9625\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 0s 588us/step - loss: 0.1247 - acc: 0.9712\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 0s 590us/step - loss: 0.1037 - acc: 0.9675\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 0s 600us/step - loss: 0.0976 - acc: 0.9700\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 0s 597us/step - loss: 0.0947 - acc: 0.9800\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 1s 629us/step - loss: 0.0834 - acc: 0.9800\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 0s 606us/step - loss: 0.0711 - acc: 0.9825\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 0s 591us/step - loss: 0.0628 - acc: 0.9875\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 0s 582us/step - loss: 0.0603 - acc: 0.9850\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 0s 587us/step - loss: 0.0525 - acc: 0.9900\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 1s 669us/step - loss: 0.0523 - acc: 0.9850\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 0s 613us/step - loss: 0.0409 - acc: 0.9912\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 0s 580us/step - loss: 0.0481 - acc: 0.9887\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 0s 611us/step - loss: 0.0343 - acc: 0.9925\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 0s 618us/step - loss: 0.0313 - acc: 0.9950\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 1s 651us/step - loss: 0.0291 - acc: 0.9950\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 1s 655us/step - loss: 0.0257 - acc: 0.9962\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 0s 616us/step - loss: 0.0249 - acc: 0.9962\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 1s 648us/step - loss: 0.0271 - acc: 0.9987\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 1s 655us/step - loss: 0.0256 - acc: 0.9937\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 0s 590us/step - loss: 0.0213 - acc: 0.9975\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 0s 598us/step - loss: 0.0177 - acc: 0.9975\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 0s 583us/step - loss: 0.0191 - acc: 0.9975\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 0s 608us/step - loss: 0.0167 - acc: 0.9975\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 1s 636us/step - loss: 0.0175 - acc: 0.9975\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 0s 611us/step - loss: 0.0143 - acc: 0.9987\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 1s 709us/step - loss: 0.0171 - acc: 0.9962\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.5553 - acc: 0.8085\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.4550 - acc: 0.8411\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.4231 - acc: 0.8586\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.3931 - acc: 0.8486\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.3683 - acc: 0.8648\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.3360 - acc: 0.8849\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.3182 - acc: 0.8936\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.3018 - acc: 0.9024\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.3200 - acc: 0.8836\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.2580 - acc: 0.9199\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.2646 - acc: 0.9111\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.2427 - acc: 0.9161\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.2354 - acc: 0.9186\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.2459 - acc: 0.9161\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1980 - acc: 0.9399\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1775 - acc: 0.9437\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1748 - acc: 0.9449\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1790 - acc: 0.9512\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1519 - acc: 0.9625\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1438 - acc: 0.9612\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1443 - acc: 0.9625\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1349 - acc: 0.9625\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1217 - acc: 0.9700\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1274 - acc: 0.9687\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1402 - acc: 0.9650\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1120 - acc: 0.9625\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1148 - acc: 0.9587\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1120 - acc: 0.9675\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1384 - acc: 0.9587\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1230 - acc: 0.9650\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1185 - acc: 0.9700\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1036 - acc: 0.9775\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0818 - acc: 0.9762\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0914 - acc: 0.9812\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.0771 - acc: 0.9800\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.0685 - acc: 0.9850\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1042 - acc: 0.9687\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0773 - acc: 0.9837\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.0761 - acc: 0.9800\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.0721 - acc: 0.9825\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 1s 691us/step - loss: 0.8056 - acc: 0.7985\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 1s 669us/step - loss: 0.4136 - acc: 0.8398\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 1s 901us/step - loss: 0.3177 - acc: 0.8886\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 1s 914us/step - loss: 0.2834 - acc: 0.9149\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 1s 903us/step - loss: 0.2230 - acc: 0.9362\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 1s 898us/step - loss: 0.1920 - acc: 0.9437\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 1s 906us/step - loss: 0.1438 - acc: 0.9549\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 1s 793us/step - loss: 0.1276 - acc: 0.9562\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 1s 736us/step - loss: 0.0997 - acc: 0.9750\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 1s 791us/step - loss: 0.0876 - acc: 0.9712\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 1s 769us/step - loss: 0.0801 - acc: 0.9750\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 1s 791us/step - loss: 0.0800 - acc: 0.9762\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 1s 856us/step - loss: 0.0586 - acc: 0.9875\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 1s 734us/step - loss: 0.0543 - acc: 0.9850\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 1s 788us/step - loss: 0.0517 - acc: 0.9887\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 1s 805us/step - loss: 0.0436 - acc: 0.9862\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 1s 775us/step - loss: 0.0401 - acc: 0.9912\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0354 - acc: 0.9925\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 1s 898us/step - loss: 0.0368 - acc: 0.9925\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 1s 789us/step - loss: 0.0357 - acc: 0.9912\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 1s 956us/step - loss: 0.0312 - acc: 0.9962\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0270 - acc: 0.9962\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 1s 723us/step - loss: 0.0253 - acc: 0.9950\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 1s 759us/step - loss: 0.0258 - acc: 0.9962\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 1s 758us/step - loss: 0.0233 - acc: 0.9975\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0217 - acc: 0.9975\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 1s 844us/step - loss: 0.0222 - acc: 0.9962\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 1s 782us/step - loss: 0.0239 - acc: 0.9950\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 1s 735us/step - loss: 0.0240 - acc: 0.9925\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 1s 857us/step - loss: 0.0190 - acc: 0.9962\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 1s 839us/step - loss: 0.0221 - acc: 0.9962\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0212 - acc: 0.9950\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 1s 966us/step - loss: 0.0203 - acc: 0.9925\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0155 - acc: 0.9987\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0116 - acc: 0.9987\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0184 - acc: 0.9950\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 1s 853us/step - loss: 0.0118 - acc: 0.9987\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 1s 880us/step - loss: 0.0120 - acc: 0.9987\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 1s 709us/step - loss: 0.0115 - acc: 0.9987\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 1s 682us/step - loss: 0.0100 - acc: 0.9987\n",
      "799/799 [==============================] - 1s 1ms/step\n",
      "799/799 [==============================] - 1s 695us/step\n",
      "Split no :1\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.6935 - acc: 0.4562\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.6931 - acc: 0.4981\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.6931 - acc: 0.5119\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.6938 - acc: 0.5069\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.6934 - acc: 0.4881\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.6928 - acc: 0.5094\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.6931 - acc: 0.4944\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.6933 - acc: 0.4856\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.6938 - acc: 0.4869\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.6931 - acc: 0.4994\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.6925 - acc: 0.5044\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.6871 - acc: 0.5957\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.6299 - acc: 0.6984\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.5153 - acc: 0.7735\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.5521 - acc: 0.7885\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.4825 - acc: 0.8085\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.5159 - acc: 0.8035\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.4500 - acc: 0.8436\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.4158 - acc: 0.8561\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3695 - acc: 0.8798\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.3656 - acc: 0.8711\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3495 - acc: 0.8936\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3284 - acc: 0.8949\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.2802 - acc: 0.9174\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.2906 - acc: 0.9086\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.2585 - acc: 0.9249\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.2760 - acc: 0.9136\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.2650 - acc: 0.9036\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.2933 - acc: 0.9011\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.2533 - acc: 0.9312\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.2055 - acc: 0.9474\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 0.2056 - acc: 0.9412\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.1848 - acc: 0.9449\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 4s 4ms/step - loss: 0.1599 - acc: 0.9537\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 0.1681 - acc: 0.9537\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1735 - acc: 0.9362\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1354 - acc: 0.9650\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1466 - acc: 0.9524\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1622 - acc: 0.9574\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1426 - acc: 0.9625\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.6974 - acc: 0.5175\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 1s 975us/step - loss: 0.6578 - acc: 0.6533\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 1s 963us/step - loss: 0.6125 - acc: 0.7259\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.5464 - acc: 0.7747\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.4740 - acc: 0.8023\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.4064 - acc: 0.8285\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.3389 - acc: 0.8711\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.2909 - acc: 0.8861\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 1s 921us/step - loss: 0.2589 - acc: 0.8999\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 1s 968us/step - loss: 0.2307 - acc: 0.9174\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 1s 803us/step - loss: 0.1870 - acc: 0.9299\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 1s 889us/step - loss: 0.1716 - acc: 0.9399\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 1s 897us/step - loss: 0.1512 - acc: 0.9549\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1228 - acc: 0.9562\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1142 - acc: 0.9612\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0929 - acc: 0.9787\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0876 - acc: 0.9775\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 1s 922us/step - loss: 0.0661 - acc: 0.9850\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0647 - acc: 0.9837\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 1s 909us/step - loss: 0.0512 - acc: 0.9837\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0518 - acc: 0.9887\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0461 - acc: 0.9925\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0427 - acc: 0.9875\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0356 - acc: 0.9950\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0327 - acc: 0.9912\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 1s 857us/step - loss: 0.0354 - acc: 0.9937\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 1s 962us/step - loss: 0.0282 - acc: 0.9962\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 1s 877us/step - loss: 0.0321 - acc: 0.9925\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 1s 888us/step - loss: 0.0283 - acc: 0.9937\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 1s 911us/step - loss: 0.0234 - acc: 0.9975\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 1s 977us/step - loss: 0.0200 - acc: 0.9962\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 1s 784us/step - loss: 0.0219 - acc: 0.9962\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0188 - acc: 0.9962\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0197 - acc: 0.9962\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0208 - acc: 0.9950\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 1s 994us/step - loss: 0.0187 - acc: 0.9950\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 1s 836us/step - loss: 0.0181 - acc: 0.9950\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 1s 885us/step - loss: 0.0223 - acc: 0.9912\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 1s 965us/step - loss: 0.0144 - acc: 0.9987\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0172 - acc: 0.9962\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.5843 - acc: 0.7722\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.5259 - acc: 0.7835\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.4908 - acc: 0.7822\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.4100 - acc: 0.8385\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.3967 - acc: 0.8448\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.3690 - acc: 0.8561\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3623 - acc: 0.8661\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.3349 - acc: 0.8748\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3016 - acc: 0.9049\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2693 - acc: 0.9174\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2699 - acc: 0.9011\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2589 - acc: 0.9161\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2097 - acc: 0.9362\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2291 - acc: 0.9362\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1921 - acc: 0.9374\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.2093 - acc: 0.9362\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1696 - acc: 0.9474\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1763 - acc: 0.9474\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1459 - acc: 0.9587\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1685 - acc: 0.9499\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1174 - acc: 0.9687\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1400 - acc: 0.9574\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1193 - acc: 0.9700\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1144 - acc: 0.9662\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1044 - acc: 0.9662\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1164 - acc: 0.9662\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1089 - acc: 0.9637\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0924 - acc: 0.9712\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0892 - acc: 0.9762\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.0999 - acc: 0.9725\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1018 - acc: 0.9725\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0923 - acc: 0.9700\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0771 - acc: 0.9787\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0853 - acc: 0.9812\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0841 - acc: 0.9737\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0864 - acc: 0.9775\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.0803 - acc: 0.9737\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1010 - acc: 0.9687\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0679 - acc: 0.9837\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0693 - acc: 0.9787\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 1s 988us/step - loss: 0.7819 - acc: 0.7922\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 1s 852us/step - loss: 0.4134 - acc: 0.8423\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 1s 855us/step - loss: 0.3198 - acc: 0.8861\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 1s 937us/step - loss: 0.2835 - acc: 0.9024\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 1s 861us/step - loss: 0.2271 - acc: 0.9312\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1799 - acc: 0.9462\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 1s 794us/step - loss: 0.1493 - acc: 0.9487\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 1s 860us/step - loss: 0.1195 - acc: 0.9612\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 1s 842us/step - loss: 0.1089 - acc: 0.9625\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 1s 972us/step - loss: 0.0907 - acc: 0.9737\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 1s 843us/step - loss: 0.0710 - acc: 0.9762\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 1s 947us/step - loss: 0.0637 - acc: 0.9875\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0642 - acc: 0.9800\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0637 - acc: 0.9825\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0472 - acc: 0.9887\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0370 - acc: 0.9900\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0369 - acc: 0.9912\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0361 - acc: 0.9912\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0329 - acc: 0.9950\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0364 - acc: 0.9887\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0285 - acc: 0.9937\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 1s 916us/step - loss: 0.0347 - acc: 0.9900\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0290 - acc: 0.9950\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0289 - acc: 0.9912\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0265 - acc: 0.9950\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 1s 944us/step - loss: 0.0172 - acc: 0.9975\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 1s 867us/step - loss: 0.0167 - acc: 0.9962\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 1s 751us/step - loss: 0.0204 - acc: 0.9937\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 1s 798us/step - loss: 0.0178 - acc: 0.9975\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 1s 956us/step - loss: 0.0142 - acc: 0.9987\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0138 - acc: 0.9987\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 1s 923us/step - loss: 0.0135 - acc: 0.9987\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0111 - acc: 0.9987\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0164 - acc: 0.9962\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0155 - acc: 0.9962\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0134 - acc: 0.9987\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0080 - acc: 1.0000\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0121 - acc: 0.9975\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0109 - acc: 0.9975\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.0125 - acc: 0.9975\n",
      "799/799 [==============================] - 2s 3ms/step\n",
      "799/799 [==============================] - 1s 1ms/step\n",
      "Split no :2\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 8s 10ms/step - loss: 0.6931 - acc: 0.5019\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 5s 6ms/step - loss: 0.6936 - acc: 0.4931\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 5s 6ms/step - loss: 0.6927 - acc: 0.5119\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 0.6938 - acc: 0.4894\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.6927 - acc: 0.5119\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 0.6927 - acc: 0.5131\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.6936 - acc: 0.5056\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 0.6937 - acc: 0.5006\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 0.6923 - acc: 0.5094\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 0.6920 - acc: 0.5244\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.6911 - acc: 0.5407\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.6839 - acc: 0.6170\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.6171 - acc: 0.7059\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.5278 - acc: 0.7522\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.5596 - acc: 0.7096\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.5233 - acc: 0.8098\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.5083 - acc: 0.7897\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.4322 - acc: 0.8285\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.4987 - acc: 0.8098\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.4216 - acc: 0.8360\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.4184 - acc: 0.8461\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.4155 - acc: 0.8511\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.4071 - acc: 0.8586\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.3636 - acc: 0.8673\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.3609 - acc: 0.8736\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.3217 - acc: 0.8811\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.3207 - acc: 0.8874\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.2887 - acc: 0.8961\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3012 - acc: 0.9086\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.2576 - acc: 0.9199\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2210 - acc: 0.9186\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2263 - acc: 0.9262\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2124 - acc: 0.9312\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1965 - acc: 0.9312\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1616 - acc: 0.9424\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1831 - acc: 0.9474\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1611 - acc: 0.9387\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1777 - acc: 0.9399\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1532 - acc: 0.9474\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.1511 - acc: 0.9424\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.6926 - acc: 0.5307\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.6655 - acc: 0.6521\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 1s 816us/step - loss: 0.6060 - acc: 0.7447\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 1s 826us/step - loss: 0.5248 - acc: 0.7985\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 1s 748us/step - loss: 0.4646 - acc: 0.8110\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 1s 932us/step - loss: 0.3849 - acc: 0.8448\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 1s 879us/step - loss: 0.3318 - acc: 0.8673\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.3031 - acc: 0.8836\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.2550 - acc: 0.9049\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.2057 - acc: 0.9324\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.1845 - acc: 0.9387\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1752 - acc: 0.9299\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1478 - acc: 0.9537\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.1214 - acc: 0.9637\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0981 - acc: 0.9750\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0889 - acc: 0.9737\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0791 - acc: 0.9750\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0680 - acc: 0.9850\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0591 - acc: 0.9875\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0537 - acc: 0.9912\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0559 - acc: 0.9875\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.0510 - acc: 0.9825\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0412 - acc: 0.9912\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.0384 - acc: 0.9950\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.0402 - acc: 0.9875\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.0266 - acc: 0.9962\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.0277 - acc: 0.9950\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.0281 - acc: 0.9937\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0292 - acc: 0.9950\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0252 - acc: 0.9937\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0246 - acc: 0.9900\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.0200 - acc: 0.9987\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0239 - acc: 0.9925\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.0173 - acc: 0.9987\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.0199 - acc: 0.9950\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0187 - acc: 0.9950\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0149 - acc: 0.9975\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0173 - acc: 0.9962\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0182 - acc: 0.9937\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0167 - acc: 0.9950\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.6128 - acc: 0.7935\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.5080 - acc: 0.7522\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.4369 - acc: 0.8523\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3940 - acc: 0.8411\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3735 - acc: 0.8335\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3519 - acc: 0.8523\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3293 - acc: 0.8786\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3032 - acc: 0.8761\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.3033 - acc: 0.8711\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2792 - acc: 0.8924\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2523 - acc: 0.8911\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.2452 - acc: 0.9124\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.2128 - acc: 0.9274\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1883 - acc: 0.9387\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.1647 - acc: 0.9449\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.1824 - acc: 0.9387\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 3s 3ms/step - loss: 0.1702 - acc: 0.9437\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1523 - acc: 0.9637\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 4s 5ms/step - loss: 0.1797 - acc: 0.9412\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 5s 6ms/step - loss: 0.1368 - acc: 0.9599\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.1087 - acc: 0.9637\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 3s 4ms/step - loss: 0.1575 - acc: 0.9537\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1260 - acc: 0.9637\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1109 - acc: 0.9650\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.1174 - acc: 0.9637\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.0976 - acc: 0.9737\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.0916 - acc: 0.9662\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.0909 - acc: 0.9700\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.0650 - acc: 0.9775\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 2s 3ms/step - loss: 0.0895 - acc: 0.9750\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 1s 2ms/step - loss: 0.0767 - acc: 0.9775\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0834 - acc: 0.9762\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0838 - acc: 0.9812\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0807 - acc: 0.9737\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0630 - acc: 0.9837\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0468 - acc: 0.9875\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0478 - acc: 0.9887\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0545 - acc: 0.9787\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0910 - acc: 0.9737\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 2s 2ms/step - loss: 0.0730 - acc: 0.9725\n",
      "Epoch 1/40\n",
      "799/799 [==============================] - 1s 709us/step - loss: 0.7956 - acc: 0.7685\n",
      "Epoch 2/40\n",
      "799/799 [==============================] - 1s 767us/step - loss: 0.4022 - acc: 0.8486\n",
      "Epoch 3/40\n",
      "799/799 [==============================] - 1s 871us/step - loss: 0.2956 - acc: 0.8886\n",
      "Epoch 4/40\n",
      "799/799 [==============================] - 1s 939us/step - loss: 0.2621 - acc: 0.9111\n",
      "Epoch 5/40\n",
      "799/799 [==============================] - 1s 740us/step - loss: 0.2104 - acc: 0.9299\n",
      "Epoch 6/40\n",
      "799/799 [==============================] - 1s 727us/step - loss: 0.1642 - acc: 0.9437\n",
      "Epoch 7/40\n",
      "799/799 [==============================] - 1s 757us/step - loss: 0.1505 - acc: 0.9424\n",
      "Epoch 8/40\n",
      "799/799 [==============================] - 1s 809us/step - loss: 0.1183 - acc: 0.9687\n",
      "Epoch 9/40\n",
      "799/799 [==============================] - 1s 697us/step - loss: 0.0944 - acc: 0.9700\n",
      "Epoch 10/40\n",
      "799/799 [==============================] - 1s 971us/step - loss: 0.0849 - acc: 0.9737\n",
      "Epoch 11/40\n",
      "799/799 [==============================] - 1s 750us/step - loss: 0.0653 - acc: 0.9825\n",
      "Epoch 12/40\n",
      "799/799 [==============================] - 1s 917us/step - loss: 0.0582 - acc: 0.9887\n",
      "Epoch 13/40\n",
      "799/799 [==============================] - 1s 845us/step - loss: 0.0563 - acc: 0.9787\n",
      "Epoch 14/40\n",
      "799/799 [==============================] - 1s 854us/step - loss: 0.0522 - acc: 0.9825\n",
      "Epoch 15/40\n",
      "799/799 [==============================] - 1s 775us/step - loss: 0.0436 - acc: 0.9862\n",
      "Epoch 16/40\n",
      "799/799 [==============================] - 1s 789us/step - loss: 0.0429 - acc: 0.9912\n",
      "Epoch 17/40\n",
      "799/799 [==============================] - 1s 910us/step - loss: 0.0380 - acc: 0.9900\n",
      "Epoch 18/40\n",
      "799/799 [==============================] - 1s 826us/step - loss: 0.0364 - acc: 0.9900\n",
      "Epoch 19/40\n",
      "799/799 [==============================] - 1s 987us/step - loss: 0.0384 - acc: 0.9875\n",
      "Epoch 20/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0333 - acc: 0.9900\n",
      "Epoch 21/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0296 - acc: 0.9912\n",
      "Epoch 22/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0304 - acc: 0.9937\n",
      "Epoch 23/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0256 - acc: 0.9937\n",
      "Epoch 24/40\n",
      "799/799 [==============================] - 1s 846us/step - loss: 0.0200 - acc: 0.9962\n",
      "Epoch 25/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0242 - acc: 0.9950\n",
      "Epoch 26/40\n",
      "799/799 [==============================] - 1s 989us/step - loss: 0.0200 - acc: 0.9962\n",
      "Epoch 27/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0242 - acc: 0.9950\n",
      "Epoch 28/40\n",
      "799/799 [==============================] - 1s 972us/step - loss: 0.0188 - acc: 0.9962\n",
      "Epoch 29/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0271 - acc: 0.9900\n",
      "Epoch 30/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0150 - acc: 0.9975\n",
      "Epoch 31/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0207 - acc: 0.9975\n",
      "Epoch 32/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0163 - acc: 0.9975\n",
      "Epoch 33/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0139 - acc: 0.9975\n",
      "Epoch 34/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 35/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0125 - acc: 0.9962\n",
      "Epoch 36/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0178 - acc: 0.9950\n",
      "Epoch 37/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0183 - acc: 0.9937\n",
      "Epoch 38/40\n",
      "799/799 [==============================] - 1s 985us/step - loss: 0.0131 - acc: 0.9987\n",
      "Epoch 39/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0121 - acc: 0.9987\n",
      "Epoch 40/40\n",
      "799/799 [==============================] - 1s 1ms/step - loss: 0.0110 - acc: 1.0000\n",
      "799/799 [==============================] - 2s 2ms/step\n",
      "799/799 [==============================] - 1s 1ms/step\n",
      "Accruracy neural network0 : 81.97747182736657% +- 1.0766364074082224%\n",
      "Accruracy neural network1 : 81.2265331117521% +- 0.2703688487869398%\n",
      "--- 11.60447654724121 minutes elapsed---\n"
     ]
    }
   ],
   "source": [
    "NNs,test,test1,test2 = multi_cross_val(model_functions,x_vec_train,y_vec_train,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting of validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-86dfb37c9bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "acc = []\n",
    "for i in range(len(hist)):\n",
    "    acc.append(hist[i][1])\n",
    "\n",
    "plt.plot(acc)    \n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best = NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009/1009 [==============================] - 0s 299us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4061938151071075, 0.8860257680872151]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.evaluate(x_vec_test,y_vec_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model,name,path):\n",
    "    \"\"\" This function saves a Keras model.\n",
    "            \n",
    "        Keyword arguments:\n",
    "        model -- The Keras model to be saved\n",
    "        name -- Name to be given to the Keras model\n",
    "        path -- Path where to save the Keras model\n",
    "    \"\"\"    \n",
    "    model_json = model.to_json()\n",
    "    with open(path+name+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "        model.save_weights(path+name+\".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "def load_model(name,path):\n",
    "    \"\"\" This function loads a Keras model.\n",
    "            /!\\ You'll need to compile the loaded model.\n",
    "        Keyword arguments:\n",
    "        name -- Name of the Keras model\n",
    "        path -- Path from where to load the Keras model\n",
    "    \"\"\"       \n",
    "    from keras.models import model_from_json\n",
    "    # load json and create model\n",
    "    json_file = open(path+name+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(path+name+\".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation of the model for live prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vote(score):\n",
    "    \"\"\" This function choose return the average of a score array for larger than the maximum size sentences.\n",
    "            \n",
    "        Keyword arguments:\n",
    "        score -- An array containing the score of the different parts of a sentence\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    neg=[]\n",
    "    pos=[]\n",
    "    for s in score :\n",
    "        neg.append(s[0][0])\n",
    "        pos.append(s[0][1])\n",
    "    sup_neg = [n for n in neg if n > 0.5]\n",
    "    sup_pos = [p for p in pos if p > 0.5]\n",
    "\n",
    "    return [np.mean(neg),np.mean(pos)]\n",
    "    \n",
    "def predict(sentence,word2Vec_model,model,max_len=50,word_length=300):\n",
    "    \"\"\" This function predict if a sentence is positive or negative\n",
    "            /!\\ If the sentence is larger than the max_len, then it's splitted into phrases of max_len length \n",
    "            then the average of the predictions of the phrases is made to make the final prediction\n",
    "\n",
    "        Keyword arguments:\n",
    "        sentence -- The sentence to make a prediction on\n",
    "        word2vec_model -- An already trained gensim Word2Vec model\n",
    "        model -- The keras model to use to make the prediction\n",
    "        max_len -- The maximum length of a sentence (default 50) \n",
    "        word_length -- The number of dimension of the vectors representing the words (default 300) \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    sentence = sentence.split(\" \")\n",
    "    good_sentence = []\n",
    "    unusued_words = []\n",
    "    for i in range(len(sentence)):\n",
    "        try :\n",
    "            sentence[i]=sentence[i].lower()\n",
    "            good_sentence.append(word2Vec_model.wv[sentence[i]])\n",
    "        except : \n",
    "            unusued_words.append(sentence[i])\n",
    "    sentence_length = len(good_sentence)\n",
    "    \n",
    "    if(sentence_length<max_len):    \n",
    "        for j in range(max_len-sentence_length):\n",
    "            good_sentence.append([0]*word_length)     \n",
    "            \n",
    "    if (sentence_length > max_len) :\n",
    "        predictions = []\n",
    "        ind = 0\n",
    "        for i in range(int(np.ceil(sentence_length/max_len))):\n",
    "            sent_tmp = good_sentence[ind:ind+max_len]\n",
    "            if(len(sent_tmp)<max_len):\n",
    "                for j in range(max_len-len(sent_tmp)):\n",
    "                    sent_tmp.append([0]*word_length)    \n",
    "            sent_tmp = np.array(sent_tmp)        \n",
    "            sent_tmp = np.reshape(sent_tmp, (1,sent_tmp.shape[0], sent_tmp.shape[1]))\n",
    "            try :\n",
    "                predictions.append(model.predict(sent_tmp))\n",
    "                ind+=max_len                    \n",
    "            except : \n",
    "                print(\"Erreur au découpage no :\"+str(i))\n",
    "        return vote(predictions)\n",
    "    \n",
    "    good_sentence = np.array(good_sentence)        \n",
    "    good_sentence = np.reshape(good_sentence, (1,good_sentence.shape[0], good_sentence.shape[1]))\n",
    "    return np.squeeze(model.predict(good_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction over a typed sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NN_loaded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fe61f985dd8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNN_loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Négatif        Positif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NN_loaded' is not defined"
     ]
    }
   ],
   "source": [
    "best=NN_loaded\n",
    "sentence =input()\n",
    "print(\" Négatif        Positif\")\n",
    "print(predict(sentence,model,best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
