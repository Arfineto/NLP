{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar 28 13:00:48 2018\n",
    "\n",
    "@author: rniel\n",
    "\"\"\"\n",
    "dataset_path = \"datasets/polarization/\"\n",
    "\n",
    "def open_uci(x_=[],y_=[]):\n",
    "    import numpy as np\n",
    "\n",
    "    file  = open(dataset_path+\"imdb_labelled.txt\", \"rt\")\n",
    "    content = file.read()\n",
    "    contents=[]\n",
    "    contents=content.split(\"\\n\")\n",
    "    del contents[-1]\n",
    "\n",
    "    for c in contents:\n",
    "        temp=c.split(\"\\t\")\n",
    "        x.append(temp[0])\n",
    "        y.append(int(temp[1]))\n",
    "\n",
    "    file  = open(dataset_path+\"amazon_cells_labelled.txt\", \"rt\")\n",
    "    content = file.read()\n",
    "    contents=[]\n",
    "    contents=content.split(\"\\n\")\n",
    "    del contents[-1]\n",
    "\n",
    "\n",
    "    for c in contents:\n",
    "        temp=c.split(\"\\t\")\n",
    "        x_.append(temp[0])\n",
    "        y_.append(int(temp[1]))\n",
    "\n",
    "    file  = open(dataset_path+\"yelp_labelled.txt\", \"rt\")\n",
    "    content = file.read()\n",
    "    contents=[]\n",
    "    contents=content.split(\"\\n\")\n",
    "    del contents[-1]\n",
    "\n",
    "\n",
    "    for c in contents:\n",
    "        temp=c.split(\"\\t\")\n",
    "        x.append(temp[0])\n",
    "        y.append(int(temp[1]))\n",
    "    return x_,y_\n",
    "\n",
    "def open_book(x_=[],y_=[]):\n",
    "    file  = open(dataset_path+\"book.txt\", \"rt\")\n",
    "\n",
    "\n",
    "    nb_duplicate = 0\n",
    "\n",
    "    content = file.read()\n",
    "    contents=[]\n",
    "    contents=content.split(\"\\n\")\n",
    "    del contents[-1]\n",
    "    del contents[-1]\n",
    "\n",
    "    for c in contents:\n",
    "        temp=c.split(\"\\t\")\n",
    "        if(temp[1] in x_):\n",
    "            nb_duplicate+=1\n",
    "        else : \n",
    "            x_.append(temp[1])\n",
    "            y_.append(int(temp[0]))  \n",
    "    return x_,y_\n",
    "\n",
    "def open_movie(x_=[],y_=[]):\n",
    "    file  = open(dataset_path+\"rt-polarity-neg.txt\", \"rt\",encoding = \"ISO-8859-1\")\n",
    "\n",
    "    content = file.read()\n",
    "    contents=[]\n",
    "    contents=content.split(\"\\n\")\n",
    "    del contents[-1]\n",
    "    del contents[-1]\n",
    "\n",
    "    for i in range(len(contents)):\n",
    "        x_.append(contents[i])\n",
    "        y_.append(0)\n",
    "\n",
    "    file  = open(dataset_path+\"rt-polarity-pos.txt\", \"rt\",encoding = \"ISO-8859-1\")\n",
    "    content = file.read()\n",
    "    contents=[]\n",
    "    contents=content.split(\"\\n\")\n",
    "    del contents[-1]\n",
    "    del contents[-1]\n",
    "\n",
    "    for i in range(len(contents)):\n",
    "        x_.append(contents[i])\n",
    "        y_.append(1)    \n",
    "    return x_,y_\n",
    "\n",
    "def pretreatment(sentences):\n",
    "    # This function take a list of sentences and treat them to remove all punctuation mark and higher case\n",
    "    to_delete = [\",\",\".\",\"-\",\"!\",\"?\",\":\",\"*\",\")\",\"(\",\"'\",'\"',\"\\n\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        for t in to_delete:\n",
    "            sentences[i] = sentences[i].replace(t,\" \")\n",
    "\n",
    "    ret_sentences = []\n",
    "    for s in sentences:\n",
    "        ret_sentences.append(s.split(\" \"))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        ret_sentences[i]=[w for w in ret_sentences[i] if w!=\"\"]\n",
    "\n",
    "    for i in range(len(ret_sentences)):\n",
    "        for j in range(len(ret_sentences[i])) :\n",
    "            ret_sentences[i][j]=ret_sentences[i][j].lower()\n",
    "    return ret_sentences\n",
    "\n",
    "def vectorization(sentences,vectors,word_length=300,max_len=50,adaptable_len=False):\n",
    "    \"\"\" This function transform an array of list of words (the splitted sentences) into a array of list of vectors \n",
    "            representing the words thanks to an already trained word2Vec model\n",
    "            \n",
    "        Keyword arguments:\n",
    "        sentences -- The different sentences you want to vectorize\n",
    "        word2vec_model -- An already trained gensim Word2Vec model\n",
    "        word_length -- The number of dimension of the vectors representing the words (default 300) \n",
    "        max_len -- The maximum length of a sentence (default 50) \n",
    "        adaptable_len -- A boolean allowing to adapt the maximum length to the longest sentence in your dataset\n",
    "    \"\"\"\n",
    "    sentences_v=[]\n",
    "    unusued_word=[]\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import numpy as np\n",
    "    sp_words = set(stopwords.words('english'))    \n",
    "    \n",
    "    for s in sentences:\n",
    "        temp=[]\n",
    "        for w in s :\n",
    "            if(w not in sp_words):\n",
    "                try :\n",
    "                    temp.append(np.float32(vectors[w]))\n",
    "                except : \n",
    "                    unusued_word.append(w)\n",
    "        sentences_v.append(temp)\n",
    "        \n",
    "    if (adaptable_len == True):\n",
    "        for s in sentences_v:\n",
    "            if(len(s)>max_len):\n",
    "                max_len=len(s)\n",
    "                \n",
    "    for s in sentences_v:\n",
    "        sentence_length = len(s)\n",
    "        if(sentence_length<max_len):    \n",
    "            for j in range(max_len-sentence_length):\n",
    "                s.append([0]*word_length)\n",
    "    return sentences_v,max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_google_word2vec(vocabulary_size = 300000, path = 'lib/GoogleNews-vectors-negative300.bin'):\n",
    "    from gensim.models import KeyedVectors\n",
    "    # load the google word2vec model\n",
    "    model = KeyedVectors.load_word2vec_format(path, binary=True , limit=vocabulary_size)\n",
    "\n",
    "    # calculate: (king - man) + woman = ?\n",
    "    #result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "    #print(result)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/robin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12070\n"
     ]
    }
   ],
   "source": [
    "x,y = open_book()\n",
    "x,y = open_movie(x,y)\n",
    "print(len(x))\n",
    "model = load_google_word2vec()\n",
    "sentences_v,max_len = vectorization(pretreatment(x),model,word_length=300,adaptable_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension transformation for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sentences_v = np.array(sentences_v)\n",
    "sentences_v_svm = np.reshape(sentences_v,(sentences_v.shape[0],sentences_v.shape[1]*sentences_v.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Training of the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1107190, 2402790)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences, size=200, min_count=20, workers=4,sorted_vocab=1)\n",
    "model.train(sentences,total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of the already trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec.load('variables/review_rating/word2vec/300/model_300_Movie_elec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec.load('variables/review_rating/word2vec/200/model_200_elec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the variables for computation time gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.save(\"variables/sentences_v_google.npy\",sentences_v)\n",
    "np.save(\"variables/y.npy\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "sentences_v = np.load(\"variables/sentences_v_google.npy\")\n",
    "y = np.load(\"variables/y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting of the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "x_vec_train,x_vec_test,y_vec_train,y_vec_test = train_test_split(sentences_v,y,test_size=0.2,random_state=42)\n",
    "x_vec_train=np.array(x_vec_train)\n",
    "x_vec_test=np.array(x_vec_test)\n",
    "from keras.utils import to_categorical\n",
    "y_vec_train=to_categorical(y_vec_train)\n",
    "y_vec_test=to_categorical(y_vec_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting of the data into train and test set (svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "x_vec_train,x_vec_test,y_vec_train,y_vec_test = train_test_split(sentences_v_svm,y,test_size=0.2,random_state=42)\n",
    "x_vec_train=np.array(x_vec_train)\n",
    "x_vec_test=np.array(x_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sampling = 2000\n",
    "x_vec_train = x_vec_train[:sub_sampling]\n",
    "y_vec_train = y_vec_train[:sub_sampling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "def model_creation():\n",
    "    \n",
    "    WORD_LENGTH=300\n",
    "    max_len=50\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation,Convolution1D,Flatten,MaxPooling1D,Conv1D,LSTM\n",
    "    from keras.layers import Dropout\n",
    "\n",
    "    NN1 = Sequential()\n",
    "\n",
    "    NN1.add(Dense(max_len, activation='relu',input_shape=(max_len,WORD_LENGTH)))\n",
    "    NN1.add(Dropout(0.5))\n",
    "    \n",
    "    NN1.add(Convolution1D(64,kernel_size=10,activation='relu',border_mode='causal',input_shape=(max_len,WORD_LENGTH)))\n",
    "    NN1.add(Dropout(0.5))\n",
    "    \n",
    "    NN1.add(MaxPooling1D(pool_size=2, strides=None, padding='same'))\n",
    "\n",
    "    NN1.add(LSTM(50))\n",
    "    NN1.add(Dropout(0.5))\n",
    "    \n",
    "    #NN1.add(Flatten())    \n",
    "\n",
    "    NN1.add(Dense(1024, activation='relu'))\n",
    "    NN1.add(Dropout(0.5))\n",
    "       \n",
    "    NN1.add(Dense(1024, activation='relu'))\n",
    "    NN1.add(Dropout(0.5))\n",
    "       \n",
    "    NN1.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    NN1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "functions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions.append(model_creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=10, activation=\"relu\", input_shape=(50, 300), padding=\"causal\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 50, 50)            15050     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 50, 50)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 50, 64)            32064     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 50, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 26)                1326      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2)                 54        \n",
      "=================================================================\n",
      "Total params: 74,044\n",
      "Trainable params: 74,044\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN= model_creation()\n",
    "NN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting between test and valid (deep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de données de validation : 441\n",
      "Nombre de données de test : 441\n"
     ]
    }
   ],
   "source": [
    "x_vec_test,x_vec_valid,y_vec_test,y_vec_valid =  train_test_split(x_vec_test,y_vec_test,test_size=0.5,random_state=42)\n",
    "print(\"Nombre de données de validation : \"+str(len(x_vec_valid)))\n",
    "print(\"Nombre de données de test : \"+str(len(x_vec_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting between test and valid (svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de données de validation : 441\n",
      "Nombre de données de test : 441\n"
     ]
    }
   ],
   "source": [
    "x_vec_test,x_vec_valid,y_vec_test,y_vec_valid =  train_test_split(x_vec_test,y_vec_test,test_size=0.5,random_state=42)\n",
    "print(\"Nombre de données de validation : \"+str(len(x_vec_valid)))\n",
    "print(\"Nombre de données de test : \"+str(len(x_vec_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=10, activation=\"relu\", input_shape=(50, 300), padding=\"causal\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8690 samples, validate on 966 samples\n",
      "Epoch 1/10\n",
      "8690/8690 [==============================] - 9s 1ms/step - loss: 0.6589 - acc: 0.5747 - val_loss: 0.5303 - val_acc: 0.7588\n",
      "Epoch 2/10\n",
      "8690/8690 [==============================] - 8s 946us/step - loss: 0.5080 - acc: 0.7687 - val_loss: 0.4779 - val_acc: 0.7785\n",
      "Epoch 3/10\n",
      "8690/8690 [==============================] - 8s 976us/step - loss: 0.4651 - acc: 0.7876 - val_loss: 0.4508 - val_acc: 0.7816\n",
      "Epoch 4/10\n",
      "8690/8690 [==============================] - 9s 1ms/step - loss: 0.4476 - acc: 0.7994 - val_loss: 0.4502 - val_acc: 0.7795\n",
      "Epoch 5/10\n",
      "8690/8690 [==============================] - 9s 1ms/step - loss: 0.4287 - acc: 0.8100 - val_loss: 0.4467 - val_acc: 0.7857\n",
      "Epoch 6/10\n",
      "8690/8690 [==============================] - 10s 1ms/step - loss: 0.4087 - acc: 0.8189 - val_loss: 0.4350 - val_acc: 0.7888\n",
      "Epoch 7/10\n",
      "8690/8690 [==============================] - 10s 1ms/step - loss: 0.3987 - acc: 0.8252 - val_loss: 0.4426 - val_acc: 0.7836\n",
      "Epoch 8/10\n",
      "8690/8690 [==============================] - 9s 1ms/step - loss: 0.3846 - acc: 0.8304 - val_loss: 0.4424 - val_acc: 0.7950\n",
      "Epoch 9/10\n",
      "8690/8690 [==============================] - 8s 927us/step - loss: 0.3806 - acc: 0.8364 - val_loss: 0.4436 - val_acc: 0.7981\n",
      "Epoch 10/10\n",
      "8690/8690 [==============================] - 8s 947us/step - loss: 0.3687 - acc: 0.8343 - val_loss: 0.4374 - val_acc: 0.8012\n"
     ]
    }
   ],
   "source": [
    "NN = model_creation()\n",
    "hist_google = NN.fit(x_vec_train, y_vec_train, epochs=10, batch_size=64,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2414/2414 [==============================] - 1s 508us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.44020758145785865, 0.8024026512507081]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN.evaluate(x_vec_test,y_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7224523612261806"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = SVC()\n",
    "\n",
    "param_grid = [\n",
    "  {'C': [1,10], 'gamma': [0.01], 'kernel': ['rbf']}\n",
    " ]\n",
    "\n",
    "grid = GridSearchCV(clf,param_grid)\n",
    "grid.fit(x_vec_train,y_vec_train)\n",
    "grid.score(x_vec_test,y_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(C=1,gamma=0.1,kernel = \"rbf\")\n",
    "clf.fit(x_vec_train,y_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.596106048053024"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_vec_test,y_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting of validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(13,8))\n",
    "\n",
    "plt.plot(hist_google.history[\"val_acc\"],'r*-')\n",
    "plt.plot(hist_google.history[\"acc\"],'b*-')\n",
    "plt.legend((\"Validation accuracy with schedule\",\"Training accuracy with schedule\"))\n",
    "plt.savefig(\"documents/Acc_google_new\")\n",
    "plt.show()    \n",
    "\n",
    "plt.figure(figsize=(13,8))\n",
    "plt.plot(hist_google.history[\"val_loss\"],'r*-')\n",
    "plt.plot(hist_google.history[\"loss\"],'b*-')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the model on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=10, activation=\"relu\", input_shape=(50, 300), padding=\"causal\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_1_input to have 3 dimensions, but got array with shape (4410, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-89b9ce85722f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_creation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mFinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_1_input to have 3 dimensions, but got array with shape (4410, 50)"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "Final = model_creation()\n",
    "Final.fit(np.array(sentences_v),to_categorical(y),epochs=30,batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation and model comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_estimation_cross_val(nb_epochs,nb_s_epoch,nb_split):\n",
    "    nb_min = (nb_epochs*nb_s_epoch*nb_split*nb_split)/60\n",
    "    print(\"The cross_validation will take around : \"+str(nb_min)+\" mn\")\n",
    "    return nb_min\n",
    "    \n",
    "def cross_val(model_function,x_train,y_train,nb_splits,nb_epochs):\n",
    "    \"\"\" This function allows to do a cross validation over a keras model\n",
    "        Keyword arguments:\n",
    "        model_function -- A function returning your model\n",
    "        x_train -- The train data\n",
    "        y_train -- The train target\n",
    "        nb_splits -- The number of folds \n",
    "        nb_epochs -- The number of epochs you want to train your network with \n",
    "                     /!\\ One validation will be a total of nb_folds*nb_epochs total number of epochs\n",
    "    \"\"\"\n",
    "    import time    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    indices = np.linspace(0,len(x_train)-1,num=nb_splits+1,dtype=int)\n",
    "    split_data = []\n",
    "\n",
    "    # Creation of the splits\n",
    "    split_data.append((x_train[indices[0]:indices[1]],y_train[indices[0]:indices[1]]))\n",
    "    for i in range(1,len(indices)-1):\n",
    "        split_data.append((x_train[indices[i]+1:indices[i+1]],y_train[indices[i]+1:indices[i+1]]))\n",
    "\n",
    "    # Cross val    \n",
    "    score = []\n",
    "    for i in range(nb_splits):\n",
    "        model = model_function()\n",
    "        valid = split_data[i]\n",
    "        for j in range(nb_splits):\n",
    "            if (j != i):\n",
    "                model.fit(split_data[j][0],split_data[j][1],epochs=nb_epochs,batch_size=150)\n",
    "        score.append(model.evaluate(valid[0],valid[1]))\n",
    "        \n",
    "    std = np.std([a[1] for a in score])\n",
    "    mean = np.mean([a[1] for a in score])\n",
    "    \n",
    "    print(\"Accruracy neural network: \"+str(mean*100)+\"% +- \"+str(std*100)+\"%\")\n",
    "    print(\"--- %s minutes elapsed---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "    return model,score,(mean,std)\n",
    "\n",
    "\n",
    "def multi_cross_val(model_functions,x_train,y_train,nb_splits,nb_epochs):\n",
    "    \"\"\" This function allows to compare different models over a k-fold cross validation\n",
    "        Keyword arguments:\n",
    "        model_functions -- A list containing the functions returning the different models you want to test\n",
    "        x_train -- The train data\n",
    "        y_train -- The train target\n",
    "        nb_splits -- The number of folds \n",
    "        nb_epochs -- The number of epochs you want to train your network with \n",
    "                     /!\\ One validation will be a total of nb_folds*nb_epochs total number of epochs\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    indices = np.linspace(0,len(x_train)-1,num=nb_splits+1,dtype=int)\n",
    "    split_data = []\n",
    "\n",
    "    # Creeation of the splits\n",
    "    split_data.append((x_train[indices[0]:indices[1]],y_train[indices[0]:indices[1]]))\n",
    "    for i in range(1,len(indices)-1):\n",
    "        split_data.append((x_train[indices[i]+1:indices[i+1]],y_train[indices[i]+1:indices[i+1]]))\n",
    "\n",
    "    # Cross val    \n",
    "    score = []\n",
    "    for i in range(nb_splits):\n",
    "        print(\"Split no :\"+str(i))\n",
    "        models = [f() for f in model_functions]\n",
    "        valid = split_data[i]\n",
    "        for j in range(nb_splits):\n",
    "            if (j != i):\n",
    "                for m in models :\n",
    "                    try : \n",
    "                        m.fit(split_data[j][0],split_data[j][1],epochs=nb_epochs,batch_size=150)\n",
    "                    except :\n",
    "                        print(m.summary())\n",
    "        for k in range(len(models)) :\n",
    "            score.append((k,models[k].evaluate(valid[0],valid[1])))\n",
    "    std = {}\n",
    "    mean = {}\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        tmp = [s for s in score if s[0]==i]\n",
    "        std[i]=np.std([a[1][1] for a in tmp])\n",
    "        mean[i]=np.mean([a[1][1] for a in tmp])\n",
    "\n",
    "    for k in mean.keys():\n",
    "        print(\"Accruracy neural network\"+str(k) +\" : \"+str(mean[k]*100)+\"% +- \"+str(std[k]*100)+\"%\")\n",
    "    \n",
    "    print(\"--- %s minutes elapsed---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "    return models,score,mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_estimation_cross_val(80,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(multi_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=10, activation=\"relu\", input_shape=(50, 300), padding=\"causal\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.6936 - acc: 0.4996\n",
      "Epoch 2/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.6937 - acc: 0.4897\n",
      "Epoch 3/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.6876 - acc: 0.5405\n",
      "Epoch 4/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.6202 - acc: 0.6652\n",
      "Epoch 5/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.5893 - acc: 0.7137\n",
      "Epoch 6/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.5422 - acc: 0.7493\n",
      "Epoch 7/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.5132 - acc: 0.7720\n",
      "Epoch 8/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.5039 - acc: 0.7772\n",
      "Epoch 9/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.4881 - acc: 0.7819\n",
      "Epoch 10/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.4754 - acc: 0.7986\n",
      "Epoch 11/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.4620 - acc: 0.8003\n",
      "Epoch 12/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.4551 - acc: 0.8076\n",
      "Epoch 13/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.4505 - acc: 0.8048\n",
      "Epoch 14/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.4106 - acc: 0.8233\n",
      "Epoch 15/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.4169 - acc: 0.8165\n",
      "Epoch 16/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.4102 - acc: 0.8305\n",
      "Epoch 17/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.3915 - acc: 0.8364\n",
      "Epoch 18/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.3778 - acc: 0.8427\n",
      "Epoch 19/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.3948 - acc: 0.8384\n",
      "Epoch 20/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.3699 - acc: 0.8536\n",
      "Epoch 21/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.3674 - acc: 0.8521\n",
      "Epoch 22/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.3537 - acc: 0.8499\n",
      "Epoch 23/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.3327 - acc: 0.8653\n",
      "Epoch 24/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.3197 - acc: 0.8696\n",
      "Epoch 25/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.3080 - acc: 0.8733\n",
      "Epoch 26/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.3186 - acc: 0.8706\n",
      "Epoch 27/32\n",
      "4017/4017 [==============================] - 4s 1ms/step - loss: 0.3078 - acc: 0.8760\n",
      "Epoch 28/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.2866 - acc: 0.8795\n",
      "Epoch 29/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.3020 - acc: 0.8735\n",
      "Epoch 30/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.2953 - acc: 0.8805\n",
      "Epoch 31/32\n",
      "4017/4017 [==============================] - 5s 1ms/step - loss: 0.2809 - acc: 0.8877\n",
      "Epoch 32/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.2696 - acc: 0.8937\n",
      "Epoch 1/32\n",
      "4018/4018 [==============================] - 6s 1ms/step - loss: 0.5563 - acc: 0.7359\n",
      "Epoch 2/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.5046 - acc: 0.7633\n",
      "Epoch 3/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.4960 - acc: 0.7713\n",
      "Epoch 4/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.4763 - acc: 0.7745\n",
      "Epoch 5/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.4593 - acc: 0.7919\n",
      "Epoch 6/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.4708 - acc: 0.7894\n",
      "Epoch 7/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.4500 - acc: 0.7962\n",
      "Epoch 8/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.4396 - acc: 0.8074\n",
      "Epoch 9/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.4278 - acc: 0.8111\n",
      "Epoch 10/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.4126 - acc: 0.8186\n",
      "Epoch 11/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.4119 - acc: 0.8218\n",
      "Epoch 12/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.4038 - acc: 0.8238\n",
      "Epoch 13/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4142 - acc: 0.8218\n",
      "Epoch 14/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.3881 - acc: 0.8305\n",
      "Epoch 15/32\n",
      "4018/4018 [==============================] - 4s 1ms/step - loss: 0.3801 - acc: 0.8362\n",
      "Epoch 16/32\n",
      "4018/4018 [==============================] - 4s 1ms/step - loss: 0.3714 - acc: 0.8397\n",
      "Epoch 17/32\n",
      "4018/4018 [==============================] - 4s 1ms/step - loss: 0.3638 - acc: 0.8502\n",
      "Epoch 18/32\n",
      "4018/4018 [==============================] - 4s 1ms/step - loss: 0.3457 - acc: 0.8534\n",
      "Epoch 19/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.3728 - acc: 0.8437\n",
      "Epoch 20/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.3346 - acc: 0.8564\n",
      "Epoch 21/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.3404 - acc: 0.8596\n",
      "Epoch 22/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.3338 - acc: 0.8654\n",
      "Epoch 23/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.3286 - acc: 0.8586\n",
      "Epoch 24/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.3236 - acc: 0.8634\n",
      "Epoch 25/32\n",
      "4018/4018 [==============================] - 6s 1ms/step - loss: 0.3034 - acc: 0.8768\n",
      "Epoch 26/32\n",
      "4018/4018 [==============================] - 6s 1ms/step - loss: 0.3154 - acc: 0.8708\n",
      "Epoch 27/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.2878 - acc: 0.8850\n",
      "Epoch 28/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.2939 - acc: 0.8800\n",
      "Epoch 29/32\n",
      "4018/4018 [==============================] - 5s 1ms/step - loss: 0.2942 - acc: 0.8858\n",
      "Epoch 30/32\n",
      "4018/4018 [==============================] - 6s 1ms/step - loss: 0.2880 - acc: 0.8848\n",
      "Epoch 31/32\n",
      "4018/4018 [==============================] - 6s 1ms/step - loss: 0.2844 - acc: 0.8820\n",
      "Epoch 32/32\n",
      "4018/4018 [==============================] - 6s 1ms/step - loss: 0.2822 - acc: 0.8875\n",
      "4018/4018 [==============================] - 3s 781us/step\n",
      "Epoch 1/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.6933 - acc: 0.4954\n",
      "Epoch 2/32\n",
      "4018/4018 [==============================] - 6s 1ms/step - loss: 0.6934 - acc: 0.5110\n",
      "Epoch 3/32\n",
      "4018/4018 [==============================] - 6s 1ms/step - loss: 0.6906 - acc: 0.5261\n",
      "Epoch 4/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.6189 - acc: 0.6829\n",
      "Epoch 5/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.5740 - acc: 0.7332\n",
      "Epoch 6/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.5446 - acc: 0.7489\n",
      "Epoch 7/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.5322 - acc: 0.7598\n",
      "Epoch 8/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.5247 - acc: 0.7601\n",
      "Epoch 9/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.5151 - acc: 0.7837\n",
      "Epoch 10/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4897 - acc: 0.7872\n",
      "Epoch 11/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.4696 - acc: 0.7974\n",
      "Epoch 12/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4530 - acc: 0.7997\n",
      "Epoch 13/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4429 - acc: 0.8111\n",
      "Epoch 14/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.4320 - acc: 0.8126\n",
      "Epoch 15/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4220 - acc: 0.8255\n",
      "Epoch 16/32\n",
      "4018/4018 [==============================] - 10s 2ms/step - loss: 0.4273 - acc: 0.8275\n",
      "Epoch 17/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.4145 - acc: 0.8270\n",
      "Epoch 18/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4066 - acc: 0.8352\n",
      "Epoch 19/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.3955 - acc: 0.8425\n",
      "Epoch 20/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3977 - acc: 0.8415\n",
      "Epoch 21/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3831 - acc: 0.8462\n",
      "Epoch 22/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.3680 - acc: 0.8529\n",
      "Epoch 23/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.3614 - acc: 0.8492\n",
      "Epoch 24/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.3608 - acc: 0.8497\n",
      "Epoch 25/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.3580 - acc: 0.8552\n",
      "Epoch 26/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3347 - acc: 0.8654\n",
      "Epoch 27/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3436 - acc: 0.8601\n",
      "Epoch 28/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3385 - acc: 0.8601\n",
      "Epoch 29/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.3239 - acc: 0.8708\n",
      "Epoch 30/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.3228 - acc: 0.8711\n",
      "Epoch 31/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.3141 - acc: 0.8766\n",
      "Epoch 32/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.3187 - acc: 0.8746\n",
      "Epoch 1/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.5818 - acc: 0.7205\n",
      "Epoch 2/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.5203 - acc: 0.7554\n",
      "Epoch 3/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4914 - acc: 0.7725\n",
      "Epoch 4/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4804 - acc: 0.7830\n",
      "Epoch 5/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4738 - acc: 0.7875\n",
      "Epoch 6/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.4674 - acc: 0.7892\n",
      "Epoch 7/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.4496 - acc: 0.7982\n",
      "Epoch 8/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4502 - acc: 0.8024\n",
      "Epoch 9/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.4279 - acc: 0.8084\n",
      "Epoch 10/32\n",
      "4018/4018 [==============================] - 10s 3ms/step - loss: 0.4196 - acc: 0.8141\n",
      "Epoch 11/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.4158 - acc: 0.8173\n",
      "Epoch 12/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.4123 - acc: 0.8191\n",
      "Epoch 13/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.4016 - acc: 0.8255\n",
      "Epoch 14/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.3835 - acc: 0.8367\n",
      "Epoch 15/32\n",
      "4018/4018 [==============================] - 10s 2ms/step - loss: 0.3715 - acc: 0.8467\n",
      "Epoch 16/32\n",
      "4018/4018 [==============================] - 10s 2ms/step - loss: 0.3916 - acc: 0.8325\n",
      "Epoch 17/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.3722 - acc: 0.8457\n",
      "Epoch 18/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.3661 - acc: 0.8492\n",
      "Epoch 19/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.3480 - acc: 0.8532\n",
      "Epoch 20/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.3471 - acc: 0.8574\n",
      "Epoch 21/32\n",
      "4018/4018 [==============================] - 10s 2ms/step - loss: 0.3509 - acc: 0.8472\n",
      "Epoch 22/32\n",
      "4018/4018 [==============================] - 10s 2ms/step - loss: 0.3355 - acc: 0.8611\n",
      "Epoch 23/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.3296 - acc: 0.8616\n",
      "Epoch 24/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.3125 - acc: 0.8693\n",
      "Epoch 25/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.3079 - acc: 0.8731\n",
      "Epoch 26/32\n",
      "4018/4018 [==============================] - 10s 2ms/step - loss: 0.3025 - acc: 0.8738\n",
      "Epoch 27/32\n",
      "4018/4018 [==============================] - 9s 2ms/step - loss: 0.2925 - acc: 0.8771\n",
      "Epoch 28/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.2995 - acc: 0.8688\n",
      "Epoch 29/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.2881 - acc: 0.8778\n",
      "Epoch 30/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.2742 - acc: 0.8845\n",
      "Epoch 31/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.2642 - acc: 0.8917\n",
      "Epoch 32/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.2690 - acc: 0.8930\n",
      "4017/4017 [==============================] - 4s 909us/step\n",
      "Epoch 1/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.6935 - acc: 0.5002\n",
      "Epoch 2/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.6931 - acc: 0.5070\n",
      "Epoch 3/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.6932 - acc: 0.5274\n",
      "Epoch 4/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.6930 - acc: 0.5107\n",
      "Epoch 5/32\n",
      "4018/4018 [==============================] - 6s 1ms/step - loss: 0.6623 - acc: 0.5749\n",
      "Epoch 6/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.5928 - acc: 0.7096\n",
      "Epoch 7/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.5429 - acc: 0.7489\n",
      "Epoch 8/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.5244 - acc: 0.7554\n",
      "Epoch 9/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.5238 - acc: 0.7641\n",
      "Epoch 10/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.5059 - acc: 0.7713\n",
      "Epoch 11/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.4804 - acc: 0.7842\n",
      "Epoch 12/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.4569 - acc: 0.7894\n",
      "Epoch 13/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.4561 - acc: 0.8071\n",
      "Epoch 14/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.4455 - acc: 0.8106\n",
      "Epoch 15/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.4299 - acc: 0.8146\n",
      "Epoch 16/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.4336 - acc: 0.8116\n",
      "Epoch 17/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.4195 - acc: 0.8263\n",
      "Epoch 18/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3961 - acc: 0.8275\n",
      "Epoch 19/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.3832 - acc: 0.8422\n",
      "Epoch 20/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.3863 - acc: 0.8315\n",
      "Epoch 21/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.3915 - acc: 0.8278\n",
      "Epoch 22/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.3731 - acc: 0.8335\n",
      "Epoch 23/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3597 - acc: 0.8537\n",
      "Epoch 24/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3594 - acc: 0.8529\n",
      "Epoch 25/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.3454 - acc: 0.8527\n",
      "Epoch 26/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.3377 - acc: 0.8641\n",
      "Epoch 27/32\n",
      "4018/4018 [==============================] - 6s 1ms/step - loss: 0.3295 - acc: 0.8574\n",
      "Epoch 28/32\n",
      "4018/4018 [==============================] - 6s 2ms/step - loss: 0.3284 - acc: 0.8644\n",
      "Epoch 29/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3196 - acc: 0.8723\n",
      "Epoch 30/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3049 - acc: 0.8771\n",
      "Epoch 31/32\n",
      "4018/4018 [==============================] - 7s 2ms/step - loss: 0.3068 - acc: 0.8751\n",
      "Epoch 32/32\n",
      "4018/4018 [==============================] - 8s 2ms/step - loss: 0.2951 - acc: 0.8788\n",
      "Epoch 1/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.5898 - acc: 0.7194\n",
      "Epoch 2/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.5223 - acc: 0.7533\n",
      "Epoch 3/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.4948 - acc: 0.7670\n",
      "Epoch 4/32\n",
      "4017/4017 [==============================] - 6s 2ms/step - loss: 0.4856 - acc: 0.7755\n",
      "Epoch 5/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.4729 - acc: 0.7814\n",
      "Epoch 6/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.4584 - acc: 0.8003\n",
      "Epoch 7/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.4471 - acc: 0.7994\n",
      "Epoch 8/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.4443 - acc: 0.8061\n",
      "Epoch 9/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.4301 - acc: 0.8173\n",
      "Epoch 10/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.4141 - acc: 0.8165\n",
      "Epoch 11/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.4215 - acc: 0.8180\n",
      "Epoch 12/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3996 - acc: 0.8245\n",
      "Epoch 13/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3993 - acc: 0.8235\n",
      "Epoch 14/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3897 - acc: 0.8345\n",
      "Epoch 15/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3752 - acc: 0.8402\n",
      "Epoch 16/32\n",
      "4017/4017 [==============================] - 8s 2ms/step - loss: 0.3738 - acc: 0.8432\n",
      "Epoch 17/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3632 - acc: 0.8437\n",
      "Epoch 18/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3556 - acc: 0.8467\n",
      "Epoch 19/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3513 - acc: 0.8486\n",
      "Epoch 20/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3377 - acc: 0.8596\n",
      "Epoch 21/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3310 - acc: 0.8643\n",
      "Epoch 22/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3277 - acc: 0.8638\n",
      "Epoch 23/32\n",
      "4017/4017 [==============================] - 8s 2ms/step - loss: 0.3267 - acc: 0.8621\n",
      "Epoch 24/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3190 - acc: 0.8631\n",
      "Epoch 25/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3082 - acc: 0.8693\n",
      "Epoch 26/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3045 - acc: 0.8743\n",
      "Epoch 27/32\n",
      "4017/4017 [==============================] - 8s 2ms/step - loss: 0.2977 - acc: 0.8668\n",
      "Epoch 28/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.2980 - acc: 0.8815\n",
      "Epoch 29/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.3042 - acc: 0.8755\n",
      "Epoch 30/32\n",
      "4017/4017 [==============================] - 6s 2ms/step - loss: 0.2786 - acc: 0.8842\n",
      "Epoch 31/32\n",
      "4017/4017 [==============================] - 6s 2ms/step - loss: 0.2814 - acc: 0.8825\n",
      "Epoch 32/32\n",
      "4017/4017 [==============================] - 7s 2ms/step - loss: 0.2769 - acc: 0.8872\n",
      "4018/4018 [==============================] - 3s 843us/step\n",
      "Accruracy neural network: 75.16786856160755% +- 1.4628569936790061%\n",
      "--- 21.45231266816457 minutes elapsed---\n"
     ]
    }
   ],
   "source": [
    "NN,score, printing = cross_val(model_creation,x_vec_train,y_vec_train,3,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_functions = [model_creation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split no :0\n",
      "WARNING:tensorflow:From /home/robin/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=10, activation=\"relu\", input_shape=(50, 300), padding=\"causal\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 744us/step - loss: 0.7043 - acc: 0.5013\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 0s 171us/step - loss: 0.6944 - acc: 0.5115\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 0s 177us/step - loss: 0.6901 - acc: 0.5251\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 0s 167us/step - loss: 0.6858 - acc: 0.5370\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 0s 163us/step - loss: 0.6813 - acc: 0.5609\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 0s 162us/step - loss: 0.6807 - acc: 0.5762\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 0s 197us/step - loss: 0.6645 - acc: 0.6111\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 0s 230us/step - loss: 0.6585 - acc: 0.6009\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 0s 227us/step - loss: 0.6496 - acc: 0.6247\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 0s 207us/step - loss: 0.6251 - acc: 0.6306\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 0s 181us/step - loss: 0.6011 - acc: 0.6783\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 0s 174us/step - loss: 0.5668 - acc: 0.7140\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 0s 180us/step - loss: 0.5363 - acc: 0.7285\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 0s 193us/step - loss: 0.4897 - acc: 0.7762\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 0s 174us/step - loss: 0.4843 - acc: 0.7711\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 0s 160us/step - loss: 0.4713 - acc: 0.7932\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 0s 196us/step - loss: 0.4075 - acc: 0.8247\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 0s 185us/step - loss: 0.4071 - acc: 0.8264\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 0s 163us/step - loss: 0.3697 - acc: 0.8391\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 0s 163us/step - loss: 0.3595 - acc: 0.8511\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 0s 153us/step - loss: 0.3615 - acc: 0.8477\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 0s 152us/step - loss: 0.3399 - acc: 0.8587\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 0s 155us/step - loss: 0.3296 - acc: 0.8570\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 0s 155us/step - loss: 0.3072 - acc: 0.8749\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 0s 158us/step - loss: 0.2767 - acc: 0.8826\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 775us/step - loss: 0.7019 - acc: 0.4809\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 0s 369us/step - loss: 0.6960 - acc: 0.5021\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 0s 385us/step - loss: 0.6948 - acc: 0.4851\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 0s 396us/step - loss: 0.6925 - acc: 0.5030\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 0s 355us/step - loss: 0.6919 - acc: 0.5064\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 0s 395us/step - loss: 0.6896 - acc: 0.5311\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 0s 403us/step - loss: 0.6835 - acc: 0.5736\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 0s 381us/step - loss: 0.6792 - acc: 0.5719\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 0s 384us/step - loss: 0.6678 - acc: 0.6060\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 0s 387us/step - loss: 0.6561 - acc: 0.6136\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 0s 377us/step - loss: 0.6383 - acc: 0.6315\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 0s 373us/step - loss: 0.5998 - acc: 0.6885\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 0s 367us/step - loss: 0.5756 - acc: 0.7098\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 0s 425us/step - loss: 0.5378 - acc: 0.7549\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 0s 403us/step - loss: 0.5301 - acc: 0.7728\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 0s 357us/step - loss: 0.4809 - acc: 0.7770\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 0s 360us/step - loss: 0.4467 - acc: 0.8000\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 0s 348us/step - loss: 0.4467 - acc: 0.8085\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 0s 354us/step - loss: 0.4382 - acc: 0.8111\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 0s 425us/step - loss: 0.4299 - acc: 0.8204\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 0s 403us/step - loss: 0.4083 - acc: 0.8255\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 436us/step - loss: 0.4012 - acc: 0.8349\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 0s 421us/step - loss: 0.3650 - acc: 0.8562\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 0s 423us/step - loss: 0.3535 - acc: 0.8536\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 430us/step - loss: 0.3235 - acc: 0.8664\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6934 - acc: 0.5009\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 562us/step - loss: 0.6930 - acc: 0.5081\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 567us/step - loss: 0.6932 - acc: 0.5038\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 508us/step - loss: 0.6933 - acc: 0.4953\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 493us/step - loss: 0.6936 - acc: 0.5072\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 493us/step - loss: 0.6926 - acc: 0.5013\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 502us/step - loss: 0.6904 - acc: 0.5455\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 491us/step - loss: 0.6831 - acc: 0.5770\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 503us/step - loss: 0.6620 - acc: 0.6136\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 538us/step - loss: 0.6253 - acc: 0.7030\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 531us/step - loss: 0.6224 - acc: 0.6843\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 537us/step - loss: 0.6015 - acc: 0.7319\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 507us/step - loss: 0.5981 - acc: 0.7021\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 493us/step - loss: 0.5626 - acc: 0.7566\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 552us/step - loss: 0.5411 - acc: 0.7668\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 582us/step - loss: 0.5200 - acc: 0.7906\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 588us/step - loss: 0.4979 - acc: 0.7889\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 590us/step - loss: 0.4690 - acc: 0.8000\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 565us/step - loss: 0.4791 - acc: 0.8179\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 559us/step - loss: 0.4520 - acc: 0.8068\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 588us/step - loss: 0.4484 - acc: 0.8170\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 565us/step - loss: 0.4244 - acc: 0.8323\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 590us/step - loss: 0.4534 - acc: 0.8213\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 553us/step - loss: 0.4124 - acc: 0.8357\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 580us/step - loss: 0.4120 - acc: 0.8417\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6934 - acc: 0.4932\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 632us/step - loss: 0.6940 - acc: 0.4647\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 713us/step - loss: 0.6935 - acc: 0.5072\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 607us/step - loss: 0.6926 - acc: 0.5174\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 622us/step - loss: 0.6937 - acc: 0.4723\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 632us/step - loss: 0.6931 - acc: 0.5115\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 595us/step - loss: 0.6928 - acc: 0.5021\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 685us/step - loss: 0.6932 - acc: 0.5004\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 646us/step - loss: 0.6925 - acc: 0.5072\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 574us/step - loss: 0.6940 - acc: 0.4834\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 606us/step - loss: 0.6932 - acc: 0.4996\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 627us/step - loss: 0.6935 - acc: 0.4945\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 650us/step - loss: 0.6921 - acc: 0.5251\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 644us/step - loss: 0.6931 - acc: 0.4945\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 585us/step - loss: 0.6933 - acc: 0.5013\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 702us/step - loss: 0.6940 - acc: 0.4970\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 693us/step - loss: 0.6930 - acc: 0.5209\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 647us/step - loss: 0.6927 - acc: 0.5064\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 548us/step - loss: 0.6931 - acc: 0.5140\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 547us/step - loss: 0.6936 - acc: 0.5030\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 586us/step - loss: 0.6932 - acc: 0.5072\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 598us/step - loss: 0.6922 - acc: 0.5277\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 594us/step - loss: 0.6878 - acc: 0.5464\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 637us/step - loss: 0.6565 - acc: 0.6374\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 661us/step - loss: 0.6135 - acc: 0.7047\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6926 - acc: 0.5238\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 637us/step - loss: 0.6936 - acc: 0.4987\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 658us/step - loss: 0.6936 - acc: 0.5055\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 637us/step - loss: 0.6946 - acc: 0.4894\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 654us/step - loss: 0.6932 - acc: 0.5157\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 657us/step - loss: 0.6943 - acc: 0.4928\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 642us/step - loss: 0.6928 - acc: 0.5081\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 647us/step - loss: 0.6933 - acc: 0.4843\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 647us/step - loss: 0.6926 - acc: 0.5157\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 667us/step - loss: 0.6928 - acc: 0.4979\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 639us/step - loss: 0.6872 - acc: 0.5345\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 647us/step - loss: 0.6438 - acc: 0.6374\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 689us/step - loss: 0.6129 - acc: 0.6698\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 656us/step - loss: 0.5604 - acc: 0.7489\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 655us/step - loss: 0.5552 - acc: 0.7430\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 625us/step - loss: 0.5182 - acc: 0.7685\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 670us/step - loss: 0.4947 - acc: 0.7966\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 644us/step - loss: 0.4684 - acc: 0.7957\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 640us/step - loss: 0.4630 - acc: 0.8128\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 649us/step - loss: 0.4669 - acc: 0.8145\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 710us/step - loss: 0.4358 - acc: 0.8332\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 690us/step - loss: 0.4433 - acc: 0.8391\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 768us/step - loss: 0.4081 - acc: 0.8383\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 858us/step - loss: 0.3737 - acc: 0.8468\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 809us/step - loss: 0.3952 - acc: 0.8443\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 3s 3ms/step - loss: 0.6932 - acc: 0.5077\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6937 - acc: 0.5013\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6930 - acc: 0.5081\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6934 - acc: 0.4843\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6940 - acc: 0.4528\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6932 - acc: 0.4962\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6933 - acc: 0.5047\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6924 - acc: 0.5200\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6839 - acc: 0.5966\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6145 - acc: 0.6706\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5803 - acc: 0.7345\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5378 - acc: 0.7472\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5361 - acc: 0.7634\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5004 - acc: 0.7915\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4472 - acc: 0.8170\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4561 - acc: 0.8162\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4415 - acc: 0.8077\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4130 - acc: 0.8255\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4634 - acc: 0.8221\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4010 - acc: 0.8357\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3927 - acc: 0.8528\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3760 - acc: 0.8638\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3705 - acc: 0.8604\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3453 - acc: 0.8553\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3629 - acc: 0.8655\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 3s 2ms/step - loss: 0.6937 - acc: 0.4774\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6938 - acc: 0.4979\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6936 - acc: 0.4826\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6933 - acc: 0.5132\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6928 - acc: 0.4996\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6948 - acc: 0.4902\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6934 - acc: 0.5072\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6935 - acc: 0.4987\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6927 - acc: 0.5081\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6927 - acc: 0.5089\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6927 - acc: 0.4800\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6803 - acc: 0.5038\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6344 - acc: 0.6817\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5931 - acc: 0.7311\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5390 - acc: 0.7523\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.5416 - acc: 0.7413\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4950 - acc: 0.7847\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.4570 - acc: 0.8009\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.4462 - acc: 0.8017\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4472 - acc: 0.8009\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4325 - acc: 0.8332\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4341 - acc: 0.8366\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3633 - acc: 0.8502\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3681 - acc: 0.8409\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3577 - acc: 0.8570\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 0s 195us/step - loss: 0.6035 - acc: 0.7464\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 0s 197us/step - loss: 0.4893 - acc: 0.7609\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 0s 201us/step - loss: 0.4581 - acc: 0.7753\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 0s 205us/step - loss: 0.4445 - acc: 0.8068\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 0s 192us/step - loss: 0.4470 - acc: 0.7898\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 0s 187us/step - loss: 0.4234 - acc: 0.8153\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 0s 196us/step - loss: 0.3687 - acc: 0.8443\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 0s 209us/step - loss: 0.3911 - acc: 0.8323\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 0s 206us/step - loss: 0.3630 - acc: 0.8332\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 0s 198us/step - loss: 0.3608 - acc: 0.8460\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 0s 194us/step - loss: 0.3423 - acc: 0.8579\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 0s 193us/step - loss: 0.3250 - acc: 0.8638\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 0s 196us/step - loss: 0.3032 - acc: 0.8698\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 0s 195us/step - loss: 0.3050 - acc: 0.8740\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 0s 196us/step - loss: 0.2920 - acc: 0.8791\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 0s 186us/step - loss: 0.2477 - acc: 0.9021\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 0s 198us/step - loss: 0.2592 - acc: 0.8936\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 0s 200us/step - loss: 0.2461 - acc: 0.8911\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 0s 199us/step - loss: 0.2525 - acc: 0.9021\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 0s 192us/step - loss: 0.2519 - acc: 0.8911\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 0s 196us/step - loss: 0.2250 - acc: 0.9123\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 0s 199us/step - loss: 0.2354 - acc: 0.9064\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 0s 195us/step - loss: 0.1968 - acc: 0.9243\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 0s 184us/step - loss: 0.2337 - acc: 0.9081\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 0s 202us/step - loss: 0.1942 - acc: 0.9243\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 497us/step - loss: 0.5351 - acc: 0.7532\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 508us/step - loss: 0.4803 - acc: 0.7753\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 478us/step - loss: 0.4567 - acc: 0.7787\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 495us/step - loss: 0.4456 - acc: 0.7881\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 514us/step - loss: 0.4358 - acc: 0.8026\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 494us/step - loss: 0.4098 - acc: 0.8162\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 496us/step - loss: 0.3989 - acc: 0.8238\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 473us/step - loss: 0.3716 - acc: 0.8417\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 485us/step - loss: 0.3559 - acc: 0.8460\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 473us/step - loss: 0.3596 - acc: 0.8426\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 469us/step - loss: 0.3316 - acc: 0.8579\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 483us/step - loss: 0.3379 - acc: 0.8426\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 494us/step - loss: 0.3108 - acc: 0.8732\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 495us/step - loss: 0.2813 - acc: 0.8791\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 483us/step - loss: 0.2711 - acc: 0.8928\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 490us/step - loss: 0.2704 - acc: 0.8868\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 499us/step - loss: 0.2762 - acc: 0.8826\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 524us/step - loss: 0.2675 - acc: 0.8987\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 472us/step - loss: 0.2550 - acc: 0.8962\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 480us/step - loss: 0.2648 - acc: 0.8962\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 487us/step - loss: 0.2493 - acc: 0.9038\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 481us/step - loss: 0.2276 - acc: 0.9089\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 485us/step - loss: 0.2220 - acc: 0.9055\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 492us/step - loss: 0.2356 - acc: 0.9072\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 507us/step - loss: 0.1985 - acc: 0.9336\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 639us/step - loss: 0.5172 - acc: 0.7600\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 636us/step - loss: 0.4641 - acc: 0.7889\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 628us/step - loss: 0.4595 - acc: 0.7787\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 655us/step - loss: 0.4637 - acc: 0.7991\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 628us/step - loss: 0.4537 - acc: 0.7991\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 631us/step - loss: 0.4136 - acc: 0.8119\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 651us/step - loss: 0.4015 - acc: 0.8221\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 631us/step - loss: 0.4205 - acc: 0.8034\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 644us/step - loss: 0.3996 - acc: 0.8332\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 639us/step - loss: 0.3729 - acc: 0.8485\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 641us/step - loss: 0.3840 - acc: 0.8281\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 632us/step - loss: 0.3716 - acc: 0.8340\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 639us/step - loss: 0.3820 - acc: 0.8391\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 643us/step - loss: 0.3433 - acc: 0.8613\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 723us/step - loss: 0.3569 - acc: 0.8485\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 669us/step - loss: 0.3516 - acc: 0.8409\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 674us/step - loss: 0.3514 - acc: 0.8502\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 680us/step - loss: 0.3252 - acc: 0.8647\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 682us/step - loss: 0.2911 - acc: 0.8732\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 699us/step - loss: 0.2820 - acc: 0.8757\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 715us/step - loss: 0.2945 - acc: 0.8757\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 719us/step - loss: 0.3178 - acc: 0.8723\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 717us/step - loss: 0.2905 - acc: 0.8732\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 769us/step - loss: 0.2853 - acc: 0.8843\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 849us/step - loss: 0.2613 - acc: 0.8936\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 836us/step - loss: 0.6334 - acc: 0.6494\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 752us/step - loss: 0.5900 - acc: 0.7038\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 672us/step - loss: 0.5703 - acc: 0.7370\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 668us/step - loss: 0.5236 - acc: 0.7617\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 677us/step - loss: 0.4984 - acc: 0.7779\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 734us/step - loss: 0.4683 - acc: 0.8051\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 684us/step - loss: 0.4295 - acc: 0.8162\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 695us/step - loss: 0.4096 - acc: 0.8264\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 685us/step - loss: 0.4004 - acc: 0.8323\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 679us/step - loss: 0.3827 - acc: 0.8451\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 675us/step - loss: 0.3630 - acc: 0.8451\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 701us/step - loss: 0.3875 - acc: 0.8400\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 719us/step - loss: 0.3497 - acc: 0.8613\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 737us/step - loss: 0.3290 - acc: 0.8655\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 742us/step - loss: 0.3325 - acc: 0.8706\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 737us/step - loss: 0.3072 - acc: 0.8613\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 736us/step - loss: 0.3170 - acc: 0.8681\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 691us/step - loss: 0.2692 - acc: 0.8962\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 707us/step - loss: 0.2992 - acc: 0.8877\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 756us/step - loss: 0.2829 - acc: 0.8851\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 734us/step - loss: 0.2782 - acc: 0.8809\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 669us/step - loss: 0.2436 - acc: 0.8945\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 662us/step - loss: 0.2559 - acc: 0.9081\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 718us/step - loss: 0.2224 - acc: 0.9123\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 759us/step - loss: 0.2315 - acc: 0.9072\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 904us/step - loss: 0.4914 - acc: 0.7762\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 829us/step - loss: 0.4780 - acc: 0.7779\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 741us/step - loss: 0.4583 - acc: 0.7983\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 728us/step - loss: 0.4407 - acc: 0.7932\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 762us/step - loss: 0.4141 - acc: 0.8187\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 721us/step - loss: 0.4348 - acc: 0.8162\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 750us/step - loss: 0.3991 - acc: 0.8349\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 714us/step - loss: 0.4024 - acc: 0.8383\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 730us/step - loss: 0.4056 - acc: 0.8289\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 756us/step - loss: 0.3614 - acc: 0.8451\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 751us/step - loss: 0.3592 - acc: 0.8477\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 723us/step - loss: 0.3533 - acc: 0.8460\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 729us/step - loss: 0.3392 - acc: 0.8468\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 710us/step - loss: 0.3431 - acc: 0.8451\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 712us/step - loss: 0.3634 - acc: 0.8443\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 711us/step - loss: 0.3402 - acc: 0.8613\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 711us/step - loss: 0.3076 - acc: 0.8621\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 713us/step - loss: 0.3002 - acc: 0.8655\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 716us/step - loss: 0.2614 - acc: 0.8894\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 708us/step - loss: 0.2856 - acc: 0.8945\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 725us/step - loss: 0.2585 - acc: 0.8919\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 710us/step - loss: 0.2507 - acc: 0.9038\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 708us/step - loss: 0.2293 - acc: 0.9038\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 724us/step - loss: 0.2470 - acc: 0.9013\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 709us/step - loss: 0.2323 - acc: 0.9098\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 921us/step - loss: 0.5322 - acc: 0.7677\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 919us/step - loss: 0.4899 - acc: 0.7787\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 928us/step - loss: 0.4800 - acc: 0.7753\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 916us/step - loss: 0.4484 - acc: 0.7940\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 917us/step - loss: 0.4167 - acc: 0.8077\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 932us/step - loss: 0.4274 - acc: 0.8085\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 944us/step - loss: 0.4081 - acc: 0.8179\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 895us/step - loss: 0.3704 - acc: 0.8400\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 959us/step - loss: 0.3916 - acc: 0.8349\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 883us/step - loss: 0.3678 - acc: 0.8391\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 911us/step - loss: 0.3762 - acc: 0.8340\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 919us/step - loss: 0.3381 - acc: 0.8485\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 924us/step - loss: 0.3422 - acc: 0.8528\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 908us/step - loss: 0.3552 - acc: 0.8511\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 898us/step - loss: 0.3118 - acc: 0.8613\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 900us/step - loss: 0.3228 - acc: 0.8706\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 881us/step - loss: 0.2929 - acc: 0.8757\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 886us/step - loss: 0.2943 - acc: 0.8740\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 884us/step - loss: 0.2775 - acc: 0.8783\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 890us/step - loss: 0.2558 - acc: 0.8843\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 898us/step - loss: 0.2477 - acc: 0.8962\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 871us/step - loss: 0.2613 - acc: 0.8936\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 886us/step - loss: 0.2403 - acc: 0.9030\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 945us/step - loss: 0.2168 - acc: 0.9030\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 913us/step - loss: 0.2288 - acc: 0.9064\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5144 - acc: 0.7617\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4937 - acc: 0.7736\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4797 - acc: 0.7762\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4547 - acc: 0.8017\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4482 - acc: 0.7949\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4176 - acc: 0.8085\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4353 - acc: 0.8111\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3857 - acc: 0.8179\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3702 - acc: 0.8391\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3588 - acc: 0.8477\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3346 - acc: 0.8570\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3205 - acc: 0.8485\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3361 - acc: 0.8596\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3081 - acc: 0.8715\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2763 - acc: 0.8894\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2739 - acc: 0.8962\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2872 - acc: 0.8851\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2680 - acc: 0.8970\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2589 - acc: 0.8996\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2645 - acc: 0.8962\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2352 - acc: 0.9064\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2352 - acc: 0.9047\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2344 - acc: 0.9140\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2240 - acc: 0.9089\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2213 - acc: 0.9140\n",
      "1175/1175 [==============================] - 1s 491us/step\n",
      "1175/1175 [==============================] - 1s 558us/step\n",
      "1175/1175 [==============================] - 1s 697us/step\n",
      "1175/1175 [==============================] - 1s 705us/step\n",
      "1175/1175 [==============================] - 1s 755us/step\n",
      "1175/1175 [==============================] - 1s 807us/step\n",
      "1175/1175 [==============================] - 1s 924us/step\n",
      "Split no :1\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.7092 - acc: 0.4609\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 0s 164us/step - loss: 0.6940 - acc: 0.5140\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 0s 172us/step - loss: 0.6889 - acc: 0.5540\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 0s 176us/step - loss: 0.6879 - acc: 0.5583\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 0s 170us/step - loss: 0.6868 - acc: 0.5660\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 0s 172us/step - loss: 0.6825 - acc: 0.5779\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 0s 174us/step - loss: 0.6760 - acc: 0.5762\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 0s 179us/step - loss: 0.6605 - acc: 0.6179\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 0s 176us/step - loss: 0.6454 - acc: 0.6417\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 0s 182us/step - loss: 0.6225 - acc: 0.6604\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 0s 179us/step - loss: 0.6032 - acc: 0.6800\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 0s 188us/step - loss: 0.5631 - acc: 0.7149\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 0s 175us/step - loss: 0.5291 - acc: 0.7302\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 0s 191us/step - loss: 0.4889 - acc: 0.7702\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 0s 179us/step - loss: 0.4497 - acc: 0.7847\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 0s 184us/step - loss: 0.4428 - acc: 0.7906\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 0s 180us/step - loss: 0.4047 - acc: 0.8204\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 0s 182us/step - loss: 0.3999 - acc: 0.8340\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 0s 173us/step - loss: 0.3589 - acc: 0.8519\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 0s 181us/step - loss: 0.3395 - acc: 0.8477\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 0s 180us/step - loss: 0.3172 - acc: 0.8647\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 0s 178us/step - loss: 0.3263 - acc: 0.8706\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 0s 181us/step - loss: 0.3006 - acc: 0.8809\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 0s 185us/step - loss: 0.2822 - acc: 0.8809\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 0s 190us/step - loss: 0.2885 - acc: 0.8817\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.6993 - acc: 0.4987\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 463us/step - loss: 0.6995 - acc: 0.4979\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 466us/step - loss: 0.6883 - acc: 0.5379\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 475us/step - loss: 0.6872 - acc: 0.5260\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 481us/step - loss: 0.6902 - acc: 0.5174\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 479us/step - loss: 0.6820 - acc: 0.5370\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 472us/step - loss: 0.6735 - acc: 0.5574\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 467us/step - loss: 0.6578 - acc: 0.5728\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 483us/step - loss: 0.6530 - acc: 0.5770\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 468us/step - loss: 0.6153 - acc: 0.6400\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 486us/step - loss: 0.5820 - acc: 0.6911\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 471us/step - loss: 0.5426 - acc: 0.7098\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 471us/step - loss: 0.5464 - acc: 0.7345\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 489us/step - loss: 0.5096 - acc: 0.7609\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 479us/step - loss: 0.4734 - acc: 0.7830\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 477us/step - loss: 0.4604 - acc: 0.7855\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 501us/step - loss: 0.4275 - acc: 0.8009\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 483us/step - loss: 0.4244 - acc: 0.8247\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 487us/step - loss: 0.4162 - acc: 0.8136\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 482us/step - loss: 0.3954 - acc: 0.8289\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 476us/step - loss: 0.3563 - acc: 0.8528\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 481us/step - loss: 0.3858 - acc: 0.8306\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 505us/step - loss: 0.3479 - acc: 0.8409\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 481us/step - loss: 0.3400 - acc: 0.8417\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 485us/step - loss: 0.3068 - acc: 0.8706\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.6923 - acc: 0.5387\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 641us/step - loss: 0.6896 - acc: 0.5404\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 667us/step - loss: 0.6918 - acc: 0.5396\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 651us/step - loss: 0.6898 - acc: 0.5396\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 675us/step - loss: 0.6914 - acc: 0.5387\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 645us/step - loss: 0.6891 - acc: 0.5396\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 664us/step - loss: 0.6900 - acc: 0.5396\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 639us/step - loss: 0.6889 - acc: 0.5396\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 647us/step - loss: 0.6870 - acc: 0.5396\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 652us/step - loss: 0.6773 - acc: 0.5387\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 692us/step - loss: 0.6417 - acc: 0.5396\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 682us/step - loss: 0.6086 - acc: 0.5583\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 650us/step - loss: 0.5845 - acc: 0.6477\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 671us/step - loss: 0.5803 - acc: 0.7574\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 661us/step - loss: 0.5787 - acc: 0.6945\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 675us/step - loss: 0.5575 - acc: 0.7847\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 675us/step - loss: 0.5217 - acc: 0.7770\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 697us/step - loss: 0.5215 - acc: 0.7813\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 669us/step - loss: 0.5034 - acc: 0.7660\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 667us/step - loss: 0.4930 - acc: 0.8026\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 662us/step - loss: 0.4968 - acc: 0.7506\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 645us/step - loss: 0.4529 - acc: 0.8170\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 659us/step - loss: 0.4497 - acc: 0.8060\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 659us/step - loss: 0.4345 - acc: 0.8349\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 622us/step - loss: 0.4239 - acc: 0.8230\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.6924 - acc: 0.5298\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 685us/step - loss: 0.6893 - acc: 0.5251\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 678us/step - loss: 0.6894 - acc: 0.5345\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 676us/step - loss: 0.6891 - acc: 0.5370\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 677us/step - loss: 0.6899 - acc: 0.5396\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 671us/step - loss: 0.6859 - acc: 0.5370\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 677us/step - loss: 0.6697 - acc: 0.5345\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 686us/step - loss: 0.6273 - acc: 0.5634\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 700us/step - loss: 0.6287 - acc: 0.5855\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 700us/step - loss: 0.6106 - acc: 0.7021\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 667us/step - loss: 0.5634 - acc: 0.7404\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 675us/step - loss: 0.5506 - acc: 0.7600\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 676us/step - loss: 0.5580 - acc: 0.7362\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 692us/step - loss: 0.5181 - acc: 0.7940\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 666us/step - loss: 0.4906 - acc: 0.7677\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 677us/step - loss: 0.5017 - acc: 0.7804\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 705us/step - loss: 0.5006 - acc: 0.7796\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 672us/step - loss: 0.4583 - acc: 0.8034\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 668us/step - loss: 0.4554 - acc: 0.7974\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 690us/step - loss: 0.4335 - acc: 0.8119\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 682us/step - loss: 0.4372 - acc: 0.8128\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 665us/step - loss: 0.4278 - acc: 0.8196\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 677us/step - loss: 0.4094 - acc: 0.8264\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 681us/step - loss: 0.3791 - acc: 0.8417\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 695us/step - loss: 0.3739 - acc: 0.8443\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 3s 2ms/step - loss: 0.6931 - acc: 0.5183\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 713us/step - loss: 0.6923 - acc: 0.5438\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 710us/step - loss: 0.6909 - acc: 0.5506\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 723us/step - loss: 0.6906 - acc: 0.5387\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 733us/step - loss: 0.6901 - acc: 0.5396\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 721us/step - loss: 0.6926 - acc: 0.5421\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 724us/step - loss: 0.6891 - acc: 0.5421\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 724us/step - loss: 0.6911 - acc: 0.5370\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 711us/step - loss: 0.6904 - acc: 0.5413\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 724us/step - loss: 0.6911 - acc: 0.5387\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 731us/step - loss: 0.6909 - acc: 0.5396\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 714us/step - loss: 0.6874 - acc: 0.5379\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 714us/step - loss: 0.6732 - acc: 0.5251\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 718us/step - loss: 0.6172 - acc: 0.5957\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 738us/step - loss: 0.6100 - acc: 0.6681\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 747us/step - loss: 0.5654 - acc: 0.6945\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 717us/step - loss: 0.5525 - acc: 0.7362\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 721us/step - loss: 0.5345 - acc: 0.7591\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 698us/step - loss: 0.5088 - acc: 0.7651\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 721us/step - loss: 0.4957 - acc: 0.7906\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 724us/step - loss: 0.4629 - acc: 0.7983\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 724us/step - loss: 0.4398 - acc: 0.8153\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 750us/step - loss: 0.4162 - acc: 0.8255\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 747us/step - loss: 0.4173 - acc: 0.8213\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 738us/step - loss: 0.3927 - acc: 0.8340\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 3s 3ms/step - loss: 0.6926 - acc: 0.5251\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 899us/step - loss: 0.6918 - acc: 0.5302\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 919us/step - loss: 0.6920 - acc: 0.5328\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 920us/step - loss: 0.6914 - acc: 0.5370\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 953us/step - loss: 0.6904 - acc: 0.5396\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 952us/step - loss: 0.6921 - acc: 0.5379\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 919us/step - loss: 0.6916 - acc: 0.5396\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 913us/step - loss: 0.6906 - acc: 0.5396\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 907us/step - loss: 0.6833 - acc: 0.5345\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 938us/step - loss: 0.6550 - acc: 0.5804\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 960us/step - loss: 0.6091 - acc: 0.6757\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 914us/step - loss: 0.5627 - acc: 0.7285\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 923us/step - loss: 0.5196 - acc: 0.7770\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 947us/step - loss: 0.5120 - acc: 0.7609\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 915us/step - loss: 0.4741 - acc: 0.7872\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 907us/step - loss: 0.4622 - acc: 0.7974\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 910us/step - loss: 0.4638 - acc: 0.7966\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 930us/step - loss: 0.4456 - acc: 0.8102\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 933us/step - loss: 0.4309 - acc: 0.8315\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 917us/step - loss: 0.4185 - acc: 0.8187\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 915us/step - loss: 0.4176 - acc: 0.8179\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 915us/step - loss: 0.3661 - acc: 0.8332\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 934us/step - loss: 0.3291 - acc: 0.8647\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 922us/step - loss: 0.3569 - acc: 0.8511\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 924us/step - loss: 0.3515 - acc: 0.8451\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 3s 3ms/step - loss: 0.6933 - acc: 0.5051\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6918 - acc: 0.5260\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6919 - acc: 0.5328\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6927 - acc: 0.5353\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6911 - acc: 0.5396\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6901 - acc: 0.5396\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6903 - acc: 0.5396\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6885 - acc: 0.5447\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6892 - acc: 0.5277\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6596 - acc: 0.5651\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6117 - acc: 0.6689\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.5653 - acc: 0.7336\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.5240 - acc: 0.7711\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.5359 - acc: 0.7455\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4820 - acc: 0.7711\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.5078 - acc: 0.7898\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4585 - acc: 0.7974\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4293 - acc: 0.8153\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3918 - acc: 0.8426\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3989 - acc: 0.8323\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 3s 2ms/step - loss: 0.3522 - acc: 0.8451\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3769 - acc: 0.8460\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3540 - acc: 0.8451\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3229 - acc: 0.8494\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.2956 - acc: 0.8647\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 0s 238us/step - loss: 0.5993 - acc: 0.7421\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 0s 233us/step - loss: 0.5294 - acc: 0.7379\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 0s 240us/step - loss: 0.4810 - acc: 0.7634\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 0s 248us/step - loss: 0.4586 - acc: 0.7753\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 0s 233us/step - loss: 0.4416 - acc: 0.7957\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 0s 234us/step - loss: 0.4244 - acc: 0.8034\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 0s 209us/step - loss: 0.4110 - acc: 0.8102\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 0s 192us/step - loss: 0.3777 - acc: 0.8289\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 0s 191us/step - loss: 0.3639 - acc: 0.8340\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 0s 189us/step - loss: 0.3747 - acc: 0.8383\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 0s 197us/step - loss: 0.3480 - acc: 0.8553\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 0s 197us/step - loss: 0.3219 - acc: 0.8647\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 0s 194us/step - loss: 0.2996 - acc: 0.8732\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 0s 188us/step - loss: 0.3108 - acc: 0.8698\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 0s 194us/step - loss: 0.2956 - acc: 0.8732\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 0s 196us/step - loss: 0.2809 - acc: 0.8826\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 0s 186us/step - loss: 0.2880 - acc: 0.8698\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 0s 189us/step - loss: 0.2727 - acc: 0.8851\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 0s 194us/step - loss: 0.2695 - acc: 0.9013\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 0s 188us/step - loss: 0.2417 - acc: 0.8919\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 0s 188us/step - loss: 0.2770 - acc: 0.8800\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 0s 189us/step - loss: 0.2443 - acc: 0.8928\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 0s 202us/step - loss: 0.2278 - acc: 0.9123\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 0s 192us/step - loss: 0.2279 - acc: 0.9064\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 0s 180us/step - loss: 0.2387 - acc: 0.9089\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 477us/step - loss: 0.5906 - acc: 0.7464\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 461us/step - loss: 0.4723 - acc: 0.7881\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 460us/step - loss: 0.4615 - acc: 0.7872\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 475us/step - loss: 0.4469 - acc: 0.8068\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 468us/step - loss: 0.4286 - acc: 0.8009\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 483us/step - loss: 0.4152 - acc: 0.8094\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 463us/step - loss: 0.4147 - acc: 0.8170\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 469us/step - loss: 0.3873 - acc: 0.8255\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 460us/step - loss: 0.3895 - acc: 0.8357\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 473us/step - loss: 0.3726 - acc: 0.8460\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 475us/step - loss: 0.3396 - acc: 0.8604\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 469us/step - loss: 0.3410 - acc: 0.8596\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 464us/step - loss: 0.3233 - acc: 0.8604\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 469us/step - loss: 0.3167 - acc: 0.8604\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 658us/step - loss: 0.3190 - acc: 0.8638\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 671us/step - loss: 0.3056 - acc: 0.8664\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 684us/step - loss: 0.2747 - acc: 0.8919\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 682us/step - loss: 0.2956 - acc: 0.8757\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 678us/step - loss: 0.2643 - acc: 0.8868\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 562us/step - loss: 0.2889 - acc: 0.8791\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 536us/step - loss: 0.2644 - acc: 0.8987\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 549us/step - loss: 0.2508 - acc: 0.9064\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 540us/step - loss: 0.2120 - acc: 0.9115\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 546us/step - loss: 0.2336 - acc: 0.9038\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 534us/step - loss: 0.2184 - acc: 0.9081\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 651us/step - loss: 0.5357 - acc: 0.7600\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 609us/step - loss: 0.5141 - acc: 0.7634\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 634us/step - loss: 0.5007 - acc: 0.7745\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 622us/step - loss: 0.4972 - acc: 0.7770\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 621us/step - loss: 0.4662 - acc: 0.8034\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 657us/step - loss: 0.4562 - acc: 0.8026\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 739us/step - loss: 0.4314 - acc: 0.8247\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 745us/step - loss: 0.4321 - acc: 0.8119\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 750us/step - loss: 0.4404 - acc: 0.8153\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 740us/step - loss: 0.4216 - acc: 0.8204\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 657us/step - loss: 0.4202 - acc: 0.8323\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 631us/step - loss: 0.3872 - acc: 0.8434\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 648us/step - loss: 0.4035 - acc: 0.8383\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 638us/step - loss: 0.3878 - acc: 0.8528\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 627us/step - loss: 0.3774 - acc: 0.8664\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 665us/step - loss: 0.3593 - acc: 0.8638\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 743us/step - loss: 0.3821 - acc: 0.8511\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 763us/step - loss: 0.3516 - acc: 0.8647\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 755us/step - loss: 0.3586 - acc: 0.8630\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 762us/step - loss: 0.3493 - acc: 0.8655\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 647us/step - loss: 0.3462 - acc: 0.8689\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 633us/step - loss: 0.3325 - acc: 0.8766\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 631us/step - loss: 0.3297 - acc: 0.8826\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 652us/step - loss: 0.3282 - acc: 0.8749\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 626us/step - loss: 0.3186 - acc: 0.8791\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 704us/step - loss: 0.5303 - acc: 0.7728\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 760us/step - loss: 0.5011 - acc: 0.7685\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 762us/step - loss: 0.4731 - acc: 0.7787\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 771us/step - loss: 0.4628 - acc: 0.7949\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 774us/step - loss: 0.4560 - acc: 0.8060\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 661us/step - loss: 0.4483 - acc: 0.8077\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 674us/step - loss: 0.4449 - acc: 0.8068\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 686us/step - loss: 0.4319 - acc: 0.8128\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 712us/step - loss: 0.4306 - acc: 0.8085\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 689us/step - loss: 0.3906 - acc: 0.8451\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 677us/step - loss: 0.3874 - acc: 0.8349\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 688us/step - loss: 0.3837 - acc: 0.8383\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 657us/step - loss: 0.3709 - acc: 0.8426\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 673us/step - loss: 0.3560 - acc: 0.8519\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 671us/step - loss: 0.3657 - acc: 0.8443\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 683us/step - loss: 0.3426 - acc: 0.8604\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 698us/step - loss: 0.3370 - acc: 0.8638\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 696us/step - loss: 0.3178 - acc: 0.8757\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 672us/step - loss: 0.2968 - acc: 0.8783\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 716us/step - loss: 0.3019 - acc: 0.8877\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 970us/step - loss: 0.3171 - acc: 0.8783\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 955us/step - loss: 0.2853 - acc: 0.8749\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 969us/step - loss: 0.2727 - acc: 0.8953\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 873us/step - loss: 0.2774 - acc: 0.8826\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 774us/step - loss: 0.2599 - acc: 0.8962\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 815us/step - loss: 0.5461 - acc: 0.7540\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 836us/step - loss: 0.4929 - acc: 0.7591\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 786us/step - loss: 0.4879 - acc: 0.7770\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 735us/step - loss: 0.4500 - acc: 0.7974\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 765us/step - loss: 0.4360 - acc: 0.8068\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 726us/step - loss: 0.4125 - acc: 0.8179\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 704us/step - loss: 0.4157 - acc: 0.8306\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 866us/step - loss: 0.4064 - acc: 0.8153\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 913us/step - loss: 0.4012 - acc: 0.8298\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 971us/step - loss: 0.4142 - acc: 0.8374\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 958us/step - loss: 0.3701 - acc: 0.8460\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 852us/step - loss: 0.3715 - acc: 0.8545\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 871us/step - loss: 0.3408 - acc: 0.8613\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 925us/step - loss: 0.3114 - acc: 0.8766\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 921us/step - loss: 0.3109 - acc: 0.8706\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 929us/step - loss: 0.3143 - acc: 0.8715\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 912us/step - loss: 0.3174 - acc: 0.8749\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 938us/step - loss: 0.3036 - acc: 0.8664\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 960us/step - loss: 0.2664 - acc: 0.8843\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 949us/step - loss: 0.2815 - acc: 0.8868\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 865us/step - loss: 0.2537 - acc: 0.8979\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 869us/step - loss: 0.2429 - acc: 0.9004\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 796us/step - loss: 0.2134 - acc: 0.9277\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 864us/step - loss: 0.2409 - acc: 0.8987\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 999us/step - loss: 0.2329 - acc: 0.9038\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5406 - acc: 0.7694\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5117 - acc: 0.7728\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4703 - acc: 0.7932\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4529 - acc: 0.7898\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4289 - acc: 0.8179\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4275 - acc: 0.8102\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4026 - acc: 0.8085\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3993 - acc: 0.8315\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.4145 - acc: 0.8281\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4143 - acc: 0.8179\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3820 - acc: 0.8323\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3707 - acc: 0.8519\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3559 - acc: 0.8502\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3419 - acc: 0.8604\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2923 - acc: 0.8894\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 973us/step - loss: 0.2982 - acc: 0.8715\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 967us/step - loss: 0.2988 - acc: 0.8689\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 978us/step - loss: 0.2946 - acc: 0.8749\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 962us/step - loss: 0.2904 - acc: 0.8826\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2666 - acc: 0.8877\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2430 - acc: 0.8979\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2779 - acc: 0.8826\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2585 - acc: 0.9047\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 990us/step - loss: 0.2668 - acc: 0.8911\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2449 - acc: 0.8996\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.5898 - acc: 0.7617\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.5162 - acc: 0.7643\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.4983 - acc: 0.7821\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.4550 - acc: 0.7872\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.4558 - acc: 0.8000\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.4341 - acc: 0.8119\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.4086 - acc: 0.8196\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.4047 - acc: 0.8204\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3960 - acc: 0.8511\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3893 - acc: 0.8374\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3535 - acc: 0.8519\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3588 - acc: 0.8494\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3449 - acc: 0.8587\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3217 - acc: 0.8723\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3139 - acc: 0.8647\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3055 - acc: 0.8774\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3265 - acc: 0.8638\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3090 - acc: 0.8706\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2918 - acc: 0.8723\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.2557 - acc: 0.8834\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.2770 - acc: 0.8911\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2515 - acc: 0.9064\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2419 - acc: 0.9013\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.2438 - acc: 0.8996\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.2424 - acc: 0.9030\n",
      "1175/1175 [==============================] - 1s 982us/step\n",
      "1175/1175 [==============================] - 1s 964us/step\n",
      "1175/1175 [==============================] - 2s 1ms/step\n",
      "1175/1175 [==============================] - 2s 1ms/step\n",
      "1175/1175 [==============================] - 2s 1ms/step\n",
      "1175/1175 [==============================] - 2s 1ms/step\n",
      "1175/1175 [==============================] - 2s 2ms/step\n",
      "Split no :2\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 3s 3ms/step - loss: 0.6986 - acc: 0.4749\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 0s 196us/step - loss: 0.6926 - acc: 0.5336\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 0s 205us/step - loss: 0.6926 - acc: 0.5174\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 0s 215us/step - loss: 0.6845 - acc: 0.5523\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 0s 224us/step - loss: 0.6863 - acc: 0.5353\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 0s 220us/step - loss: 0.6870 - acc: 0.5404\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 0s 228us/step - loss: 0.6826 - acc: 0.5379\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 0s 254us/step - loss: 0.6748 - acc: 0.5702\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 0s 267us/step - loss: 0.6660 - acc: 0.5847\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 0s 252us/step - loss: 0.6414 - acc: 0.6391\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 0s 255us/step - loss: 0.6324 - acc: 0.6443\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 0s 266us/step - loss: 0.6025 - acc: 0.6749\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 0s 257us/step - loss: 0.5684 - acc: 0.7004\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 0s 251us/step - loss: 0.5330 - acc: 0.7387\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 0s 242us/step - loss: 0.4977 - acc: 0.7719\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 0s 213us/step - loss: 0.4760 - acc: 0.7787\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 0s 236us/step - loss: 0.4538 - acc: 0.7779\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 0s 241us/step - loss: 0.4264 - acc: 0.8043\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 0s 247us/step - loss: 0.4206 - acc: 0.7915\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 0s 258us/step - loss: 0.3831 - acc: 0.8247\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 0s 257us/step - loss: 0.3766 - acc: 0.8366\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 0s 250us/step - loss: 0.3385 - acc: 0.8545\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 0s 257us/step - loss: 0.3234 - acc: 0.8570\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 0s 248us/step - loss: 0.3322 - acc: 0.8528\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 0s 217us/step - loss: 0.3014 - acc: 0.8783\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 3s 2ms/step - loss: 0.6939 - acc: 0.5289\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 578us/step - loss: 0.6936 - acc: 0.5166\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 542us/step - loss: 0.6867 - acc: 0.5515\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 565us/step - loss: 0.6914 - acc: 0.5379\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 570us/step - loss: 0.6851 - acc: 0.5438\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 568us/step - loss: 0.6793 - acc: 0.5438\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 560us/step - loss: 0.6770 - acc: 0.5591\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 572us/step - loss: 0.6640 - acc: 0.5523\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 555us/step - loss: 0.6488 - acc: 0.5830\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 565us/step - loss: 0.6227 - acc: 0.6247\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 578us/step - loss: 0.5936 - acc: 0.6749\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 572us/step - loss: 0.5659 - acc: 0.7183\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 598us/step - loss: 0.5309 - acc: 0.7387\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 561us/step - loss: 0.5135 - acc: 0.7455\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 570us/step - loss: 0.4966 - acc: 0.7540\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 552us/step - loss: 0.4791 - acc: 0.7872\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 559us/step - loss: 0.4466 - acc: 0.7983\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 587us/step - loss: 0.4565 - acc: 0.7932\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 572us/step - loss: 0.4262 - acc: 0.8170\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 567us/step - loss: 0.4195 - acc: 0.8060\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 557us/step - loss: 0.3996 - acc: 0.8255\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 575us/step - loss: 0.3762 - acc: 0.8238\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 584us/step - loss: 0.3763 - acc: 0.8289\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 583us/step - loss: 0.3468 - acc: 0.8570\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 571us/step - loss: 0.3428 - acc: 0.8562\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 4s 3ms/step - loss: 0.6929 - acc: 0.5162\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 691us/step - loss: 0.6922 - acc: 0.5345\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 707us/step - loss: 0.6910 - acc: 0.5362\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 702us/step - loss: 0.6909 - acc: 0.5379\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 702us/step - loss: 0.6906 - acc: 0.5396\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 702us/step - loss: 0.6900 - acc: 0.5413\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 710us/step - loss: 0.6898 - acc: 0.5413\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 723us/step - loss: 0.6884 - acc: 0.5362\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 693us/step - loss: 0.6856 - acc: 0.5421\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 723us/step - loss: 0.6785 - acc: 0.5600\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 732us/step - loss: 0.6538 - acc: 0.5923\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 735us/step - loss: 0.6206 - acc: 0.6494\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 711us/step - loss: 0.5789 - acc: 0.7013\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 716us/step - loss: 0.5744 - acc: 0.7115\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 734us/step - loss: 0.5788 - acc: 0.7532\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 728us/step - loss: 0.5629 - acc: 0.7430\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 714us/step - loss: 0.5479 - acc: 0.7566\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 735us/step - loss: 0.5118 - acc: 0.7813\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 751us/step - loss: 0.4827 - acc: 0.8034\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 716us/step - loss: 0.4554 - acc: 0.8153\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 721us/step - loss: 0.4563 - acc: 0.8213\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 726us/step - loss: 0.4262 - acc: 0.8289\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 728us/step - loss: 0.4290 - acc: 0.8162\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 729us/step - loss: 0.4102 - acc: 0.8238\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 735us/step - loss: 0.3769 - acc: 0.8426\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 4s 3ms/step - loss: 0.6917 - acc: 0.5349\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 713us/step - loss: 0.6921 - acc: 0.5362\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 745us/step - loss: 0.6910 - acc: 0.5370\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 742us/step - loss: 0.6893 - acc: 0.5404\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 751us/step - loss: 0.6904 - acc: 0.5404\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 736us/step - loss: 0.6886 - acc: 0.5370\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 733us/step - loss: 0.6776 - acc: 0.5370\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 774us/step - loss: 0.6266 - acc: 0.5345\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 746us/step - loss: 0.6065 - acc: 0.5694\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 726us/step - loss: 0.5846 - acc: 0.6698\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 755us/step - loss: 0.5651 - acc: 0.7481\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 757us/step - loss: 0.5420 - acc: 0.7736\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 734us/step - loss: 0.5417 - acc: 0.7745\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 751us/step - loss: 0.5046 - acc: 0.7574\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 759us/step - loss: 0.4960 - acc: 0.7626\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 744us/step - loss: 0.5197 - acc: 0.7787\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 740us/step - loss: 0.4902 - acc: 0.7838\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 735us/step - loss: 0.4634 - acc: 0.7855\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 740us/step - loss: 0.4538 - acc: 0.8111\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 737us/step - loss: 0.4416 - acc: 0.8145\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 739us/step - loss: 0.4168 - acc: 0.8264\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 775us/step - loss: 0.4018 - acc: 0.8289\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 780us/step - loss: 0.3919 - acc: 0.8340\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 750us/step - loss: 0.3918 - acc: 0.8349\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 771us/step - loss: 0.3814 - acc: 0.8477\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 4s 3ms/step - loss: 0.6924 - acc: 0.5170\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 823us/step - loss: 0.6906 - acc: 0.5430\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 821us/step - loss: 0.6920 - acc: 0.5311\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 841us/step - loss: 0.6914 - acc: 0.5404\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 826us/step - loss: 0.6919 - acc: 0.5311\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 795us/step - loss: 0.6910 - acc: 0.5294\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 795us/step - loss: 0.6898 - acc: 0.5404\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 805us/step - loss: 0.6902 - acc: 0.5421\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 821us/step - loss: 0.6907 - acc: 0.5396\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 843us/step - loss: 0.6900 - acc: 0.5404\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 836us/step - loss: 0.6847 - acc: 0.5396\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 838us/step - loss: 0.6442 - acc: 0.5736\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 799us/step - loss: 0.6088 - acc: 0.6255\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 823us/step - loss: 0.5584 - acc: 0.6953\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 831us/step - loss: 0.5344 - acc: 0.7311\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 812us/step - loss: 0.5229 - acc: 0.7506\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 802us/step - loss: 0.5234 - acc: 0.7847\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 815us/step - loss: 0.4895 - acc: 0.7830\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 828us/step - loss: 0.4735 - acc: 0.7813\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 822us/step - loss: 0.4465 - acc: 0.7957\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 806us/step - loss: 0.4524 - acc: 0.8043\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 827us/step - loss: 0.4217 - acc: 0.8213\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 807us/step - loss: 0.4057 - acc: 0.8315\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 803us/step - loss: 0.3868 - acc: 0.8119\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 845us/step - loss: 0.3757 - acc: 0.8477\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 4s 3ms/step - loss: 0.6927 - acc: 0.5272\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6910 - acc: 0.5421\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6894 - acc: 0.5464\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6915 - acc: 0.5336\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6908 - acc: 0.5370\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6921 - acc: 0.5421\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6901 - acc: 0.5353\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6879 - acc: 0.5370\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6741 - acc: 0.5319\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6315 - acc: 0.5728\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6151 - acc: 0.6809\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.6310 - acc: 0.6298\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 990us/step - loss: 0.5686 - acc: 0.7123\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5504 - acc: 0.7583\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5078 - acc: 0.7677\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4854 - acc: 0.7915\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4824 - acc: 0.7949\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4836 - acc: 0.7872\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4494 - acc: 0.8068\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4182 - acc: 0.8255\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4235 - acc: 0.8179\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3935 - acc: 0.8289\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3991 - acc: 0.8340\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3671 - acc: 0.8460\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3443 - acc: 0.8613\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 5s 4ms/step - loss: 0.6933 - acc: 0.5081\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6920 - acc: 0.5464\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6905 - acc: 0.5353\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6909 - acc: 0.5396\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6918 - acc: 0.5243\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6887 - acc: 0.5413\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6907 - acc: 0.5387\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6856 - acc: 0.5277\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6655 - acc: 0.5549\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6119 - acc: 0.6860\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.6154 - acc: 0.6936\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.5545 - acc: 0.7532\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.5236 - acc: 0.7660\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4899 - acc: 0.7898\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4690 - acc: 0.8085\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4594 - acc: 0.8060\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4326 - acc: 0.8332\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4108 - acc: 0.8238\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3850 - acc: 0.8434\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3862 - acc: 0.8502\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3804 - acc: 0.8443\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3371 - acc: 0.8630\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3336 - acc: 0.8587\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3241 - acc: 0.8664\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2984 - acc: 0.8843\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 0s 264us/step - loss: 0.5941 - acc: 0.7464\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 0s 262us/step - loss: 0.5319 - acc: 0.7600\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 0s 265us/step - loss: 0.4907 - acc: 0.7745\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 0s 269us/step - loss: 0.4550 - acc: 0.7932\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 0s 261us/step - loss: 0.4278 - acc: 0.8094\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 0s 268us/step - loss: 0.4088 - acc: 0.8213\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 0s 268us/step - loss: 0.4048 - acc: 0.8119\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 0s 262us/step - loss: 0.3995 - acc: 0.8153\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 0s 261us/step - loss: 0.3833 - acc: 0.8332\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 0s 268us/step - loss: 0.3512 - acc: 0.8613\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 0s 240us/step - loss: 0.3506 - acc: 0.8536\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 0s 236us/step - loss: 0.3330 - acc: 0.8536\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 0s 239us/step - loss: 0.3412 - acc: 0.8562\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 0s 247us/step - loss: 0.3188 - acc: 0.8783\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 0s 236us/step - loss: 0.3084 - acc: 0.8800\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 0s 238us/step - loss: 0.2873 - acc: 0.8800\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 0s 243us/step - loss: 0.2771 - acc: 0.8902\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 0s 248us/step - loss: 0.2572 - acc: 0.8894\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 0s 232us/step - loss: 0.2449 - acc: 0.9055\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 0s 243us/step - loss: 0.2546 - acc: 0.8953\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 0s 239us/step - loss: 0.2310 - acc: 0.9149\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 0s 253us/step - loss: 0.2209 - acc: 0.9072\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 0s 246us/step - loss: 0.2233 - acc: 0.9166\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 0s 249us/step - loss: 0.2423 - acc: 0.9081\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 0s 262us/step - loss: 0.2040 - acc: 0.9191\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 618us/step - loss: 0.5631 - acc: 0.7574\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 606us/step - loss: 0.5145 - acc: 0.7566\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 615us/step - loss: 0.4917 - acc: 0.7838\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 609us/step - loss: 0.4734 - acc: 0.7847\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 608us/step - loss: 0.4461 - acc: 0.8085\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 577us/step - loss: 0.4399 - acc: 0.8187\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 569us/step - loss: 0.4238 - acc: 0.8136\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 570us/step - loss: 0.3962 - acc: 0.8204\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 585us/step - loss: 0.3859 - acc: 0.8289\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 573us/step - loss: 0.3933 - acc: 0.8323\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 562us/step - loss: 0.3661 - acc: 0.8477\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 589us/step - loss: 0.3615 - acc: 0.8494\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 585us/step - loss: 0.3450 - acc: 0.8511\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 603us/step - loss: 0.3423 - acc: 0.8502\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 592us/step - loss: 0.3396 - acc: 0.8553\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 591us/step - loss: 0.3168 - acc: 0.8757\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 608us/step - loss: 0.3245 - acc: 0.8774\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 580us/step - loss: 0.3069 - acc: 0.8723\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 578us/step - loss: 0.2909 - acc: 0.8740\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 563us/step - loss: 0.2937 - acc: 0.8851\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 581us/step - loss: 0.2997 - acc: 0.8826\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 571us/step - loss: 0.2657 - acc: 0.8928\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 607us/step - loss: 0.2603 - acc: 0.8953\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 568us/step - loss: 0.2520 - acc: 0.8987\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 581us/step - loss: 0.2149 - acc: 0.9191\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 746us/step - loss: 0.5703 - acc: 0.7557\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 753us/step - loss: 0.5037 - acc: 0.7651\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 753us/step - loss: 0.4894 - acc: 0.7906\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 765us/step - loss: 0.4729 - acc: 0.7957\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 778us/step - loss: 0.4955 - acc: 0.7957\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 772us/step - loss: 0.4487 - acc: 0.8077\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 769us/step - loss: 0.4337 - acc: 0.8085\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 771us/step - loss: 0.4361 - acc: 0.8213\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 776us/step - loss: 0.4141 - acc: 0.8315\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 776us/step - loss: 0.4197 - acc: 0.8255\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 760us/step - loss: 0.4115 - acc: 0.8264\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 763us/step - loss: 0.3971 - acc: 0.8357\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 743us/step - loss: 0.3811 - acc: 0.8451\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 772us/step - loss: 0.3728 - acc: 0.8621\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 745us/step - loss: 0.3441 - acc: 0.8715\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 782us/step - loss: 0.3411 - acc: 0.8698\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 988us/step - loss: 0.3346 - acc: 0.8613\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 990us/step - loss: 0.3417 - acc: 0.8723\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3431 - acc: 0.8689\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 880us/step - loss: 0.3475 - acc: 0.8706\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 789us/step - loss: 0.3290 - acc: 0.8826\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 797us/step - loss: 0.3145 - acc: 0.8877\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 787us/step - loss: 0.3129 - acc: 0.8809\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 771us/step - loss: 0.2862 - acc: 0.8919\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 698us/step - loss: 0.2707 - acc: 0.9030\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 736us/step - loss: 0.5911 - acc: 0.7311\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 757us/step - loss: 0.5294 - acc: 0.7549\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 750us/step - loss: 0.4981 - acc: 0.7847\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 782us/step - loss: 0.4792 - acc: 0.7804\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 773us/step - loss: 0.4829 - acc: 0.7804\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 756us/step - loss: 0.4499 - acc: 0.7966\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 762us/step - loss: 0.4492 - acc: 0.8136\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 800us/step - loss: 0.4326 - acc: 0.8085\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 745us/step - loss: 0.4136 - acc: 0.8247\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 770us/step - loss: 0.4156 - acc: 0.8255\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 769us/step - loss: 0.4105 - acc: 0.8162\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 762us/step - loss: 0.3752 - acc: 0.8349\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 965us/step - loss: 0.3735 - acc: 0.8477\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3849 - acc: 0.8443\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3834 - acc: 0.8502\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 965us/step - loss: 0.3694 - acc: 0.8502\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 853us/step - loss: 0.3324 - acc: 0.8630\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 843us/step - loss: 0.3200 - acc: 0.8757\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 833us/step - loss: 0.3152 - acc: 0.8689\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 774us/step - loss: 0.3235 - acc: 0.8715\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 758us/step - loss: 0.3044 - acc: 0.8809\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 771us/step - loss: 0.2669 - acc: 0.9021\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 765us/step - loss: 0.2723 - acc: 0.8928\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 758us/step - loss: 0.2627 - acc: 0.9021\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 773us/step - loss: 0.2904 - acc: 0.8817\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 845us/step - loss: 0.5946 - acc: 0.7302\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 843us/step - loss: 0.5518 - acc: 0.7464\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 856us/step - loss: 0.5109 - acc: 0.7447\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 841us/step - loss: 0.4855 - acc: 0.7821\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 853us/step - loss: 0.4682 - acc: 0.7898\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 885us/step - loss: 0.4418 - acc: 0.8009\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 847us/step - loss: 0.4388 - acc: 0.8060\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 855us/step - loss: 0.4125 - acc: 0.8213\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 845us/step - loss: 0.4034 - acc: 0.8111\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 884us/step - loss: 0.3972 - acc: 0.8213\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 858us/step - loss: 0.3913 - acc: 0.8289\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 860us/step - loss: 0.3615 - acc: 0.8409\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 872us/step - loss: 0.3507 - acc: 0.8502\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 867us/step - loss: 0.3592 - acc: 0.8391\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 859us/step - loss: 0.3411 - acc: 0.8545\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 879us/step - loss: 0.3370 - acc: 0.8570\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 868us/step - loss: 0.3220 - acc: 0.8494\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 880us/step - loss: 0.2956 - acc: 0.8774\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 888us/step - loss: 0.3275 - acc: 0.8672\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 862us/step - loss: 0.2940 - acc: 0.8817\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 899us/step - loss: 0.2762 - acc: 0.8894\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 880us/step - loss: 0.2792 - acc: 0.8740\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 877us/step - loss: 0.2745 - acc: 0.8843\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 854us/step - loss: 0.2824 - acc: 0.8851\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 879us/step - loss: 0.2714 - acc: 0.8962\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5281 - acc: 0.7685\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.5400 - acc: 0.7515\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4991 - acc: 0.7694\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4689 - acc: 0.7915\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4676 - acc: 0.7855\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4364 - acc: 0.8145\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3992 - acc: 0.8255\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4190 - acc: 0.8145\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.4023 - acc: 0.8196\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3874 - acc: 0.8281\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3889 - acc: 0.8298\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3770 - acc: 0.8400\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3472 - acc: 0.8502\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3583 - acc: 0.8528\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3554 - acc: 0.8460\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3449 - acc: 0.8443\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3006 - acc: 0.8834\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2994 - acc: 0.8774\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.3136 - acc: 0.8655\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2757 - acc: 0.8919\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2882 - acc: 0.8791\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2609 - acc: 0.9004\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2546 - acc: 0.8945\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2536 - acc: 0.8987\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 1s 1ms/step - loss: 0.2402 - acc: 0.9021\n",
      "Epoch 1/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.5883 - acc: 0.7404\n",
      "Epoch 2/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.5596 - acc: 0.7634\n",
      "Epoch 3/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4928 - acc: 0.7677\n",
      "Epoch 4/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4796 - acc: 0.7830\n",
      "Epoch 5/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4652 - acc: 0.8017\n",
      "Epoch 6/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4340 - acc: 0.8264\n",
      "Epoch 7/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4274 - acc: 0.8221\n",
      "Epoch 8/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4496 - acc: 0.8179\n",
      "Epoch 9/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4183 - acc: 0.8366\n",
      "Epoch 10/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.4073 - acc: 0.8315\n",
      "Epoch 11/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3674 - acc: 0.8536\n",
      "Epoch 12/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3844 - acc: 0.8519\n",
      "Epoch 13/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3532 - acc: 0.8519\n",
      "Epoch 14/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3659 - acc: 0.8511\n",
      "Epoch 15/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3469 - acc: 0.8434\n",
      "Epoch 16/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3134 - acc: 0.8604\n",
      "Epoch 17/25\n",
      "1175/1175 [==============================] - 2s 1ms/step - loss: 0.3169 - acc: 0.8740\n",
      "Epoch 18/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2933 - acc: 0.8774\n",
      "Epoch 19/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2947 - acc: 0.8987\n",
      "Epoch 20/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.3056 - acc: 0.8826\n",
      "Epoch 21/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2915 - acc: 0.8834\n",
      "Epoch 22/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2658 - acc: 0.8894\n",
      "Epoch 23/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2563 - acc: 0.8970\n",
      "Epoch 24/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2556 - acc: 0.8894\n",
      "Epoch 25/25\n",
      "1175/1175 [==============================] - 2s 2ms/step - loss: 0.2475 - acc: 0.9021\n",
      "1175/1175 [==============================] - 2s 1ms/step\n",
      "1175/1175 [==============================] - 2s 1ms/step\n",
      "1175/1175 [==============================] - 2s 2ms/step\n",
      "1175/1175 [==============================] - 2s 2ms/step\n",
      "1175/1175 [==============================] - 2s 2ms/step\n",
      "1175/1175 [==============================] - 2s 2ms/step\n",
      "1175/1175 [==============================] - 2s 2ms/step\n",
      "Accruracy neural network0 : 78.83687943769685% +- 0.6820320582859697%\n",
      "Accruracy neural network1 : 79.68794326748409% +- 0.5215539235485964%\n",
      "Accruracy neural network2 : 79.34751771527824% +- 0.39513158233146667%\n",
      "Accruracy neural network3 : 79.60283686421442% +- 0.743024147193032%\n",
      "Accruracy neural network4 : 79.82978722558799% +- 0.9033579110405514%\n",
      "Accruracy neural network5 : 78.97872340256441% +- 1.0423360524750565%\n",
      "Accruracy neural network6 : 78.80851062984333% +- 1.949413045917284%\n",
      "--- 17.429425259431202 minutes elapsed---\n"
     ]
    }
   ],
   "source": [
    "NNs,score,mean,std = multi_cross_val(functions,x_vec_train,y_vec_train,3,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0kNYeBLUkMY+JLUEP8fbhxSEKk6tOoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4d9ded4358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for i in range(len(mean)):\n",
    "    mean_tmp = list(mean.values())\n",
    "    std_tmp = list(std.values())\n",
    "    plt.errorbar(mean_tmp[i],i,xerr=std_tmp[i], linestyle='--', marker='*')\n",
    "    \n",
    "plt.xlabel(\"Accuracy\")\n",
    "plt.ylabel(\"Model number\")\n",
    "plt.savefig('documents/Cross_val_graph.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = NN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441/441 [==============================] - 0s 293us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5740751975517965, 0.8616780042648315]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.evaluate(x_vec_test,y_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882/882 [==============================] - 0s 162us/step\n",
      "[0.4740270571508646, 0.7981859411782418]\n",
      "882/882 [==============================] - 0s 220us/step\n",
      "[0.43143712230820774, 0.8004535150095449]\n",
      "882/882 [==============================] - 0s 374us/step\n",
      "[0.4472087235137178, 0.80272108870569]\n",
      "882/882 [==============================] - 0s 436us/step\n",
      "[0.47223375092287995, 0.7857142859846016]\n"
     ]
    }
   ],
   "source": [
    "for nn in NNs :\n",
    "    print(nn.evaluate(x_vec_test,y_vec_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model,name,path):\n",
    "    model_json = model.to_json()\n",
    "    with open(path+name+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "        model.save_weights(path+name+\".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "def load_model(name,path):\n",
    "    from keras.models import model_from_json\n",
    "    # load json and create model\n",
    "    json_file = open(path+name+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(path+name+\".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "save_model(NN,\"Final\",\"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "NN_loaded = load_model(\"labelled+books_final\",\"models/\")\n",
    "NN_loaded.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_39_input to have 3 dimensions, but got array with shape (882, 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-75c13fdc1fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNN_loaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vec_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_vec_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    124\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected dense_39_input to have 3 dimensions, but got array with shape (882, 50)"
     ]
    }
   ],
   "source": [
    "NN_loaded.evaluate(x_vec_test,y_vec_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation of the model for live prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote(score):\n",
    "    import numpy as np\n",
    "    neg=[]\n",
    "    pos=[]\n",
    "    for s in score :\n",
    "        neg.append(s[0][0])\n",
    "        pos.append(s[0][1])\n",
    "    sup_neg = [n for n in neg if n > 0.5]\n",
    "    sup_pos = [p for p in pos if p > 0.5]\n",
    "\n",
    "    return [np.mean(neg),np.mean(pos)]\n",
    "    \n",
    "def predict(sentence,word2Vec_model,model,max_len=50,word_length=200):\n",
    "    import numpy as np\n",
    "    sentence = sentence.split(\" \")\n",
    "    good_sentence = []\n",
    "    unusued_words = []\n",
    "    for i in range(len(sentence)):\n",
    "        try :\n",
    "            sentence[i]=sentence[i].lower()\n",
    "            good_sentence.append(word2Vec_model.wv[sentence[i]])\n",
    "        except : \n",
    "            unusued_words.append(sentence[i])\n",
    "    sentence_length = len(good_sentence)\n",
    "    \n",
    "    if(sentence_length<max_len):    \n",
    "        for j in range(max_len-sentence_length):\n",
    "            good_sentence.append([0]*word_length)     \n",
    "            \n",
    "    if (sentence_length > max_len) :\n",
    "        predictions = []\n",
    "        ind = 0\n",
    "        for i in range(int(np.ceil(sentence_length/max_len))):\n",
    "            sent_tmp = good_sentence[ind:ind+max_len]\n",
    "            if(len(sent_tmp)<max_len):\n",
    "                for j in range(max_len-len(sent_tmp)):\n",
    "                    sent_tmp.append([0]*word_length)    \n",
    "            sent_tmp = np.array(sent_tmp)        \n",
    "            sent_tmp = np.reshape(sent_tmp, (1,sent_tmp.shape[0], sent_tmp.shape[1]))\n",
    "            try :\n",
    "                predictions.append(model.predict(sent_tmp))\n",
    "                ind+=max_len                    \n",
    "            except : \n",
    "                print(\"Erreur au découpage no :\"+str(i))\n",
    "        return vote(predictions)\n",
    "    \n",
    "    good_sentence = np.array(good_sentence)        \n",
    "    good_sentence = np.reshape(good_sentence, (1,good_sentence.shape[0], good_sentence.shape[1]))\n",
    "    return np.squeeze(model.predict(good_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Négatif        Positif\n",
      "[0.57874113 0.42125887]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "best=NN_loaded\n",
    "sentence =input()\n",
    "print(\" Négatif        Positif\")\n",
    "print(predict(sentence,model,best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data opening for test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(word2Vec_model,model):\n",
    "    # This function take an already trained model return the accuracy of the model over an external dataset \n",
    "    y=[]\n",
    "    y_pred=[]\n",
    "    cnt = 1\n",
    "\n",
    "    with open(\"datasets/polarization/rt-polarity-neg.txt\",\"rt\",encoding = \"ISO-8859-1\") as fp:\n",
    "            line = fp.readline()\n",
    "            while line:\n",
    "                y.append(0)\n",
    "                y_pred.append(predict(line,word2Vec_model,model))\n",
    "                line = fp.readline()\n",
    "                cnt += 1\n",
    "                \n",
    "    with open(\"datasets/polarization/rt-polarity-pos.txt\",\"rt\",encoding = \"ISO-8859-1\") as fp:\n",
    "            line = fp.readline()\n",
    "            while line:\n",
    "                y.append(1)\n",
    "                y_pred.append(predict(line,word2Vec_model,model))\n",
    "                line = fp.readline()\n",
    "                cnt += 1\n",
    "                \n",
    "    print(str(cnt)+\" lines tested\")\n",
    "    \n",
    "    y_pred_trans=[]  \n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        try : \n",
    "            if(y_pred[i][0]>y_pred[i][1]):\n",
    "                y_pred_trans.append(0)\n",
    "            else :\n",
    "                y_pred_trans.append(1)\n",
    "        except : \n",
    "            print(\"Bug ligne : \"+str(i))\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cf = confusion_matrix(y,y_pred_trans)\n",
    "    acc = (cf[0][0]+cf[1][1])/(cf[0][0]+cf[1][1]+cf[0][1]+cf[1][0])\n",
    "    print(\"Confusion matrix : \\n \"+ str (cf))\n",
    "    print(\"Accuracy : \"+ str(acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10663 lines tested\n",
      "Confusion matrix : \n",
      " [[3643 1688]\n",
      " [1153 4178]]\n",
      "Accuracy : 0.7335396736072032\n"
     ]
    }
   ],
   "source": [
    "test(model,best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4410/4410 [==============================] - 4s 857us/step - loss: 0.6928 - acc: 0.5204\n",
      "Epoch 2/30\n",
      "4410/4410 [==============================] - 1s 210us/step - loss: 0.6608 - acc: 0.6009\n",
      "Epoch 3/30\n",
      "4410/4410 [==============================] - 1s 205us/step - loss: 0.5886 - acc: 0.6902\n",
      "Epoch 4/30\n",
      "4410/4410 [==============================] - 1s 213us/step - loss: 0.5147 - acc: 0.7628\n",
      "Epoch 5/30\n",
      "4410/4410 [==============================] - 1s 198us/step - loss: 0.4738 - acc: 0.7785\n",
      "Epoch 6/30\n",
      "4410/4410 [==============================] - 1s 195us/step - loss: 0.4495 - acc: 0.7950\n",
      "Epoch 7/30\n",
      "4410/4410 [==============================] - 1s 195us/step - loss: 0.4296 - acc: 0.8000\n",
      "Epoch 8/30\n",
      "4410/4410 [==============================] - 1s 197us/step - loss: 0.4074 - acc: 0.8209\n",
      "Epoch 9/30\n",
      "4410/4410 [==============================] - 1s 197us/step - loss: 0.3888 - acc: 0.8340\n",
      "Epoch 10/30\n",
      "4410/4410 [==============================] - 1s 209us/step - loss: 0.3882 - acc: 0.8317\n",
      "Epoch 11/30\n",
      "4410/4410 [==============================] - 1s 204us/step - loss: 0.3605 - acc: 0.8429\n",
      "Epoch 12/30\n",
      "4410/4410 [==============================] - 1s 207us/step - loss: 0.3547 - acc: 0.8516\n",
      "Epoch 13/30\n",
      "4410/4410 [==============================] - 1s 195us/step - loss: 0.3350 - acc: 0.8524\n",
      "Epoch 14/30\n",
      "4410/4410 [==============================] - 1s 191us/step - loss: 0.3429 - acc: 0.8522\n",
      "Epoch 15/30\n",
      "4410/4410 [==============================] - 1s 220us/step - loss: 0.3211 - acc: 0.8587\n",
      "Epoch 16/30\n",
      "4410/4410 [==============================] - 1s 226us/step - loss: 0.3124 - acc: 0.8721\n",
      "Epoch 17/30\n",
      "4410/4410 [==============================] - 1s 233us/step - loss: 0.3049 - acc: 0.8705\n",
      "Epoch 18/30\n",
      "4410/4410 [==============================] - 1s 212us/step - loss: 0.2951 - acc: 0.8753\n",
      "Epoch 19/30\n",
      "4410/4410 [==============================] - 1s 204us/step - loss: 0.2906 - acc: 0.8805\n",
      "Epoch 20/30\n",
      "4410/4410 [==============================] - 1s 239us/step - loss: 0.2761 - acc: 0.8814\n",
      "Epoch 21/30\n",
      "4410/4410 [==============================] - 1s 207us/step - loss: 0.2724 - acc: 0.8878\n",
      "Epoch 22/30\n",
      "4410/4410 [==============================] - 1s 217us/step - loss: 0.2698 - acc: 0.8873\n",
      "Epoch 23/30\n",
      "4410/4410 [==============================] - 1s 208us/step - loss: 0.2617 - acc: 0.8891\n",
      "Epoch 24/30\n",
      "4410/4410 [==============================] - 1s 205us/step - loss: 0.2644 - acc: 0.8900\n",
      "Epoch 25/30\n",
      "4410/4410 [==============================] - 1s 210us/step - loss: 0.2577 - acc: 0.8959\n",
      "Epoch 26/30\n",
      "4410/4410 [==============================] - 1s 210us/step - loss: 0.2538 - acc: 0.8893\n",
      "Epoch 27/30\n",
      "4410/4410 [==============================] - 1s 212us/step - loss: 0.2370 - acc: 0.9005\n",
      "Epoch 28/30\n",
      "4410/4410 [==============================] - 1s 208us/step - loss: 0.2207 - acc: 0.9063\n",
      "Epoch 29/30\n",
      "4410/4410 [==============================] - 1s 216us/step - loss: 0.2181 - acc: 0.9086\n",
      "Epoch 30/30\n",
      "4410/4410 [==============================] - 1s 213us/step - loss: 0.2460 - acc: 0.9036\n",
      "Neural network 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10663 lines tested\n",
      "Confusion matrix : \n",
      " [[3427 1904]\n",
      " [1437 3894]]\n",
      "Accuracy : 0.6866441568186081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, kernel_size=10, activation=\"relu\", input_shape=(50, 300), padding=\"causal\")`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.6986 - acc: 0.5010\n",
      "Epoch 2/30\n",
      "4410/4410 [==============================] - 2s 506us/step - loss: 0.6732 - acc: 0.5685\n",
      "Epoch 3/30\n",
      "4410/4410 [==============================] - 2s 523us/step - loss: 0.5872 - acc: 0.6884\n",
      "Epoch 4/30\n",
      "4410/4410 [==============================] - 2s 528us/step - loss: 0.5293 - acc: 0.7476\n",
      "Epoch 5/30\n",
      "4410/4410 [==============================] - 2s 564us/step - loss: 0.4928 - acc: 0.7803\n",
      "Epoch 6/30\n",
      "4410/4410 [==============================] - 2s 557us/step - loss: 0.4610 - acc: 0.7866\n",
      "Epoch 7/30\n",
      "4410/4410 [==============================] - 2s 533us/step - loss: 0.4613 - acc: 0.7878\n",
      "Epoch 8/30\n",
      "4410/4410 [==============================] - 2s 531us/step - loss: 0.4293 - acc: 0.8066\n",
      "Epoch 9/30\n",
      "4410/4410 [==============================] - 2s 521us/step - loss: 0.4037 - acc: 0.8209\n",
      "Epoch 10/30\n",
      "4410/4410 [==============================] - 2s 517us/step - loss: 0.4018 - acc: 0.8197\n",
      "Epoch 11/30\n",
      "4410/4410 [==============================] - 2s 526us/step - loss: 0.3865 - acc: 0.8281\n",
      "Epoch 12/30\n",
      "4410/4410 [==============================] - 2s 519us/step - loss: 0.3742 - acc: 0.8408\n",
      "Epoch 13/30\n",
      "4410/4410 [==============================] - 2s 523us/step - loss: 0.3734 - acc: 0.8329\n",
      "Epoch 14/30\n",
      "4410/4410 [==============================] - 2s 524us/step - loss: 0.3494 - acc: 0.8472\n",
      "Epoch 15/30\n",
      "4410/4410 [==============================] - 2s 529us/step - loss: 0.3617 - acc: 0.8451\n",
      "Epoch 16/30\n",
      "4410/4410 [==============================] - 2s 535us/step - loss: 0.3296 - acc: 0.8551\n",
      "Epoch 17/30\n",
      "4410/4410 [==============================] - 2s 544us/step - loss: 0.3301 - acc: 0.8549\n",
      "Epoch 18/30\n",
      "4410/4410 [==============================] - 2s 533us/step - loss: 0.3199 - acc: 0.8612\n",
      "Epoch 19/30\n",
      "4410/4410 [==============================] - 2s 538us/step - loss: 0.3125 - acc: 0.8676\n",
      "Epoch 20/30\n",
      "4410/4410 [==============================] - 2s 518us/step - loss: 0.3046 - acc: 0.8678\n",
      "Epoch 21/30\n",
      "4410/4410 [==============================] - 2s 515us/step - loss: 0.3026 - acc: 0.8703\n",
      "Epoch 22/30\n",
      "4410/4410 [==============================] - 2s 536us/step - loss: 0.3026 - acc: 0.8744\n",
      "Epoch 23/30\n",
      "4410/4410 [==============================] - 2s 531us/step - loss: 0.2897 - acc: 0.8789\n",
      "Epoch 24/30\n",
      "4410/4410 [==============================] - 2s 531us/step - loss: 0.2662 - acc: 0.8866\n",
      "Epoch 25/30\n",
      "4410/4410 [==============================] - 2s 558us/step - loss: 0.2727 - acc: 0.8821\n",
      "Epoch 26/30\n",
      "4410/4410 [==============================] - 2s 538us/step - loss: 0.2725 - acc: 0.8882\n",
      "Epoch 27/30\n",
      "4410/4410 [==============================] - 2s 557us/step - loss: 0.2586 - acc: 0.8900\n",
      "Epoch 28/30\n",
      "4410/4410 [==============================] - 3s 596us/step - loss: 0.2588 - acc: 0.8934\n",
      "Epoch 29/30\n",
      "4410/4410 [==============================] - 3s 590us/step - loss: 0.2583 - acc: 0.8909\n",
      "Epoch 30/30\n",
      "4410/4410 [==============================] - 2s 530us/step - loss: 0.2587 - acc: 0.8884\n",
      "Neural network 1\n",
      "10663 lines tested\n",
      "Confusion matrix : \n",
      " [[3916 1415]\n",
      " [1705 3626]]\n",
      "Accuracy : 0.7073719752391672\n",
      "Epoch 1/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.6933 - acc: 0.5069\n",
      "Epoch 2/30\n",
      "4410/4410 [==============================] - 3s 763us/step - loss: 0.6926 - acc: 0.5152\n",
      "Epoch 3/30\n",
      "4410/4410 [==============================] - 3s 765us/step - loss: 0.6598 - acc: 0.5483\n",
      "Epoch 4/30\n",
      "4410/4410 [==============================] - 3s 773us/step - loss: 0.5841 - acc: 0.7358\n",
      "Epoch 5/30\n",
      "4410/4410 [==============================] - 3s 783us/step - loss: 0.5387 - acc: 0.7687\n",
      "Epoch 6/30\n",
      "4410/4410 [==============================] - 4s 796us/step - loss: 0.5048 - acc: 0.7723\n",
      "Epoch 7/30\n",
      "4410/4410 [==============================] - 4s 801us/step - loss: 0.4867 - acc: 0.7871\n",
      "Epoch 8/30\n",
      "4410/4410 [==============================] - 4s 796us/step - loss: 0.4570 - acc: 0.7998\n",
      "Epoch 9/30\n",
      "4410/4410 [==============================] - 4s 799us/step - loss: 0.4303 - acc: 0.8075\n",
      "Epoch 10/30\n",
      "4410/4410 [==============================] - 4s 825us/step - loss: 0.4248 - acc: 0.8084\n",
      "Epoch 11/30\n",
      "4410/4410 [==============================] - 4s 819us/step - loss: 0.4088 - acc: 0.8247\n",
      "Epoch 12/30\n",
      "4410/4410 [==============================] - 4s 843us/step - loss: 0.4049 - acc: 0.8159\n",
      "Epoch 13/30\n",
      "4410/4410 [==============================] - 4s 810us/step - loss: 0.4044 - acc: 0.8227\n",
      "Epoch 14/30\n",
      "4410/4410 [==============================] - 4s 814us/step - loss: 0.3754 - acc: 0.8349\n",
      "Epoch 15/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3744 - acc: 0.8435\n",
      "Epoch 16/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.3704 - acc: 0.8438\n",
      "Epoch 17/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3566 - acc: 0.8444\n",
      "Epoch 18/30\n",
      "4410/4410 [==============================] - 4s 865us/step - loss: 0.3521 - acc: 0.8506\n",
      "Epoch 19/30\n",
      "4410/4410 [==============================] - 3s 760us/step - loss: 0.3397 - acc: 0.8469\n",
      "Epoch 20/30\n",
      "4410/4410 [==============================] - 3s 737us/step - loss: 0.3533 - acc: 0.8508\n",
      "Epoch 21/30\n",
      "4410/4410 [==============================] - 3s 779us/step - loss: 0.3453 - acc: 0.8515\n",
      "Epoch 22/30\n",
      "4410/4410 [==============================] - 3s 749us/step - loss: 0.3317 - acc: 0.8567\n",
      "Epoch 23/30\n",
      "4410/4410 [==============================] - 3s 791us/step - loss: 0.3187 - acc: 0.8737\n",
      "Epoch 24/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3182 - acc: 0.8633\n",
      "Epoch 25/30\n",
      "4410/4410 [==============================] - 4s 993us/step - loss: 0.3136 - acc: 0.8649\n",
      "Epoch 26/30\n",
      "4410/4410 [==============================] - 4s 796us/step - loss: 0.3068 - acc: 0.8726\n",
      "Epoch 27/30\n",
      "4410/4410 [==============================] - 4s 794us/step - loss: 0.3073 - acc: 0.8685\n",
      "Epoch 28/30\n",
      "4410/4410 [==============================] - 4s 795us/step - loss: 0.3186 - acc: 0.8635\n",
      "Epoch 29/30\n",
      "4410/4410 [==============================] - 4s 796us/step - loss: 0.3081 - acc: 0.8649\n",
      "Epoch 30/30\n",
      "4410/4410 [==============================] - 4s 805us/step - loss: 0.3010 - acc: 0.8741\n",
      "Neural network 2\n",
      "10663 lines tested\n",
      "Confusion matrix : \n",
      " [[3120 2211]\n",
      " [ 894 4437]]\n",
      "Accuracy : 0.7087788407428249\n",
      "Epoch 1/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.6923 - acc: 0.5076\n",
      "Epoch 2/30\n",
      "4410/4410 [==============================] - 4s 806us/step - loss: 0.6747 - acc: 0.5422\n",
      "Epoch 3/30\n",
      "4410/4410 [==============================] - 4s 812us/step - loss: 0.5927 - acc: 0.7200\n",
      "Epoch 4/30\n",
      "4410/4410 [==============================] - 4s 828us/step - loss: 0.5463 - acc: 0.7481\n",
      "Epoch 5/30\n",
      "4410/4410 [==============================] - 4s 825us/step - loss: 0.5147 - acc: 0.7644\n",
      "Epoch 6/30\n",
      "4410/4410 [==============================] - 4s 829us/step - loss: 0.4879 - acc: 0.7844\n",
      "Epoch 7/30\n",
      "4410/4410 [==============================] - 4s 835us/step - loss: 0.4661 - acc: 0.7898\n",
      "Epoch 8/30\n",
      "4410/4410 [==============================] - 4s 831us/step - loss: 0.4349 - acc: 0.7959\n",
      "Epoch 9/30\n",
      "4410/4410 [==============================] - 4s 820us/step - loss: 0.4212 - acc: 0.8147\n",
      "Epoch 10/30\n",
      "4410/4410 [==============================] - 4s 814us/step - loss: 0.4159 - acc: 0.8122\n",
      "Epoch 11/30\n",
      "4410/4410 [==============================] - 4s 824us/step - loss: 0.3985 - acc: 0.8236\n",
      "Epoch 12/30\n",
      "4410/4410 [==============================] - 4s 821us/step - loss: 0.3891 - acc: 0.8322\n",
      "Epoch 13/30\n",
      "4410/4410 [==============================] - 4s 815us/step - loss: 0.3739 - acc: 0.8333\n",
      "Epoch 14/30\n",
      "4410/4410 [==============================] - 4s 821us/step - loss: 0.3767 - acc: 0.8358\n",
      "Epoch 15/30\n",
      "4410/4410 [==============================] - 4s 850us/step - loss: 0.3518 - acc: 0.8481\n",
      "Epoch 16/30\n",
      "4410/4410 [==============================] - 4s 825us/step - loss: 0.3509 - acc: 0.8506\n",
      "Epoch 17/30\n",
      "4410/4410 [==============================] - 4s 826us/step - loss: 0.3433 - acc: 0.8503\n",
      "Epoch 18/30\n",
      "4410/4410 [==============================] - 4s 986us/step - loss: 0.3341 - acc: 0.8506\n",
      "Epoch 19/30\n",
      "4410/4410 [==============================] - 4s 979us/step - loss: 0.3108 - acc: 0.8619\n",
      "Epoch 20/30\n",
      "4410/4410 [==============================] - 4s 818us/step - loss: 0.3310 - acc: 0.8590\n",
      "Epoch 21/30\n",
      "4410/4410 [==============================] - 3s 786us/step - loss: 0.3312 - acc: 0.8594\n",
      "Epoch 22/30\n",
      "4410/4410 [==============================] - 3s 790us/step - loss: 0.3202 - acc: 0.8696\n",
      "Epoch 23/30\n",
      "4410/4410 [==============================] - 4s 829us/step - loss: 0.3252 - acc: 0.8617\n",
      "Epoch 24/30\n",
      "4410/4410 [==============================] - 4s 880us/step - loss: 0.3136 - acc: 0.8646\n",
      "Epoch 25/30\n",
      "4410/4410 [==============================] - 4s 878us/step - loss: 0.2948 - acc: 0.8785\n",
      "Epoch 26/30\n",
      "4410/4410 [==============================] - 4s 821us/step - loss: 0.3009 - acc: 0.8683\n",
      "Epoch 27/30\n",
      "4410/4410 [==============================] - 4s 923us/step - loss: 0.2930 - acc: 0.8803\n",
      "Epoch 28/30\n",
      "4410/4410 [==============================] - 4s 883us/step - loss: 0.2848 - acc: 0.8787\n",
      "Epoch 29/30\n",
      "4410/4410 [==============================] - 4s 864us/step - loss: 0.2810 - acc: 0.8819\n",
      "Epoch 30/30\n",
      "4410/4410 [==============================] - 4s 860us/step - loss: 0.2953 - acc: 0.8816\n",
      "Neural network 3\n",
      "10663 lines tested\n",
      "Confusion matrix : \n",
      " [[3567 1764]\n",
      " [1211 4120]]\n",
      "Accuracy : 0.7209716751078596\n",
      "Epoch 1/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.6933 - acc: 0.5050\n",
      "Epoch 2/30\n",
      "4410/4410 [==============================] - 4s 825us/step - loss: 0.6929 - acc: 0.5179\n",
      "Epoch 3/30\n",
      "4410/4410 [==============================] - 4s 811us/step - loss: 0.6824 - acc: 0.5501\n",
      "Epoch 4/30\n",
      "4410/4410 [==============================] - 4s 829us/step - loss: 0.5921 - acc: 0.7147\n",
      "Epoch 5/30\n",
      "4410/4410 [==============================] - 4s 849us/step - loss: 0.5261 - acc: 0.7633\n",
      "Epoch 6/30\n",
      "4410/4410 [==============================] - 4s 834us/step - loss: 0.4870 - acc: 0.7875\n",
      "Epoch 7/30\n",
      "4410/4410 [==============================] - 4s 846us/step - loss: 0.4600 - acc: 0.8002\n",
      "Epoch 8/30\n",
      "4410/4410 [==============================] - 4s 832us/step - loss: 0.4421 - acc: 0.8002\n",
      "Epoch 9/30\n",
      "4410/4410 [==============================] - 4s 826us/step - loss: 0.4254 - acc: 0.8107\n",
      "Epoch 10/30\n",
      "4410/4410 [==============================] - 4s 847us/step - loss: 0.4157 - acc: 0.8231\n",
      "Epoch 11/30\n",
      "4410/4410 [==============================] - 4s 873us/step - loss: 0.3863 - acc: 0.8195\n",
      "Epoch 12/30\n",
      "4410/4410 [==============================] - 4s 908us/step - loss: 0.3884 - acc: 0.8306\n",
      "Epoch 13/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3843 - acc: 0.8331\n",
      "Epoch 14/30\n",
      "4410/4410 [==============================] - 4s 894us/step - loss: 0.3667 - acc: 0.8404\n",
      "Epoch 15/30\n",
      "4410/4410 [==============================] - 4s 806us/step - loss: 0.3691 - acc: 0.8408\n",
      "Epoch 16/30\n",
      "4410/4410 [==============================] - 4s 827us/step - loss: 0.3538 - acc: 0.8467\n",
      "Epoch 17/30\n",
      "4410/4410 [==============================] - 4s 845us/step - loss: 0.3419 - acc: 0.8522\n",
      "Epoch 18/30\n",
      "4410/4410 [==============================] - 4s 839us/step - loss: 0.3367 - acc: 0.8587\n",
      "Epoch 19/30\n",
      "4410/4410 [==============================] - 4s 860us/step - loss: 0.3409 - acc: 0.8567\n",
      "Epoch 20/30\n",
      "4410/4410 [==============================] - 4s 857us/step - loss: 0.3128 - acc: 0.8683\n",
      "Epoch 21/30\n",
      "4410/4410 [==============================] - 4s 862us/step - loss: 0.3200 - acc: 0.8653\n",
      "Epoch 22/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3196 - acc: 0.8669\n",
      "Epoch 23/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3053 - acc: 0.8739\n",
      "Epoch 24/30\n",
      "4410/4410 [==============================] - 4s 1ms/step - loss: 0.3140 - acc: 0.8755\n",
      "Epoch 25/30\n",
      "4410/4410 [==============================] - 4s 1ms/step - loss: 0.3034 - acc: 0.8748\n",
      "Epoch 26/30\n",
      "4410/4410 [==============================] - 4s 1ms/step - loss: 0.3021 - acc: 0.8719\n",
      "Epoch 27/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.2876 - acc: 0.8782\n",
      "Epoch 28/30\n",
      "4410/4410 [==============================] - 4s 921us/step - loss: 0.2993 - acc: 0.8812\n",
      "Epoch 29/30\n",
      "4410/4410 [==============================] - 4s 880us/step - loss: 0.2770 - acc: 0.8878\n",
      "Epoch 30/30\n",
      "4410/4410 [==============================] - 4s 905us/step - loss: 0.2881 - acc: 0.8800\n",
      "Neural network 4\n",
      "10663 lines tested\n",
      "Confusion matrix : \n",
      " [[4029 1302]\n",
      " [1597 3734]]\n",
      "Accuracy : 0.7280997936597261\n",
      "Epoch 1/30\n",
      "4410/4410 [==============================] - 8s 2ms/step - loss: 0.6933 - acc: 0.5029\n",
      "Epoch 2/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.6788 - acc: 0.5404\n",
      "Epoch 3/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.5743 - acc: 0.7295\n",
      "Epoch 4/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.5107 - acc: 0.7692\n",
      "Epoch 5/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.4685 - acc: 0.7887\n",
      "Epoch 6/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.4467 - acc: 0.7984\n",
      "Epoch 7/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.4319 - acc: 0.8023\n",
      "Epoch 8/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.4149 - acc: 0.8143\n",
      "Epoch 9/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.4063 - acc: 0.8143\n",
      "Epoch 10/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3896 - acc: 0.8204\n",
      "Epoch 11/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3787 - acc: 0.8351\n",
      "Epoch 12/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3764 - acc: 0.8324\n",
      "Epoch 13/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3475 - acc: 0.8506\n",
      "Epoch 14/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3760 - acc: 0.8444\n",
      "Epoch 15/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3448 - acc: 0.8420\n",
      "Epoch 16/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3439 - acc: 0.8481\n",
      "Epoch 17/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3368 - acc: 0.8594\n",
      "Epoch 18/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3334 - acc: 0.8603\n",
      "Epoch 19/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3289 - acc: 0.8655\n",
      "Epoch 20/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3152 - acc: 0.8615\n",
      "Epoch 21/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3071 - acc: 0.8732\n",
      "Epoch 22/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3011 - acc: 0.8741\n",
      "Epoch 23/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.3047 - acc: 0.8712\n",
      "Epoch 24/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.2880 - acc: 0.8782\n",
      "Epoch 25/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.2970 - acc: 0.8705\n",
      "Epoch 26/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.2975 - acc: 0.8766\n",
      "Epoch 27/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.2896 - acc: 0.8800\n",
      "Epoch 28/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.2820 - acc: 0.8834\n",
      "Epoch 29/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.2691 - acc: 0.8866\n",
      "Epoch 30/30\n",
      "4410/4410 [==============================] - 5s 1ms/step - loss: 0.2673 - acc: 0.8896\n",
      "Neural network 5\n",
      "10663 lines tested\n",
      "Confusion matrix : \n",
      " [[3910 1421]\n",
      " [1547 3784]]\n",
      "Accuracy : 0.7216282123429\n",
      "Epoch 1/30\n",
      "4410/4410 [==============================] - 11s 2ms/step - loss: 0.6935 - acc: 0.5120\n",
      "Epoch 2/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.6934 - acc: 0.5009\n",
      "Epoch 3/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.6756 - acc: 0.5517\n",
      "Epoch 4/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.5573 - acc: 0.7272\n",
      "Epoch 5/30\n",
      "4410/4410 [==============================] - 6s 1ms/step - loss: 0.4959 - acc: 0.7655\n",
      "Epoch 6/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.4629 - acc: 0.7916\n",
      "Epoch 7/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.4513 - acc: 0.8011\n",
      "Epoch 8/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.4324 - acc: 0.8095\n",
      "Epoch 9/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.4271 - acc: 0.8100\n",
      "Epoch 10/30\n",
      "4410/4410 [==============================] - 6s 1ms/step - loss: 0.4039 - acc: 0.8265\n",
      "Epoch 11/30\n",
      "4410/4410 [==============================] - 6s 1ms/step - loss: 0.3864 - acc: 0.8297\n",
      "Epoch 12/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.3816 - acc: 0.8311\n",
      "Epoch 13/30\n",
      "4410/4410 [==============================] - 6s 1ms/step - loss: 0.3687 - acc: 0.8397\n",
      "Epoch 14/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.3626 - acc: 0.8401\n",
      "Epoch 15/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.3466 - acc: 0.8469\n",
      "Epoch 16/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.3440 - acc: 0.8567\n",
      "Epoch 17/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.3325 - acc: 0.8615\n",
      "Epoch 18/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.3205 - acc: 0.8596\n",
      "Epoch 19/30\n",
      "4410/4410 [==============================] - 8s 2ms/step - loss: 0.3220 - acc: 0.8587\n",
      "Epoch 20/30\n",
      "4410/4410 [==============================] - 8s 2ms/step - loss: 0.3162 - acc: 0.8703\n",
      "Epoch 21/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.3256 - acc: 0.8585\n",
      "Epoch 22/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.3223 - acc: 0.8651\n",
      "Epoch 23/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.2958 - acc: 0.8726\n",
      "Epoch 24/30\n",
      "4410/4410 [==============================] - 10s 2ms/step - loss: 0.3121 - acc: 0.8689\n",
      "Epoch 25/30\n",
      "4410/4410 [==============================] - 8s 2ms/step - loss: 0.2849 - acc: 0.8785\n",
      "Epoch 26/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.2868 - acc: 0.8810\n",
      "Epoch 27/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.2870 - acc: 0.8794\n",
      "Epoch 28/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.2807 - acc: 0.8805\n",
      "Epoch 29/30\n",
      "4410/4410 [==============================] - 7s 1ms/step - loss: 0.2826 - acc: 0.8868\n",
      "Epoch 30/30\n",
      "4410/4410 [==============================] - 7s 2ms/step - loss: 0.2678 - acc: 0.8902\n",
      "Neural network 6\n",
      "10663 lines tested\n",
      "Confusion matrix : \n",
      " [[3266 2065]\n",
      " [1127 4204]]\n",
      "Accuracy : 0.7006190208216094\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(len(functions)) :\n",
    "    models.append(functions[i]())\n",
    "    models[i].fit(np.array(sentences_v),to_categorical(y),epochs=30,batch_size=100)\n",
    "    print(\"Neural network \"+str(i))\n",
    "    test(model,models[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob   \n",
    "path = 'datasets/polarization/txt_sentoken/pos/*.txt'   \n",
    "files=glob.glob(path)   \n",
    "sentences_test = []\n",
    "y_test = []\n",
    "for file in files:\n",
    "    f=open(file, 'r')  \n",
    "    sentences_test.append(f.read())\n",
    "    y_test.append(1)\n",
    "    f.close()\n",
    "    \n",
    "path = 'datasets/polarization/txt_sentoken/neg/*.txt'   \n",
    "files=glob.glob(path)\n",
    "for file in files:\n",
    "    f=open(file, 'r')  \n",
    "    sentences_test.append(f.read())\n",
    "    y_test.append(0)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for s in sentences_test:\n",
    "    y_pred.append(predict(s,model,best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion : \n",
      " [[769 231]\n",
      " [277 723]]\n",
      "Accuracy : 0.746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_trans=[]\n",
    "i=0\n",
    "for y_tmp in y_pred:\n",
    "    i+=1\n",
    "    if(y_tmp[0] > y_tmp[1]):\n",
    "        y_pred_trans.append(0)\n",
    "    else :\n",
    "        y_pred_trans.append(1)\n",
    "        \n",
    "cf = confusion_matrix(y_test,y_pred_trans)\n",
    "acc = (cf[0][0]+cf[1][1])/(cf[0][0]+cf[1][1]+cf[0][1]+cf[1][0])\n",
    "print(\"Matrice de confusion : \\n \"+ str (cf))\n",
    "print(\"Accuracy : \"+ str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
