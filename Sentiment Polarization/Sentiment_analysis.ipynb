{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opening of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"datasets/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar 28 13:00:48 2018\n",
    "\n",
    "@author: rniel\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "file  = open(dataset_path+\"imdb_labelled.txt\", \"rt\")\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "\n",
    "x=[]\n",
    "y=[]\n",
    "for c in contents:\n",
    "    temp=c.split(\"\\t\")\n",
    "    x.append(temp[0])\n",
    "    y.append(int(temp[1]))\n",
    "    \n",
    "file  = open(dataset_path+\"amazon_cells_labelled.txt\", \"rt\")\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "\n",
    "\n",
    "for c in contents:\n",
    "    temp=c.split(\"\\t\")\n",
    "    x.append(temp[0])\n",
    "    y.append(int(temp[1]))\n",
    "    \n",
    "file  = open(dataset_path+\"yelp_labelled.txt\", \"rt\")\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "\n",
    "\n",
    "for c in contents:\n",
    "    temp=c.split(\"\\t\")\n",
    "    x.append(temp[0])\n",
    "    y.append(int(temp[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file  = open(dataset_path+\"book.txt\", \"rt\")\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "del contents[-1]\n",
    "\n",
    "for c in contents:\n",
    "    temp=c.split(\"\\t\")\n",
    "    x.append(temp[1])\n",
    "    y.append(int(temp[0]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file  = open(\"Text/rt-polarity-neg.txt\", \"rt\",encoding = \"ISO-8859-1\")\n",
    "\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "del contents[-1]\n",
    "\n",
    "for i in range(len(contents)):\n",
    "    x.append(contents[i])\n",
    "    y.append(0)\n",
    "\n",
    "file  = open(\"Text/rt-polarity-pos.txt\", \"rt\",encoding = \"ISO-8859-1\")\n",
    "content = file.read()\n",
    "contents=[]\n",
    "contents=content.split(\"\\n\")\n",
    "del contents[-1]\n",
    "del contents[-1]\n",
    "\n",
    "for i in range(len(contents)):\n",
    "    x.append(contents[i])\n",
    "    y.append(1)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretreatment of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pretreatment(sentences):\n",
    "    \"\"\" This function clean an array of sentence and split them into list of words.\n",
    "            \n",
    "        Keyword arguments:\n",
    "        sentences -- An array of sentences\n",
    "    \"\"\"\n",
    "    for s in sentences:\n",
    "        s = s.replace(\",\", \" \")\n",
    "        s = s.replace(\".\", \" \")\n",
    "        s = s.replace(\"-\", \" \")\n",
    "        s = s.replace(\"!\", \" \")\n",
    "        s = s.replace(\"?\", \" \")\n",
    "        s = s.replace(\":\", \" \")\n",
    "        s = s.replace(\"*\", \" \")\n",
    "        s = s.replace(\"\\n\", \" \")\n",
    "\n",
    "\n",
    "    ret_sentences = []\n",
    "    for s in sentences:\n",
    "        ret_sentences.append(s.split(\" \"))\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        ret_sentences[i]=[w for w in ret_sentences[i] if w!=\"\"]\n",
    "\n",
    "    for s in ret_sentences:\n",
    "        for w in s :\n",
    "            w=w.lower()\n",
    "    return ret_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = pretreatment(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Training of the Word2Vec model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences, size=300, min_count=20, workers=4,sorted_vocab=1)\n",
    "model.train(sentences,total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of the GoogleNews vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the google word2vec model\n",
    "filename = 'lib/GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True,limit= 500000)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation of sentences into list of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorization(sentences,word2vec_model,word_length=300,max_len=50,adaptable_len=False):\n",
    "    \"\"\" This function transform an array of list of words (the splitted sentences) into a array of list of vectors \n",
    "            representing the words thanks to an already trained word2Vec model\n",
    "            \n",
    "        Keyword arguments:\n",
    "        sentences -- The different sentences you want to vectorize\n",
    "        word2vec_model -- An already trained gensim Word2Vec model\n",
    "        word_length -- The number of dimension of the vectors representing the words (default 300) \n",
    "        max_len -- The maximum length of a sentence (default 50) \n",
    "        adaptable_len -- A boolean allowing to adapt the maximum length to the longest sentence in your dataset\n",
    "    \"\"\"\n",
    "    sentences_v=[]\n",
    "    unusued_word=[]\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    sp_words = set(stopwords.words('english'))    \n",
    "    \n",
    "    for s in sentences:\n",
    "        temp=[]\n",
    "        for w in s :\n",
    "            if(w not in sp_words):\n",
    "                try :\n",
    "                    temp.append(word2vec_model[w])\n",
    "                except : \n",
    "                    unusued_word.append(w)\n",
    "        sentences_v.append(temp)\n",
    "        \n",
    "    if (adaptable_len is True):\n",
    "        for i in range(len(sentences_v)):\n",
    "            if(len(sentences_v[i])>max_len):\n",
    "                max_len=len(sentences_v[i])\n",
    "\n",
    "    for s in sentences_v:\n",
    "        sentence_length = len(s)\n",
    "        if(sentence_length<max_len):    \n",
    "            for j in range(max_len-sentence_length):\n",
    "                s.append([0]*word_length)\n",
    "    return sentences_v,max_len\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_v = vectorization(sentences,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving of the variables for computation time gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.save(\"variables/sentences_v_google.npy\",sentences_v)\n",
    "np.save(\"variables/y.npy\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "x_vec_train = np.load(\"variables/sentences_v_google.npy\")\n",
    "y = np.load(\"variables/y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting of the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_vec_train,x_vec_test,y_vec_train,y_vec_test = train_test_split(sentences_v,y,test_size=0.2,random_state=42)\n",
    "x_vec_train=np.array(x_vec_train)\n",
    "x_vec_test=np.array(x_vec_test)\n",
    "from keras.utils import to_categorical\n",
    "y_vec_train=to_categorical(y_vec_train)\n",
    "y_vec_test=to_categorical(y_vec_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "def model_creation():\n",
    "    \n",
    "    WORD_LENGTH=300\n",
    "    max_len=50\n",
    "    \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation,Convolution1D,Flatten,MaxPooling1D,Conv1D,LSTM\n",
    "    from keras.layers import Dropout\n",
    "\n",
    "    NN = Sequential()\n",
    "\n",
    "    NN.add(Dense(max_len, activation='relu',input_shape=(max_len,WORD_LENGTH)))\n",
    "    NN.add(Dropout(0.5))\n",
    "    \n",
    "    NN.add(Convolution1D(64,kernel_size=10,activation='relu',border_mode='same',input_shape=(max_len,WORD_LENGTH)))\n",
    "    NN.add(Dropout(0.5))\n",
    "    \n",
    "    NN.add(MaxPooling1D(pool_size=2, strides=None, padding='same'))\n",
    "\n",
    "    NN.add(LSTM(50))\n",
    "    NN.add(Dropout(0.5))\n",
    "        \n",
    "    NN.add(Dense(50, activation='relu'))\n",
    "    NN.add(Dropout(0.5))\n",
    "\n",
    "    NN.add(Dense(25, activation='relu'))\n",
    "    NN.add(Dropout(0.5))\n",
    "\n",
    "    NN.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting between test and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_vec_test,x_vec_valid,y_vec_test,y_vec_valid =  train_test_split(x_vec_test,y_vec_test,test_size=0.5,random_state=42)\n",
    "print(\"Number of lines in the validation set : \"+str(len(x_vec_valid)))\n",
    "print(\"Number of lines in the test set: \"+str(len(x_vec_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import clone_model\n",
    "\n",
    "hist=[]\n",
    "NB_STEPS = 100\n",
    "NN = model_creation()\n",
    "for i in range(NB_STEPS):\n",
    "    print(\"Etape i :\" + str(i))\n",
    "    #temp_model=clone_model(NN)\n",
    "    #temp_model.set_weights(NN.get_weights())\n",
    "    NN.fit(np.array(x_vec_train), np.array(y_vec_train), epochs=1, batch_size=100)\n",
    "    #hist.append((temp_model,NN.evaluate(x_vec_valid,y_vec_valid)[1]))\n",
    "    hist.append(NN.evaluate(x_vec_valid,y_vec_valid)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting of validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(hist)    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the model on all the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Final = model_creation()\n",
    "Final.fit(np.array(sentences_v),to_categorical(y),epochs=150,batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation and model comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def time_estimation_cross_val(nb_epochs,nb_s_epoch,nb_split):\n",
    "    nb_min = (nb_epochs*nb_s_epoch*nb_split*nb_split)/60\n",
    "    print(\"The cross_validation will take around : \"+str(nb_min)+\" mn\")\n",
    "    return nb_min\n",
    "    \n",
    "def cross_val(model_function,x_train,y_train,nb_splits,nb_epochs):\n",
    "    \"\"\" This function allows to do a cross validation over a keras model\n",
    "        Keyword arguments:\n",
    "        model_function -- A function returning your model\n",
    "        x_train -- The train data\n",
    "        y_train -- The train target\n",
    "        nb_splits -- The number of folds \n",
    "        nb_epochs -- The number of epochs you want to train your network with \n",
    "                     /!\\ One validation will be a total of nb_folds*nb_epochs total number of epochs\n",
    "    \"\"\"\n",
    "    import time    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    indices = np.linspace(0,len(x_train)-1,num=nb_splits+1,dtype=int)\n",
    "    split_data = []\n",
    "\n",
    "    # Creation of the splits\n",
    "    split_data.append((x_train[indices[0]:indices[1]],y_train[indices[0]:indices[1]]))\n",
    "    for i in range(1,len(indices)-1):\n",
    "        split_data.append((x_train[indices[i]+1:indices[i+1]],y_train[indices[i]+1:indices[i+1]]))\n",
    "\n",
    "    # Cross val    \n",
    "    score = []\n",
    "    for i in range(nb_splits):\n",
    "        model = model_function()\n",
    "        valid = split_data[i]\n",
    "        for j in range(nb_splits):\n",
    "            if (j != i):\n",
    "                model.fit(split_data[j][0],split_data[j][1],epochs=nb_epochs,batch_size=100)\n",
    "        score.append(model.evaluate(valid[0],valid[1]))\n",
    "        \n",
    "    std = np.std([a[1] for a in score])\n",
    "    mean = np.mean([a[1] for a in score])\n",
    "    \n",
    "    print(\"Accruracy neural network: \"+str(mean*100)+\"% +- \"+str(std*100)+\"%\")\n",
    "    print(\"--- %s minutes elapsed---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "    return model,score,(mean,std)\n",
    "\n",
    "\n",
    "def multi_cross_val(model_functions,x_train,y_train,nb_splits,nb_epochs):\n",
    "    \"\"\" This function allows to compare different models over a k-fold cross validation\n",
    "        Keyword arguments:\n",
    "        model_functions -- A list containing the functions returning the different models you want to test\n",
    "        x_train -- The train data\n",
    "        y_train -- The train target\n",
    "        nb_splits -- The number of folds \n",
    "        nb_epochs -- The number of epochs you want to train your network with \n",
    "                     /!\\ One validation will be a total of nb_folds*nb_epochs total number of epochs\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    indices = np.linspace(0,len(x_train)-1,num=nb_splits+1,dtype=int)\n",
    "    split_data = []\n",
    "\n",
    "    # Creeation of the splits\n",
    "    split_data.append((x_train[indices[0]:indices[1]],y_train[indices[0]:indices[1]]))\n",
    "    for i in range(1,len(indices)-1):\n",
    "        split_data.append((x_train[indices[i]+1:indices[i+1]],y_train[indices[i]+1:indices[i+1]]))\n",
    "\n",
    "    # Cross val    \n",
    "    score = []\n",
    "    for i in range(nb_splits):\n",
    "        print(\"Split no :\"+str(i))\n",
    "        models = [f() for f in model_functions]\n",
    "        valid = split_data[i]\n",
    "        for j in range(nb_splits):\n",
    "            if (j != i):\n",
    "                for m in models : \n",
    "                    m.fit(split_data[j][0],split_data[j][1],epochs=nb_epochs,batch_size=150)                 \n",
    "        for k in range(len(models)) :\n",
    "            score.append((k,models[k].evaluate(valid[0],valid[1])))\n",
    "    std = {}\n",
    "    mean = {}\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        tmp = [s for s in score if s[0]==i]\n",
    "        std[i]=np.std([a[1][1] for a in tmp])\n",
    "        mean[i]=np.mean([a[1][1] for a in tmp])\n",
    "\n",
    "    for k in mean.keys():\n",
    "        print(\"Accruracy neural network\"+str(k) +\" : \"+str(mean[k]*100)+\"% +- \"+str(std[k]*100)+\"%\")\n",
    "    \n",
    "    print(\"--- %s minutes elapsed---\" % ((time.time() - start_time)/60))\n",
    "\n",
    "    return models,score,mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_estimation_cross_val(80,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(multi_cross_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NN,score, printing = cross_val(model_creation,x_vec_train,y_vec_train,3,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_functions = [model_creation,model_creation1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NNs,test,test1,test2 = multi_cross_val(model_functions,x_vec_train,y_vec_train,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting of validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "acc = []\n",
    "for i in range(len(hist)):\n",
    "    acc.append(hist[i][1])\n",
    "\n",
    "plt.plot(acc)    \n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperation of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best = NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best.evaluate(x_vec_test,y_vec_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model,name,path):\n",
    "    \"\"\" This function saves a Keras model.\n",
    "            \n",
    "        Keyword arguments:\n",
    "        model -- The Keras model to be saved\n",
    "        name -- Name to be given to the Keras model\n",
    "        path -- Path where to save the Keras model\n",
    "    \"\"\"    \n",
    "    model_json = model.to_json()\n",
    "    with open(path+name+\".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "        model.save_weights(path+name+\".h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "def load_model(name,path):\n",
    "    \"\"\" This function loads a Keras model.\n",
    "            /!\\ You'll need to compile the loaded model.\n",
    "        Keyword arguments:\n",
    "        name -- Name of the Keras model\n",
    "        path -- Path from where to load the Keras model\n",
    "    \"\"\"       \n",
    "    from keras.models import model_from_json\n",
    "    # load json and create model\n",
    "    json_file = open(path+name+'.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(path+name+\".h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation of the model for live prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vote(score):\n",
    "    \"\"\" This function choose return the average of a score array for larger than the maximum size sentences.\n",
    "            \n",
    "        Keyword arguments:\n",
    "        score -- An array containing the score of the different parts of a sentence\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    neg=[]\n",
    "    pos=[]\n",
    "    for s in score :\n",
    "        neg.append(s[0][0])\n",
    "        pos.append(s[0][1])\n",
    "    sup_neg = [n for n in neg if n > 0.5]\n",
    "    sup_pos = [p for p in pos if p > 0.5]\n",
    "\n",
    "    return [np.mean(neg),np.mean(pos)]\n",
    "    \n",
    "def predict(sentence,word2Vec_model,model,max_len=50,word_length=300):\n",
    "    \"\"\" This function predict if a sentence is positive or negative\n",
    "            /!\\ If the sentence is larger than the max_len, then it's splitted into phrases of max_len length \n",
    "            then the average of the predictions of the phrases is made to make the final prediction\n",
    "\n",
    "        Keyword arguments:\n",
    "        sentence -- The sentence to make a prediction on\n",
    "        word2vec_model -- An already trained gensim Word2Vec model\n",
    "        model -- The keras model to use to make the prediction\n",
    "        max_len -- The maximum length of a sentence (default 50) \n",
    "        word_length -- The number of dimension of the vectors representing the words (default 300) \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    sentence = sentence.split(\" \")\n",
    "    good_sentence = []\n",
    "    unusued_words = []\n",
    "    for i in range(len(sentence)):\n",
    "        try :\n",
    "            sentence[i]=sentence[i].lower()\n",
    "            good_sentence.append(word2Vec_model.wv[sentence[i]])\n",
    "        except : \n",
    "            unusued_words.append(sentence[i])\n",
    "    sentence_length = len(good_sentence)\n",
    "    \n",
    "    if(sentence_length<max_len):    \n",
    "        for j in range(max_len-sentence_length):\n",
    "            good_sentence.append([0]*word_length)     \n",
    "            \n",
    "    if (sentence_length > max_len) :\n",
    "        predictions = []\n",
    "        ind = 0\n",
    "        for i in range(int(np.ceil(sentence_length/max_len))):\n",
    "            sent_tmp = good_sentence[ind:ind+max_len]\n",
    "            if(len(sent_tmp)<max_len):\n",
    "                for j in range(max_len-len(sent_tmp)):\n",
    "                    sent_tmp.append([0]*word_length)    \n",
    "            sent_tmp = np.array(sent_tmp)        \n",
    "            sent_tmp = np.reshape(sent_tmp, (1,sent_tmp.shape[0], sent_tmp.shape[1]))\n",
    "            try :\n",
    "                predictions.append(model.predict(sent_tmp))\n",
    "                ind+=max_len                    \n",
    "            except : \n",
    "                print(\"Erreur au découpage no :\"+str(i))\n",
    "        return vote(predictions)\n",
    "    \n",
    "    good_sentence = np.array(good_sentence)        \n",
    "    good_sentence = np.reshape(good_sentence, (1,good_sentence.shape[0], good_sentence.shape[1]))\n",
    "    return np.squeeze(model.predict(good_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction over a typed sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best=NN_loaded\n",
    "sentence =input()\n",
    "print(\" Négatif        Positif\")\n",
    "print(predict(sentence,model,best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
